
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Leveraging BERT and a class-based TF-IDF to create easily interpretable topics.">
      
      
      
        <meta name="author" content="Maarten P. Grootendorst">
      
      
        <link rel="canonical" href="https://maartengr.github.io/BERTopic/api/bertopic.html">
      
      <link rel="icon" href="../icon.png">
      <meta name="generator" content="mkdocs-1.1.2, mkdocs-material-7.1.6">
    
    
      
        <title>BERTopic - BERTopic</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.875de78c.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.f1a3b89f.min.css">
        
          
          
          <meta name="theme-color" content="#000000">
        
      
    
    
    
      
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Ubuntu:300,400,400i,700%7CUbuntu+Mono&display=fallback">
        <style>:root{--md-text-font-family:"Ubuntu";--md-code-font-family:"Ubuntu Mono"}</style>
      
    
    
    
      <link rel="stylesheet" href="../assets/_mkdocstrings.css">
    
      <link rel="stylesheet" href="../style.css">
    
    
      
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="" data-md-color-primary="black" data-md-color-accent="grey">
  
    
    <script>function __prefix(e){return new URL("..",location).pathname+"."+e}function __get(e,t=localStorage){return JSON.parse(t.getItem(__prefix(e)))}</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#bertopic" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="BERTopic" class="md-header__button md-logo" aria-label="BERTopic" data-md-component="logo">
      
  <img src="../icon_white.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            BERTopic
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              BERTopic
            
          </span>
        </div>
      </div>
    </div>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
      </label>
      
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" data-md-state="active" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
      </label>
      <button type="reset" class="md-search__icon md-icon" aria-label="Clear" tabindex="-1">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg>
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        
<a href="https://github.com/MaartenGr/BERTopic/" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="BERTopic" class="md-nav__button md-logo" aria-label="BERTopic" data-md-component="logo">
      
  <img src="../icon_white.png" alt="logo">

    </a>
    BERTopic
  </label>
  
    <div class="md-nav__source">
      
<a href="https://github.com/MaartenGr/BERTopic/" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../index.html" class="md-nav__link">
        Home
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../tutorial/algorithm/algorithm.html" class="md-nav__link">
        The Algorithm
      </a>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3" type="checkbox" id="__nav_3" >
      
      <label class="md-nav__link" for="__nav_3">
        Guides
        <span class="md-nav__icon md-icon"></span>
      </label>
      <nav class="md-nav" aria-label="Guides" data-md-level="1">
        <label class="md-nav__title" for="__nav_3">
          <span class="md-nav__icon md-icon"></span>
          Guides
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
  
  
  
    <li class="md-nav__item">
      <a href="../tutorial/quickstart/quickstart.html" class="md-nav__link">
        Getting Started
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../tutorial/embeddings/embeddings.html" class="md-nav__link">
        Embedding Models
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../tutorial/visualization/visualization.html" class="md-nav__link">
        Topic Visualization
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../tutorial/topicreduction/topicreduction.html" class="md-nav__link">
        Topic Reduction
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../tutorial/topicrepresentation/topicrepresentation.html" class="md-nav__link">
        Topic Representation
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../tutorial/search/search.html" class="md-nav__link">
        Search Topics
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../tutorial/models/models.html" class="md-nav__link">
        Custom Sub-Models
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../tutorial/topicsperclass/topicsperclass.html" class="md-nav__link">
        Topics per Class
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../tutorial/supervised/supervised.html" class="md-nav__link">
        (semi)-Supervised Topic Modeling
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../tutorial/topicsovertime/topicsovertime.html" class="md-nav__link">
        Dynamic Topic Modeling
      </a>
    </li>
  

          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_4" type="checkbox" id="__nav_4" checked>
      
      <label class="md-nav__link" for="__nav_4">
        API
        <span class="md-nav__icon md-icon"></span>
      </label>
      <nav class="md-nav" aria-label="API" data-md-level="1">
        <label class="md-nav__title" for="__nav_4">
          <span class="md-nav__icon md-icon"></span>
          API
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          BERTopic
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="bertopic.html" class="md-nav__link md-nav__link--active">
        BERTopic
      </a>
      
        
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#bertopic._bertopic.BERTopic" class="md-nav__link">
    bertopic._bertopic.BERTopic
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bertopic._bertopic.BERTopic.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bertopic._bertopic.BERTopic.__str__" class="md-nav__link">
    __str__()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bertopic._bertopic.BERTopic.find_topics" class="md-nav__link">
    find_topics()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bertopic._bertopic.BERTopic.fit" class="md-nav__link">
    fit()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bertopic._bertopic.BERTopic.fit_transform" class="md-nav__link">
    fit_transform()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bertopic._bertopic.BERTopic.get_params" class="md-nav__link">
    get_params()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bertopic._bertopic.BERTopic.get_topic" class="md-nav__link">
    get_topic()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bertopic._bertopic.BERTopic.get_topic_freq" class="md-nav__link">
    get_topic_freq()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bertopic._bertopic.BERTopic.get_topic_info" class="md-nav__link">
    get_topic_info()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bertopic._bertopic.BERTopic.get_topics" class="md-nav__link">
    get_topics()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bertopic._bertopic.BERTopic.load" class="md-nav__link">
    load()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bertopic._bertopic.BERTopic.reduce_topics" class="md-nav__link">
    reduce_topics()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bertopic._bertopic.BERTopic.save" class="md-nav__link">
    save()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bertopic._bertopic.BERTopic.topics_over_time" class="md-nav__link">
    topics_over_time()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bertopic._bertopic.BERTopic.topics_per_class" class="md-nav__link">
    topics_per_class()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bertopic._bertopic.BERTopic.transform" class="md-nav__link">
    transform()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bertopic._bertopic.BERTopic.update_topics" class="md-nav__link">
    update_topics()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bertopic._bertopic.BERTopic.visualize_barchart" class="md-nav__link">
    visualize_barchart()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bertopic._bertopic.BERTopic.visualize_distribution" class="md-nav__link">
    visualize_distribution()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bertopic._bertopic.BERTopic.visualize_heatmap" class="md-nav__link">
    visualize_heatmap()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bertopic._bertopic.BERTopic.visualize_hierarchy" class="md-nav__link">
    visualize_hierarchy()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bertopic._bertopic.BERTopic.visualize_term_rank" class="md-nav__link">
    visualize_term_rank()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bertopic._bertopic.BERTopic.visualize_topics" class="md-nav__link">
    visualize_topics()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bertopic._bertopic.BERTopic.visualize_topics_over_time" class="md-nav__link">
    visualize_topics_over_time()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bertopic._bertopic.BERTopic.visualize_topics_per_class" class="md-nav__link">
    visualize_topics_per_class()
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="ctfidf.html" class="md-nav__link">
        cTFIDF
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="mmr.html" class="md-nav__link">
        MMR
      </a>
    </li>
  

          
            
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_4_4" type="checkbox" id="__nav_4_4" >
      
      <label class="md-nav__link" for="__nav_4_4">
        Backends
        <span class="md-nav__icon md-icon"></span>
      </label>
      <nav class="md-nav" aria-label="Backends" data-md-level="2">
        <label class="md-nav__title" for="__nav_4_4">
          <span class="md-nav__icon md-icon"></span>
          Backends
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
  
  
  
    <li class="md-nav__item">
      <a href="backends/base.html" class="md-nav__link">
        Base
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="backends/word_doc.html" class="md-nav__link">
        Word Doc
      </a>
    </li>
  

          
        </ul>
      </nav>
    </li>
  

          
            
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_4_5" type="checkbox" id="__nav_4_5" >
      
      <label class="md-nav__link" for="__nav_4_5">
        Plotting
        <span class="md-nav__icon md-icon"></span>
      </label>
      <nav class="md-nav" aria-label="Plotting" data-md-level="2">
        <label class="md-nav__title" for="__nav_4_5">
          <span class="md-nav__icon md-icon"></span>
          Plotting
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
  
  
  
    <li class="md-nav__item">
      <a href="plotting/barchart.html" class="md-nav__link">
        Barchart
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="plotting/distribution.html" class="md-nav__link">
        Distribution
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="plotting/heatmap.html" class="md-nav__link">
        Heatmap
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="plotting/hierarchy.html" class="md-nav__link">
        Hierarchy
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="plotting/term.html" class="md-nav__link">
        Term Scores
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="plotting/topics.html" class="md-nav__link">
        Topics
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="plotting/dtm.html" class="md-nav__link">
        DTM
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="plotting/topics_per_class.html" class="md-nav__link">
        Topics per Class
      </a>
    </li>
  

          
        </ul>
      </nav>
    </li>
  

          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../faq.html" class="md-nav__link">
        FAQ
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../changelog.html" class="md-nav__link">
        Changelog
      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#bertopic._bertopic.BERTopic" class="md-nav__link">
    bertopic._bertopic.BERTopic
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bertopic._bertopic.BERTopic.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bertopic._bertopic.BERTopic.__str__" class="md-nav__link">
    __str__()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bertopic._bertopic.BERTopic.find_topics" class="md-nav__link">
    find_topics()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bertopic._bertopic.BERTopic.fit" class="md-nav__link">
    fit()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bertopic._bertopic.BERTopic.fit_transform" class="md-nav__link">
    fit_transform()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bertopic._bertopic.BERTopic.get_params" class="md-nav__link">
    get_params()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bertopic._bertopic.BERTopic.get_topic" class="md-nav__link">
    get_topic()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bertopic._bertopic.BERTopic.get_topic_freq" class="md-nav__link">
    get_topic_freq()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bertopic._bertopic.BERTopic.get_topic_info" class="md-nav__link">
    get_topic_info()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bertopic._bertopic.BERTopic.get_topics" class="md-nav__link">
    get_topics()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bertopic._bertopic.BERTopic.load" class="md-nav__link">
    load()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bertopic._bertopic.BERTopic.reduce_topics" class="md-nav__link">
    reduce_topics()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bertopic._bertopic.BERTopic.save" class="md-nav__link">
    save()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bertopic._bertopic.BERTopic.topics_over_time" class="md-nav__link">
    topics_over_time()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bertopic._bertopic.BERTopic.topics_per_class" class="md-nav__link">
    topics_per_class()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bertopic._bertopic.BERTopic.transform" class="md-nav__link">
    transform()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bertopic._bertopic.BERTopic.update_topics" class="md-nav__link">
    update_topics()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bertopic._bertopic.BERTopic.visualize_barchart" class="md-nav__link">
    visualize_barchart()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bertopic._bertopic.BERTopic.visualize_distribution" class="md-nav__link">
    visualize_distribution()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bertopic._bertopic.BERTopic.visualize_heatmap" class="md-nav__link">
    visualize_heatmap()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bertopic._bertopic.BERTopic.visualize_hierarchy" class="md-nav__link">
    visualize_hierarchy()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bertopic._bertopic.BERTopic.visualize_term_rank" class="md-nav__link">
    visualize_term_rank()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bertopic._bertopic.BERTopic.visualize_topics" class="md-nav__link">
    visualize_topics()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bertopic._bertopic.BERTopic.visualize_topics_over_time" class="md-nav__link">
    visualize_topics_over_time()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bertopic._bertopic.BERTopic.visualize_topics_per_class" class="md-nav__link">
    visualize_topics_per_class()
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content" data-md-component="content">
            <article class="md-content__inner md-typeset">
              
                
                  <a href="https://github.com/MaartenGr/BERTopic/edit/master/docs/api/bertopic.md" title="Edit this page" class="md-content__button md-icon">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25z"/></svg>
                  </a>
                
                
                <h1 id="bertopic"><code>BERTopic</code><a class="headerlink" href="#bertopic" title="Permanent link">&para;</a></h1>


  <div class="doc doc-object doc-class">

<a id="bertopic._bertopic.BERTopic"></a>
    <div class="doc doc-contents first">

      <p>BERTopic is a topic modeling technique that leverages BERT embeddings and
c-TF-IDF to create dense clusters allowing for easily interpretable topics
whilst keeping important words in the topic descriptions.</p>
<p>Usage:</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">bertopic</span> <span class="kn">import</span> <span class="n">BERTopic</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">fetch_20newsgroups</span>

<span class="n">docs</span> <span class="o">=</span> <span class="n">fetch_20newsgroups</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="s1">&#39;all&#39;</span><span class="p">)[</span><span class="s1">&#39;data&#39;</span><span class="p">]</span>
<span class="n">topic_model</span> <span class="o">=</span> <span class="n">BERTopic</span><span class="p">(</span><span class="n">calculate_probabilities</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">topics</span><span class="p">,</span> <span class="n">probabilities</span> <span class="o">=</span> <span class="n">topic_model</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>
</code></pre></div>
<p>If you want to use your own embedding model, use it as follows:</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">bertopic</span> <span class="kn">import</span> <span class="n">BERTopic</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">fetch_20newsgroups</span>
<span class="kn">from</span> <span class="nn">sentence_transformers</span> <span class="kn">import</span> <span class="n">SentenceTransformer</span>

<span class="n">docs</span> <span class="o">=</span> <span class="n">fetch_20newsgroups</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="s1">&#39;all&#39;</span><span class="p">)[</span><span class="s1">&#39;data&#39;</span><span class="p">]</span>
<span class="n">sentence_model</span> <span class="o">=</span> <span class="n">SentenceTransformer</span><span class="p">(</span><span class="s2">&quot;distilbert-base-nli-mean-tokens&quot;</span><span class="p">)</span>
<span class="n">topic_model</span> <span class="o">=</span> <span class="n">BERTopic</span><span class="p">(</span><span class="n">embedding_model</span><span class="o">=</span><span class="n">sentence_model</span><span class="p">)</span>
</code></pre></div>
<p>Due to the stochastisch nature of UMAP, the results from BERTopic might differ
and the quality can degrade. Using your own embeddings allows you to
try out BERTopic several times until you find the topics that suit
you best.</p>




  <div class="doc doc-children">









  <div class="doc doc-object doc-method">



<h2 id="bertopic._bertopic.BERTopic.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">language</span><span class="o">=</span><span class="s1">&#39;english&#39;</span><span class="p">,</span> <span class="n">top_n_words</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">n_gram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">min_topic_size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">nr_topics</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">low_memory</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">calculate_probabilities</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">embedding_model</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">umap_model</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">hdbscan_model</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">vectorizer_model</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

<a href="#bertopic._bertopic.BERTopic.__init__" class="headerlink" title="Permanent link">&para;</a></h2>

    <div class="doc doc-contents ">

      <p>BERTopic initialization</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>language</code></td>
        <td><code>str</code></td>
        <td><p>The main language used in your documents. For a full overview of
      supported languages see bertopic.backends.languages. Select
      "multilingual" to load in a sentence-tranformers model that supports 50+ languages.</p></td>
        <td><code>&#39;english&#39;</code></td>
      </tr>
      <tr>
        <td><code>top_n_words</code></td>
        <td><code>int</code></td>
        <td><p>The number of words per topic to extract. Setting this
         too high can negatively impact topic embeddings as topics
         are typically best represented by at most 10 words.</p></td>
        <td><code>10</code></td>
      </tr>
      <tr>
        <td><code>n_gram_range</code></td>
        <td><code>Tuple[int, int]</code></td>
        <td><p>The n-gram range for the CountVectorizer.
          Advised to keep high values between 1 and 3.
          More would likely lead to memory issues.
          NOTE: This param will not be used if you pass in your own
          CountVectorizer.</p></td>
        <td><code>(1, 1)</code></td>
      </tr>
      <tr>
        <td><code>min_topic_size</code></td>
        <td><code>int</code></td>
        <td><p>The minimum size of the topic. Increasing this value will lead
            to a lower number of clusters/topics.</p></td>
        <td><code>10</code></td>
      </tr>
      <tr>
        <td><code>nr_topics</code></td>
        <td><code>Union[int, str]</code></td>
        <td><p>Specifying the number of topics will reduce the initial
       number of topics to the value specified. This reduction can take
       a while as each reduction in topics (-1) activates a c-TF-IDF
       calculation. If this is set to None, no reduction is applied. Use
       "auto" to automatically reduce topics that have a similarity of at
       least 0.9, do not maps all others.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>low_memory</code></td>
        <td><code>bool</code></td>
        <td><p>Sets UMAP low memory to True to make sure less memory is used.</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>calculate_probabilities</code></td>
        <td><code>bool</code></td>
        <td><p>Whether to calculate the topic probabilities. This could
                     slow down the extraction of topics if you have many
                     documents (&gt; 100_000). Set this only to True if you
                     have a low amount of documents or if you do not mind
                     more computation time.
                     NOTE: since probabilities are not calculated, you cannot
                     use the corresponding visualization <code>visualize_probabilities</code>.</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>verbose</code></td>
        <td><code>bool</code></td>
        <td><p>Changes the verbosity of the model, Set to True if you want
     to track the stages of the model.</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>embedding_model</code></td>
        <td><code></code></td>
        <td><p>Use a custom embedding model.
             The following backends are currently supported
               * SentenceTransformers
               * Flair
               * Spacy
               * Gensim
               * USE (TF-Hub)
             You can also pass in a string that points to one of the following
             sentence-transformers models:
               * https://www.sbert.net/docs/pretrained_models.html</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>umap_model</code></td>
        <td><code>UMAP</code></td>
        <td><p>Pass in a UMAP model to be used instead of the default</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>hdbscan_model</code></td>
        <td><code>HDBSCAN</code></td>
        <td><p>Pass in a hdbscan.HDBSCAN model to be used instead of the default</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>vectorizer_model</code></td>
        <td><code>CountVectorizer</code></td>
        <td><p>Pass in a CountVectorizer instead of the default</p></td>
        <td><code>None</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>bertopic\_bertopic.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
             <span class="n">language</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;english&quot;</span><span class="p">,</span>
             <span class="n">top_n_words</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
             <span class="n">n_gram_range</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
             <span class="n">min_topic_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
             <span class="n">nr_topics</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
             <span class="n">low_memory</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
             <span class="n">calculate_probabilities</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
             <span class="n">embedding_model</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
             <span class="n">umap_model</span><span class="p">:</span> <span class="n">UMAP</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
             <span class="n">hdbscan_model</span><span class="p">:</span> <span class="n">hdbscan</span><span class="o">.</span><span class="n">HDBSCAN</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
             <span class="n">vectorizer_model</span><span class="p">:</span> <span class="n">CountVectorizer</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
             <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
             <span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;BERTopic initialization</span>

<span class="sd">    Arguments:</span>
<span class="sd">        language: The main language used in your documents. For a full overview of</span>
<span class="sd">                  supported languages see bertopic.backends.languages. Select</span>
<span class="sd">                  &quot;multilingual&quot; to load in a sentence-tranformers model that supports 50+ languages.</span>
<span class="sd">        top_n_words: The number of words per topic to extract. Setting this</span>
<span class="sd">                     too high can negatively impact topic embeddings as topics</span>
<span class="sd">                     are typically best represented by at most 10 words.</span>
<span class="sd">        n_gram_range: The n-gram range for the CountVectorizer.</span>
<span class="sd">                      Advised to keep high values between 1 and 3.</span>
<span class="sd">                      More would likely lead to memory issues.</span>
<span class="sd">                      NOTE: This param will not be used if you pass in your own</span>
<span class="sd">                      CountVectorizer.</span>
<span class="sd">        min_topic_size: The minimum size of the topic. Increasing this value will lead</span>
<span class="sd">                        to a lower number of clusters/topics.</span>
<span class="sd">        nr_topics: Specifying the number of topics will reduce the initial</span>
<span class="sd">                   number of topics to the value specified. This reduction can take</span>
<span class="sd">                   a while as each reduction in topics (-1) activates a c-TF-IDF</span>
<span class="sd">                   calculation. If this is set to None, no reduction is applied. Use</span>
<span class="sd">                   &quot;auto&quot; to automatically reduce topics that have a similarity of at</span>
<span class="sd">                   least 0.9, do not maps all others.</span>
<span class="sd">        low_memory: Sets UMAP low memory to True to make sure less memory is used.</span>
<span class="sd">        calculate_probabilities: Whether to calculate the topic probabilities. This could</span>
<span class="sd">                                 slow down the extraction of topics if you have many</span>
<span class="sd">                                 documents (&gt; 100_000). Set this only to True if you</span>
<span class="sd">                                 have a low amount of documents or if you do not mind</span>
<span class="sd">                                 more computation time.</span>
<span class="sd">                                 NOTE: since probabilities are not calculated, you cannot</span>
<span class="sd">                                 use the corresponding visualization `visualize_probabilities`.</span>
<span class="sd">        verbose: Changes the verbosity of the model, Set to True if you want</span>
<span class="sd">                 to track the stages of the model.</span>
<span class="sd">        embedding_model: Use a custom embedding model.</span>
<span class="sd">                         The following backends are currently supported</span>
<span class="sd">                           * SentenceTransformers</span>
<span class="sd">                           * Flair</span>
<span class="sd">                           * Spacy</span>
<span class="sd">                           * Gensim</span>
<span class="sd">                           * USE (TF-Hub)</span>
<span class="sd">                         You can also pass in a string that points to one of the following</span>
<span class="sd">                         sentence-transformers models:</span>
<span class="sd">                           * https://www.sbert.net/docs/pretrained_models.html</span>
<span class="sd">        umap_model: Pass in a UMAP model to be used instead of the default</span>
<span class="sd">        hdbscan_model: Pass in a hdbscan.HDBSCAN model to be used instead of the default</span>
<span class="sd">        vectorizer_model: Pass in a CountVectorizer instead of the default</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Topic-based parameters</span>
    <span class="k">if</span> <span class="n">top_n_words</span> <span class="o">&gt;</span> <span class="mi">30</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;top_n_words should be lower or equal to 30. The preferred value is 10.&quot;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">top_n_words</span> <span class="o">=</span> <span class="n">top_n_words</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">min_topic_size</span> <span class="o">=</span> <span class="n">min_topic_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">nr_topics</span> <span class="o">=</span> <span class="n">nr_topics</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">low_memory</span> <span class="o">=</span> <span class="n">low_memory</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">calculate_probabilities</span> <span class="o">=</span> <span class="n">calculate_probabilities</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">=</span> <span class="n">verbose</span>

    <span class="c1"># Embedding model</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">language</span> <span class="o">=</span> <span class="n">language</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">embedding_model</span> <span class="k">else</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">embedding_model</span> <span class="o">=</span> <span class="n">embedding_model</span>

    <span class="c1"># Vectorizer</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_gram_range</span> <span class="o">=</span> <span class="n">n_gram_range</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">vectorizer_model</span> <span class="o">=</span> <span class="n">vectorizer_model</span> <span class="ow">or</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">ngram_range</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_gram_range</span><span class="p">)</span>

    <span class="c1"># UMAP</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">umap_model</span> <span class="o">=</span> <span class="n">umap_model</span> <span class="ow">or</span> <span class="n">UMAP</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span>
                                         <span class="n">n_components</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
                                         <span class="n">min_dist</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
                                         <span class="n">metric</span><span class="o">=</span><span class="s1">&#39;cosine&#39;</span><span class="p">,</span>
                                         <span class="n">low_memory</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">low_memory</span><span class="p">)</span>

    <span class="c1"># HDBSCAN</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">hdbscan_model</span> <span class="o">=</span> <span class="n">hdbscan_model</span> <span class="ow">or</span> <span class="n">hdbscan</span><span class="o">.</span><span class="n">HDBSCAN</span><span class="p">(</span><span class="n">min_cluster_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">min_topic_size</span><span class="p">,</span>
                                                          <span class="n">metric</span><span class="o">=</span><span class="s1">&#39;euclidean&#39;</span><span class="p">,</span>
                                                          <span class="n">cluster_selection_method</span><span class="o">=</span><span class="s1">&#39;eom&#39;</span><span class="p">,</span>
                                                          <span class="n">prediction_data</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">topics</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">topic_sizes</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mapped_topics</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">topic_embeddings</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">topic_sim_matrix</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">set_level</span><span class="p">(</span><span class="s2">&quot;DEBUG&quot;</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h2 id="bertopic._bertopic.BERTopic.__str__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__str__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

<a href="#bertopic._bertopic.BERTopic.__str__" class="headerlink" title="Permanent link">&para;</a></h2>

    <div class="doc doc-contents ">

      <p>Get a string representation of the current object.</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>str</code></td>
      <td><p>Human readable representation of the most important model parameters.
     The parameters that represent models are ignored due to their</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>bertopic\_bertopic.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__str__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Get a string representation of the current object.</span>

<span class="sd">    Returns:</span>
<span class="sd">        str: Human readable representation of the most important model parameters.</span>
<span class="sd">             The parameters that represent models are ignored due to their</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">parameters</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
    <span class="k">for</span> <span class="n">parameter</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_params</span><span class="p">()</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">value</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
        <span class="k">if</span> <span class="s2">&quot;(&quot;</span> <span class="ow">in</span> <span class="n">value</span> <span class="ow">and</span> <span class="n">value</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="s2">&quot;(&quot;</span><span class="p">:</span>
            <span class="n">value</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;(&quot;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="s2">&quot;(...)&quot;</span>
        <span class="n">parameters</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">parameter</span><span class="si">}</span><span class="s2">=</span><span class="si">{</span><span class="n">value</span><span class="si">}</span><span class="s2">, &quot;</span>

    <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;BERTopic(</span><span class="si">{</span><span class="n">parameters</span><span class="p">[:</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span><span class="si">}</span><span class="s2">)&quot;</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h2 id="bertopic._bertopic.BERTopic.find_topics" class="doc doc-heading">
<code class="highlight language-python"><span class="n">find_topics</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">search_term</span><span class="p">,</span> <span class="n">top_n</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span></code>


<a href="#bertopic._bertopic.BERTopic.find_topics" class="headerlink" title="Permanent link">&para;</a></h2>

    <div class="doc doc-contents ">

      <p>Find topics most similar to a search_term</p>
<p>Creates an embedding for search_term and compares that with
the topic embeddings. The most similar topics are returned
along with their similarity values.</p>
<p>The search_term can be of any size but since it compares
with the topic representation it is advised to keep it
below 5 words.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>search_term</code></td>
        <td><code>str</code></td>
        <td><p>the term you want to use to search for topics</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>top_n</code></td>
        <td><code>int</code></td>
        <td><p>the number of topics to return</p></td>
        <td><code>5</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Tuple[List[int], List[float]]</code></td>
      <td><p>similar_topics: the most similar topics from high to low
similarity: the similarity scores from high to low</p></td>
    </tr>
  </tbody>
</table>      <p>Usage:</p>
<p>You can use the underlying embedding model to find topics that
best represent the search term:</p>
<div class="highlight"><pre><span></span><code><span class="n">topics</span><span class="p">,</span> <span class="n">similarity</span> <span class="o">=</span> <span class="n">topic_model</span><span class="o">.</span><span class="n">find_topics</span><span class="p">(</span><span class="s2">&quot;sports&quot;</span><span class="p">,</span> <span class="n">top_n</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
</code></pre></div>
<p>Note that the search query is typically more accurate if the
search_term consists of a phrase or multiple words.</p>

        <details class="quote">
          <summary>Source code in <code>bertopic\_bertopic.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">find_topics</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                <span class="n">search_term</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
                <span class="n">top_n</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]]:</span>
    <span class="sd">&quot;&quot;&quot; Find topics most similar to a search_term</span>

<span class="sd">    Creates an embedding for search_term and compares that with</span>
<span class="sd">    the topic embeddings. The most similar topics are returned</span>
<span class="sd">    along with their similarity values.</span>

<span class="sd">    The search_term can be of any size but since it compares</span>
<span class="sd">    with the topic representation it is advised to keep it</span>
<span class="sd">    below 5 words.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        search_term: the term you want to use to search for topics</span>
<span class="sd">        top_n: the number of topics to return</span>

<span class="sd">    Returns:</span>
<span class="sd">        similar_topics: the most similar topics from high to low</span>
<span class="sd">        similarity: the similarity scores from high to low</span>

<span class="sd">    Usage:</span>

<span class="sd">    You can use the underlying embedding model to find topics that</span>
<span class="sd">    best represent the search term:</span>

<span class="sd">    ```python</span>
<span class="sd">    topics, similarity = topic_model.find_topics(&quot;sports&quot;, top_n=5)</span>
<span class="sd">    ```</span>

<span class="sd">    Note that the search query is typically more accurate if the</span>
<span class="sd">    search_term consists of a phrase or multiple words.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding_model</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s2">&quot;This method can only be used if you did not use custom embeddings.&quot;</span><span class="p">)</span>

    <span class="n">topic_list</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">topics</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
    <span class="n">topic_list</span><span class="o">.</span><span class="n">sort</span><span class="p">()</span>

    <span class="c1"># Extract search_term embeddings and compare with topic embeddings</span>
    <span class="n">search_embedding</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_extract_embeddings</span><span class="p">([</span><span class="n">search_term</span><span class="p">],</span>
                                                <span class="n">method</span><span class="o">=</span><span class="s2">&quot;word&quot;</span><span class="p">,</span>
                                                <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
    <span class="n">sims</span> <span class="o">=</span> <span class="n">cosine_similarity</span><span class="p">(</span><span class="n">search_embedding</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">topic_embeddings</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>

    <span class="c1"># Extract topics most similar to search_term</span>
    <span class="n">ids</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">sims</span><span class="p">)[</span><span class="o">-</span><span class="n">top_n</span><span class="p">:]</span>
    <span class="n">similarity</span> <span class="o">=</span> <span class="p">[</span><span class="n">sims</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">ids</span><span class="p">][::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">similar_topics</span> <span class="o">=</span> <span class="p">[</span><span class="n">topic_list</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="n">ids</span><span class="p">][::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">similar_topics</span><span class="p">,</span> <span class="n">similarity</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h2 id="bertopic._bertopic.BERTopic.fit" class="doc doc-heading">
<code class="highlight language-python"><span class="n">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">documents</span><span class="p">,</span> <span class="n">embeddings</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>


<a href="#bertopic._bertopic.BERTopic.fit" class="headerlink" title="Permanent link">&para;</a></h2>

    <div class="doc doc-contents ">

      <p>Fit the models (Bert, UMAP, and, HDBSCAN) on a collection of documents and generate topics</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>documents</code></td>
        <td><code>List[str]</code></td>
        <td><p>A list of documents to fit on</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>embeddings</code></td>
        <td><code>ndarray</code></td>
        <td><p>Pre-trained document embeddings. These can be used
        instead of the sentence-transformer model</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>y</code></td>
        <td><code>Union[List[int], numpy.ndarray]</code></td>
        <td><p>The target class for (semi)-supervised modeling. Use -1 if no class for a
specific instance is specified.</p></td>
        <td><code>None</code></td>
      </tr>
  </tbody>
</table>      <p>Usage:</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">bertopic</span> <span class="kn">import</span> <span class="n">BERTopic</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">fetch_20newsgroups</span>

<span class="n">docs</span> <span class="o">=</span> <span class="n">fetch_20newsgroups</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="s1">&#39;all&#39;</span><span class="p">)[</span><span class="s1">&#39;data&#39;</span><span class="p">]</span>
<span class="n">topic_model</span> <span class="o">=</span> <span class="n">BERTopic</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>
</code></pre></div>
<p>If you want to use your own embeddings, use it as follows:</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">bertopic</span> <span class="kn">import</span> <span class="n">BERTopic</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">fetch_20newsgroups</span>
<span class="kn">from</span> <span class="nn">sentence_transformers</span> <span class="kn">import</span> <span class="n">SentenceTransformer</span>

<span class="c1"># Create embeddings</span>
<span class="n">docs</span> <span class="o">=</span> <span class="n">fetch_20newsgroups</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="s1">&#39;all&#39;</span><span class="p">)[</span><span class="s1">&#39;data&#39;</span><span class="p">]</span>
<span class="n">sentence_model</span> <span class="o">=</span> <span class="n">SentenceTransformer</span><span class="p">(</span><span class="s2">&quot;distilbert-base-nli-mean-tokens&quot;</span><span class="p">)</span>
<span class="n">embeddings</span> <span class="o">=</span> <span class="n">sentence_model</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">docs</span><span class="p">,</span> <span class="n">show_progress_bar</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Create topic model</span>
<span class="n">topic_model</span> <span class="o">=</span> <span class="n">BERTopic</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">docs</span><span class="p">,</span> <span class="n">embeddings</span><span class="p">)</span>
</code></pre></div>

        <details class="quote">
          <summary>Source code in <code>bertopic\_bertopic.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
        <span class="n">documents</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
        <span class="n">embeddings</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">y</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot; Fit the models (Bert, UMAP, and, HDBSCAN) on a collection of documents and generate topics</span>

<span class="sd">    Arguments:</span>
<span class="sd">        documents: A list of documents to fit on</span>
<span class="sd">        embeddings: Pre-trained document embeddings. These can be used</span>
<span class="sd">                    instead of the sentence-transformer model</span>
<span class="sd">        y: The target class for (semi)-supervised modeling. Use -1 if no class for a</span>
<span class="sd">           specific instance is specified.</span>

<span class="sd">    Usage:</span>

<span class="sd">    ```python</span>
<span class="sd">    from bertopic import BERTopic</span>
<span class="sd">    from sklearn.datasets import fetch_20newsgroups</span>

<span class="sd">    docs = fetch_20newsgroups(subset=&#39;all&#39;)[&#39;data&#39;]</span>
<span class="sd">    topic_model = BERTopic().fit(docs)</span>
<span class="sd">    ```</span>

<span class="sd">    If you want to use your own embeddings, use it as follows:</span>

<span class="sd">    ```python</span>
<span class="sd">    from bertopic import BERTopic</span>
<span class="sd">    from sklearn.datasets import fetch_20newsgroups</span>
<span class="sd">    from sentence_transformers import SentenceTransformer</span>

<span class="sd">    # Create embeddings</span>
<span class="sd">    docs = fetch_20newsgroups(subset=&#39;all&#39;)[&#39;data&#39;]</span>
<span class="sd">    sentence_model = SentenceTransformer(&quot;distilbert-base-nli-mean-tokens&quot;)</span>
<span class="sd">    embeddings = sentence_model.encode(docs, show_progress_bar=True)</span>

<span class="sd">    # Create topic model</span>
<span class="sd">    topic_model = BERTopic().fit(docs, embeddings)</span>
<span class="sd">    ```</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">documents</span><span class="p">,</span> <span class="n">embeddings</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h2 id="bertopic._bertopic.BERTopic.fit_transform" class="doc doc-heading">
<code class="highlight language-python"><span class="n">fit_transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">documents</span><span class="p">,</span> <span class="n">embeddings</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>


<a href="#bertopic._bertopic.BERTopic.fit_transform" class="headerlink" title="Permanent link">&para;</a></h2>

    <div class="doc doc-contents ">

      <p>Fit the models on a collection of documents, generate topics, and return the docs with topics</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>documents</code></td>
        <td><code>List[str]</code></td>
        <td><p>A list of documents to fit on</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>embeddings</code></td>
        <td><code>ndarray</code></td>
        <td><p>Pre-trained document embeddings. These can be used
        instead of the sentence-transformer model</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>y</code></td>
        <td><code>Union[List[int], numpy.ndarray]</code></td>
        <td><p>The target class for (semi)-supervised modeling. Use -1 if no class for a
specific instance is specified.</p></td>
        <td><code>None</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Tuple[List[int], Optional[numpy.ndarray]]</code></td>
      <td><p>predictions: Topic predictions for each documents
probabilities: The topic probability distribution which is returned by default.
               If <code>calculate_probabilities</code> in BERTopic is set to False, then the
               probabilities are not calculated to speed up computation and
               decrease memory usage.</p></td>
    </tr>
  </tbody>
</table>      <p>Usage:</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">bertopic</span> <span class="kn">import</span> <span class="n">BERTopic</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">fetch_20newsgroups</span>

<span class="n">docs</span> <span class="o">=</span> <span class="n">fetch_20newsgroups</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="s1">&#39;all&#39;</span><span class="p">)[</span><span class="s1">&#39;data&#39;</span><span class="p">]</span>
<span class="n">topic_model</span> <span class="o">=</span> <span class="n">BERTopic</span><span class="p">(</span><span class="n">calculate_probabilities</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">topics</span><span class="p">,</span> <span class="n">probs</span> <span class="o">=</span> <span class="n">topic_model</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>
</code></pre></div>
<p>If you want to use your own embeddings, use it as follows:</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">bertopic</span> <span class="kn">import</span> <span class="n">BERTopic</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">fetch_20newsgroups</span>
<span class="kn">from</span> <span class="nn">sentence_transformers</span> <span class="kn">import</span> <span class="n">SentenceTransformer</span>

<span class="c1"># Create embeddings</span>
<span class="n">docs</span> <span class="o">=</span> <span class="n">fetch_20newsgroups</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="s1">&#39;all&#39;</span><span class="p">)[</span><span class="s1">&#39;data&#39;</span><span class="p">]</span>
<span class="n">sentence_model</span> <span class="o">=</span> <span class="n">SentenceTransformer</span><span class="p">(</span><span class="s2">&quot;distilbert-base-nli-mean-tokens&quot;</span><span class="p">)</span>
<span class="n">embeddings</span> <span class="o">=</span> <span class="n">sentence_model</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">docs</span><span class="p">,</span> <span class="n">show_progress_bar</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Create topic model</span>
<span class="n">topic_model</span> <span class="o">=</span> <span class="n">BERTopic</span><span class="p">(</span><span class="n">calculate_probabilities</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">topics</span><span class="p">,</span> <span class="n">probs</span> <span class="o">=</span> <span class="n">topic_model</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">docs</span><span class="p">,</span> <span class="n">embeddings</span><span class="p">)</span>
</code></pre></div>

        <details class="quote">
          <summary>Source code in <code>bertopic\_bertopic.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">fit_transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                  <span class="n">documents</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
                  <span class="n">embeddings</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                  <span class="n">y</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                                                                   <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="kc">None</span><span class="p">]]:</span>
    <span class="sd">&quot;&quot;&quot; Fit the models on a collection of documents, generate topics, and return the docs with topics</span>

<span class="sd">    Arguments:</span>
<span class="sd">        documents: A list of documents to fit on</span>
<span class="sd">        embeddings: Pre-trained document embeddings. These can be used</span>
<span class="sd">                    instead of the sentence-transformer model</span>
<span class="sd">        y: The target class for (semi)-supervised modeling. Use -1 if no class for a</span>
<span class="sd">           specific instance is specified.</span>

<span class="sd">    Returns:</span>
<span class="sd">        predictions: Topic predictions for each documents</span>
<span class="sd">        probabilities: The topic probability distribution which is returned by default.</span>
<span class="sd">                       If `calculate_probabilities` in BERTopic is set to False, then the</span>
<span class="sd">                       probabilities are not calculated to speed up computation and</span>
<span class="sd">                       decrease memory usage.</span>

<span class="sd">    Usage:</span>

<span class="sd">    ```python</span>
<span class="sd">    from bertopic import BERTopic</span>
<span class="sd">    from sklearn.datasets import fetch_20newsgroups</span>

<span class="sd">    docs = fetch_20newsgroups(subset=&#39;all&#39;)[&#39;data&#39;]</span>
<span class="sd">    topic_model = BERTopic(calculate_probabilities=True)</span>
<span class="sd">    topics, probs = topic_model.fit_transform(docs)</span>
<span class="sd">    ```</span>

<span class="sd">    If you want to use your own embeddings, use it as follows:</span>

<span class="sd">    ```python</span>
<span class="sd">    from bertopic import BERTopic</span>
<span class="sd">    from sklearn.datasets import fetch_20newsgroups</span>
<span class="sd">    from sentence_transformers import SentenceTransformer</span>

<span class="sd">    # Create embeddings</span>
<span class="sd">    docs = fetch_20newsgroups(subset=&#39;all&#39;)[&#39;data&#39;]</span>
<span class="sd">    sentence_model = SentenceTransformer(&quot;distilbert-base-nli-mean-tokens&quot;)</span>
<span class="sd">    embeddings = sentence_model.encode(docs, show_progress_bar=True)</span>

<span class="sd">    # Create topic model</span>
<span class="sd">    topic_model = BERTopic(calculate_probabilities=True)</span>
<span class="sd">    topics, probs = topic_model.fit_transform(docs, embeddings)</span>
<span class="sd">    ```</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">check_documents_type</span><span class="p">(</span><span class="n">documents</span><span class="p">)</span>
    <span class="n">check_embeddings_shape</span><span class="p">(</span><span class="n">embeddings</span><span class="p">,</span> <span class="n">documents</span><span class="p">)</span>

    <span class="n">documents</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s2">&quot;Document&quot;</span><span class="p">:</span> <span class="n">documents</span><span class="p">,</span>
                              <span class="s2">&quot;ID&quot;</span><span class="p">:</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">documents</span><span class="p">)),</span>
                              <span class="s2">&quot;Topic&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">})</span>

    <span class="c1"># Extract embeddings</span>
    <span class="k">if</span> <span class="n">embeddings</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding_model</span> <span class="o">=</span> <span class="n">select_backend</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embedding_model</span><span class="p">,</span>
                                              <span class="n">language</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">language</span><span class="p">)</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_extract_embeddings</span><span class="p">(</span><span class="n">documents</span><span class="o">.</span><span class="n">Document</span><span class="p">,</span>
                                              <span class="n">method</span><span class="o">=</span><span class="s2">&quot;document&quot;</span><span class="p">,</span>
                                              <span class="n">verbose</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">)</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Transformed documents to Embeddings&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding_model</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">embedding_model</span> <span class="o">=</span> <span class="n">select_backend</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embedding_model</span><span class="p">,</span>
                                                  <span class="n">language</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">language</span><span class="p">)</span>

    <span class="c1"># Reduce dimensionality with UMAP</span>
    <span class="n">umap_embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reduce_dimensionality</span><span class="p">(</span><span class="n">embeddings</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="c1"># Cluster UMAP embeddings with HDBSCAN</span>
    <span class="n">documents</span><span class="p">,</span> <span class="n">probabilities</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cluster_embeddings</span><span class="p">(</span><span class="n">umap_embeddings</span><span class="p">,</span> <span class="n">documents</span><span class="p">)</span>

    <span class="c1"># Sort and Map Topic IDs by their frequency</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">nr_topics</span><span class="p">:</span>
        <span class="n">documents</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sort_mappings_by_frequency</span><span class="p">(</span><span class="n">documents</span><span class="p">)</span>

    <span class="c1"># Extract topics by calculating c-TF-IDF</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_extract_topics</span><span class="p">(</span><span class="n">documents</span><span class="p">)</span>

    <span class="c1"># Reduce topics</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">nr_topics</span><span class="p">:</span>
        <span class="n">documents</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reduce_topics</span><span class="p">(</span><span class="n">documents</span><span class="p">)</span>
        <span class="n">probabilities</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_map_probabilities</span><span class="p">(</span><span class="n">probabilities</span><span class="p">)</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">documents</span><span class="o">.</span><span class="n">Topic</span><span class="o">.</span><span class="n">to_list</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">predictions</span><span class="p">,</span> <span class="n">probabilities</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h2 id="bertopic._bertopic.BERTopic.get_params" class="doc doc-heading">
<code class="highlight language-python"><span class="n">get_params</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">deep</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>


<a href="#bertopic._bertopic.BERTopic.get_params" class="headerlink" title="Permanent link">&para;</a></h2>

    <div class="doc doc-contents ">

      <p>Get parameters for this estimator.</p>
<p>Adapted from:
    https://github.com/scikit-learn/scikit-learn/blob/b3ea3ed6a/sklearn/base.py#L178</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>deep</code></td>
        <td><code>bool</code></td>
        <td><p>bool, default=True
  If True, will return the parameters for this estimator and
  contained subobjects that are estimators.</p></td>
        <td><code>False</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Mapping[str, Any]</code></td>
      <td><p>out: Parameter names mapped to their values.</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>bertopic\_bertopic.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">get_params</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">deep</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Mapping</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot; Get parameters for this estimator.</span>

<span class="sd">    Adapted from:</span>
<span class="sd">        https://github.com/scikit-learn/scikit-learn/blob/b3ea3ed6a/sklearn/base.py#L178</span>

<span class="sd">    Arguments:</span>
<span class="sd">        deep: bool, default=True</span>
<span class="sd">              If True, will return the parameters for this estimator and</span>
<span class="sd">              contained subobjects that are estimators.</span>

<span class="sd">    Returns:</span>
<span class="sd">        out: Parameter names mapped to their values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">out</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_param_names</span><span class="p">():</span>
        <span class="n">value</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">deep</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="s1">&#39;get_params&#39;</span><span class="p">):</span>
            <span class="n">deep_items</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">get_params</span><span class="p">()</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
            <span class="n">out</span><span class="o">.</span><span class="n">update</span><span class="p">((</span><span class="n">key</span> <span class="o">+</span> <span class="s1">&#39;__&#39;</span> <span class="o">+</span> <span class="n">k</span><span class="p">,</span> <span class="n">val</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">deep_items</span><span class="p">)</span>
        <span class="n">out</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>
    <span class="k">return</span> <span class="n">out</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h2 id="bertopic._bertopic.BERTopic.get_topic" class="doc doc-heading">
<code class="highlight language-python"><span class="n">get_topic</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">topic</span><span class="p">)</span></code>


<a href="#bertopic._bertopic.BERTopic.get_topic" class="headerlink" title="Permanent link">&para;</a></h2>

    <div class="doc doc-contents ">

      <p>Return top n words for a specific topic and their c-TF-IDF scores</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>topic</code></td>
        <td><code>int</code></td>
        <td><p>A specific topic for which you want its representation</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Union[Mapping[str, Tuple[str, float]], bool]</code></td>
      <td><p>The top n words for a specific word and its respective c-TF-IDF scores</p></td>
    </tr>
  </tbody>
</table>      <p>Usage:</p>
<div class="highlight"><pre><span></span><code><span class="n">topic</span> <span class="o">=</span> <span class="n">topic_model</span><span class="o">.</span><span class="n">get_topic</span><span class="p">(</span><span class="mi">12</span><span class="p">)</span>
</code></pre></div>

        <details class="quote">
          <summary>Source code in <code>bertopic\_bertopic.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">get_topic</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">topic</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Mapping</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]],</span> <span class="nb">bool</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot; Return top n words for a specific topic and their c-TF-IDF scores</span>

<span class="sd">    Arguments:</span>
<span class="sd">        topic: A specific topic for which you want its representation</span>

<span class="sd">    Returns:</span>
<span class="sd">        The top n words for a specific word and its respective c-TF-IDF scores</span>

<span class="sd">    Usage:</span>

<span class="sd">    ```python</span>
<span class="sd">    topic = topic_model.get_topic(12)</span>
<span class="sd">    ```</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">topic</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">topics</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">topics</span><span class="p">[</span><span class="n">topic</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">False</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h2 id="bertopic._bertopic.BERTopic.get_topic_freq" class="doc doc-heading">
<code class="highlight language-python"><span class="n">get_topic_freq</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">topic</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>


<a href="#bertopic._bertopic.BERTopic.get_topic_freq" class="headerlink" title="Permanent link">&para;</a></h2>

    <div class="doc doc-contents ">

      <p>Return the the size of topics (descending order)</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>topic</code></td>
        <td><code>int</code></td>
        <td><p>A specific topic for which you want the frequency</p></td>
        <td><code>None</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Union[pandas.core.frame.DataFrame, int]</code></td>
      <td><p>Either the frequency of a single topic or dataframe with
the frequencies of all topics</p></td>
    </tr>
  </tbody>
</table>      <p>Usage:</p>
<p>To extract the frequency of all topics:</p>
<div class="highlight"><pre><span></span><code><span class="n">frequency</span> <span class="o">=</span> <span class="n">topic_model</span><span class="o">.</span><span class="n">get_topic_freq</span><span class="p">()</span>
</code></pre></div>
<p>To get the frequency of a single topic:</p>
<div class="highlight"><pre><span></span><code><span class="n">frequency</span> <span class="o">=</span> <span class="n">topic_model</span><span class="o">.</span><span class="n">get_topic_freq</span><span class="p">(</span><span class="mi">12</span><span class="p">)</span>
</code></pre></div>

        <details class="quote">
          <summary>Source code in <code>bertopic\_bertopic.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">get_topic_freq</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">topic</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span> <span class="nb">int</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot; Return the the size of topics (descending order)</span>

<span class="sd">    Arguments:</span>
<span class="sd">        topic: A specific topic for which you want the frequency</span>

<span class="sd">    Returns:</span>
<span class="sd">        Either the frequency of a single topic or dataframe with</span>
<span class="sd">        the frequencies of all topics</span>

<span class="sd">    Usage:</span>

<span class="sd">    To extract the frequency of all topics:</span>

<span class="sd">    ```python</span>
<span class="sd">    frequency = topic_model.get_topic_freq()</span>
<span class="sd">    ```</span>

<span class="sd">    To get the frequency of a single topic:</span>

<span class="sd">    ```python</span>
<span class="sd">    frequency = topic_model.get_topic_freq(12)</span>
<span class="sd">    ```</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">topic</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">topic_sizes</span><span class="p">[</span><span class="n">topic</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">topic_sizes</span><span class="o">.</span><span class="n">items</span><span class="p">(),</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Topic&#39;</span><span class="p">,</span> <span class="s1">&#39;Count&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="s2">&quot;Count&quot;</span><span class="p">,</span>
                                                                                              <span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h2 id="bertopic._bertopic.BERTopic.get_topic_info" class="doc doc-heading">
<code class="highlight language-python"><span class="n">get_topic_info</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">topic</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>


<a href="#bertopic._bertopic.BERTopic.get_topic_info" class="headerlink" title="Permanent link">&para;</a></h2>

    <div class="doc doc-contents ">

      <p>Get information about each topic including its id, frequency, and name</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>topic</code></td>
        <td><code>int</code></td>
        <td><p>A specific topic for which you want the frequency</p></td>
        <td><code>None</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>DataFrame</code></td>
      <td><p>info: The information relating to either a single topic or all topics</p></td>
    </tr>
  </tbody>
</table>      <p>Usage:</p>
<div class="highlight"><pre><span></span><code><span class="n">info_df</span> <span class="o">=</span> <span class="n">topic_model</span><span class="o">.</span><span class="n">get_topic_info</span><span class="p">()</span>
</code></pre></div>

        <details class="quote">
          <summary>Source code in <code>bertopic\_bertopic.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">get_topic_info</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">topic</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot; Get information about each topic including its id, frequency, and name</span>

<span class="sd">    Arguments:</span>
<span class="sd">        topic: A specific topic for which you want the frequency</span>

<span class="sd">    Returns:</span>
<span class="sd">        info: The information relating to either a single topic or all topics</span>

<span class="sd">    Usage:</span>

<span class="sd">    ```python</span>
<span class="sd">    info_df = topic_model.get_topic_info()</span>
<span class="sd">    ```</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

    <span class="n">info</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">topic_sizes</span><span class="o">.</span><span class="n">items</span><span class="p">(),</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Topic&#39;</span><span class="p">,</span> <span class="s1">&#39;Count&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="s2">&quot;Count&quot;</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">info</span><span class="p">[</span><span class="s2">&quot;Name&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">info</span><span class="o">.</span><span class="n">Topic</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">topic_names</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">topic</span><span class="p">:</span>
        <span class="n">info</span> <span class="o">=</span> <span class="n">info</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">info</span><span class="o">.</span><span class="n">Topic</span> <span class="o">==</span> <span class="n">topic</span><span class="p">,</span> <span class="p">:]</span>

    <span class="k">return</span> <span class="n">info</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h2 id="bertopic._bertopic.BERTopic.get_topics" class="doc doc-heading">
<code class="highlight language-python"><span class="n">get_topics</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span></code>


<a href="#bertopic._bertopic.BERTopic.get_topics" class="headerlink" title="Permanent link">&para;</a></h2>

    <div class="doc doc-contents ">

      <p>Return topics with top n words and their c-TF-IDF score</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Mapping[str, Tuple[str, float]]</code></td>
      <td><p>self.topic: The top n words per topic and the corresponding c-TF-IDF score</p></td>
    </tr>
  </tbody>
</table>      <p>Usage:</p>
<div class="highlight"><pre><span></span><code><span class="n">all_topics</span> <span class="o">=</span> <span class="n">topic_model</span><span class="o">.</span><span class="n">get_topics</span><span class="p">()</span>
</code></pre></div>

        <details class="quote">
          <summary>Source code in <code>bertopic\_bertopic.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">get_topics</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Mapping</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]]:</span>
    <span class="sd">&quot;&quot;&quot; Return topics with top n words and their c-TF-IDF score</span>

<span class="sd">    Returns:</span>
<span class="sd">        self.topic: The top n words per topic and the corresponding c-TF-IDF score</span>

<span class="sd">    Usage:</span>

<span class="sd">    ```python</span>
<span class="sd">    all_topics = topic_model.get_topics()</span>
<span class="sd">    ```</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">topics</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h2 id="bertopic._bertopic.BERTopic.load" class="doc doc-heading">
<code class="highlight language-python"><span class="n">load</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">embedding_model</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-classmethod"><code>classmethod</code></small>
  </span>

<a href="#bertopic._bertopic.BERTopic.load" class="headerlink" title="Permanent link">&para;</a></h2>

    <div class="doc doc-contents ">

      <p>Loads the model from the specified path</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>path</code></td>
        <td><code>str</code></td>
        <td><p>the location and name of the BERTopic file you want to load</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>embedding_model</code></td>
        <td><code></code></td>
        <td><p>If the embedding_model was not saved to save space or to load
             it in from the cloud, you can load it in by specifying it here.</p></td>
        <td><code>None</code></td>
      </tr>
  </tbody>
</table>      <p>Usage:</p>
<div class="highlight"><pre><span></span><code><span class="n">BERTopic</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;my_model&quot;</span><span class="p">)</span>
</code></pre></div>
<p>or if you did not save the embedding model:</p>
<div class="highlight"><pre><span></span><code><span class="n">BERTopic</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;my_model&quot;</span><span class="p">,</span> <span class="n">embedding_model</span><span class="o">=</span><span class="s2">&quot;xlm-r-bert-base-nli-stsb-mean-tokens&quot;</span><span class="p">)</span>
</code></pre></div>

        <details class="quote">
          <summary>Source code in <code>bertopic\_bertopic.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="nd">@classmethod</span>
<span class="k">def</span> <span class="nf">load</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span>
         <span class="n">path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
         <span class="n">embedding_model</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot; Loads the model from the specified path</span>

<span class="sd">    Arguments:</span>
<span class="sd">        path: the location and name of the BERTopic file you want to load</span>
<span class="sd">        embedding_model: If the embedding_model was not saved to save space or to load</span>
<span class="sd">                         it in from the cloud, you can load it in by specifying it here.</span>

<span class="sd">    Usage:</span>

<span class="sd">    ```python</span>
<span class="sd">    BERTopic.load(&quot;my_model&quot;)</span>
<span class="sd">    ```</span>

<span class="sd">    or if you did not save the embedding model:</span>

<span class="sd">    ```python</span>
<span class="sd">    BERTopic.load(&quot;my_model&quot;, embedding_model=&quot;xlm-r-bert-base-nli-stsb-mean-tokens&quot;)</span>
<span class="sd">    ```</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="s1">&#39;rb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">embedding_model</span><span class="p">:</span>
            <span class="n">topic_model</span> <span class="o">=</span> <span class="n">joblib</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">file</span><span class="p">)</span>
            <span class="n">topic_model</span><span class="o">.</span><span class="n">embedding_model</span> <span class="o">=</span> <span class="n">embedding_model</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">topic_model</span> <span class="o">=</span> <span class="n">joblib</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">file</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">topic_model</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h2 id="bertopic._bertopic.BERTopic.reduce_topics" class="doc doc-heading">
<code class="highlight language-python"><span class="n">reduce_topics</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">docs</span><span class="p">,</span> <span class="n">topics</span><span class="p">,</span> <span class="n">probabilities</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">nr_topics</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span></code>


<a href="#bertopic._bertopic.BERTopic.reduce_topics" class="headerlink" title="Permanent link">&para;</a></h2>

    <div class="doc doc-contents ">

      <p>Further reduce the number of topics to nr_topics.</p>
<p>The number of topics is further reduced by calculating the c-TF-IDF matrix
of the documents and then reducing them by iteratively merging the least
frequent topic with the most similar one based on their c-TF-IDF matrices.
The topics, their sizes, and representations are updated.</p>
<p>The reasoning for putting <code>docs</code>, <code>topics</code>, and <code>probs</code> as parameters is that
these values are not saved within BERTopic on purpose. If you were to have a
million documents, it seems very inefficient to save those in BERTopic
instead of a dedicated database.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>docs</code></td>
        <td><code>List[str]</code></td>
        <td><p>The docs you used when calling either <code>fit</code> or <code>fit_transform</code></p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>topics</code></td>
        <td><code>List[int]</code></td>
        <td><p>The topics that were returned when calling either <code>fit</code> or <code>fit_transform</code></p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>probabilities</code></td>
        <td><code>ndarray</code></td>
        <td><p>The probabilities that were returned when calling either <code>fit</code> or <code>fit_transform</code></p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>nr_topics</code></td>
        <td><code>int</code></td>
        <td><p>The number of topics you want reduced to</p></td>
        <td><code>20</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Tuple[List[int], numpy.ndarray]</code></td>
      <td><p>new_topics: Updated topics
new_probabilities: Updated probabilities</p></td>
    </tr>
  </tbody>
</table>      <p>Usage:</p>
<p>You can further reduce the topics by passing the documents with its
topics and probabilities (if they were calculated):</p>
<div class="highlight"><pre><span></span><code><span class="n">new_topics</span><span class="p">,</span> <span class="n">new_probs</span> <span class="o">=</span> <span class="n">topic_model</span><span class="o">.</span><span class="n">reduce_topics</span><span class="p">(</span><span class="n">docs</span><span class="p">,</span> <span class="n">topics</span><span class="p">,</span> <span class="n">probabilities</span><span class="p">,</span> <span class="n">nr_topics</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
</code></pre></div>
<p>If probabilities were not calculated simply run the function without them:</p>
<div class="highlight"><pre><span></span><code><span class="n">new_topics</span><span class="p">,</span> <span class="n">_</span><span class="o">=</span> <span class="n">topic_model</span><span class="o">.</span><span class="n">reduce_topics</span><span class="p">(</span><span class="n">docs</span><span class="p">,</span> <span class="n">topics</span><span class="p">,</span> <span class="n">nr_topics</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
</code></pre></div>

        <details class="quote">
          <summary>Source code in <code>bertopic\_bertopic.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">reduce_topics</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                  <span class="n">docs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
                  <span class="n">topics</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                  <span class="n">probabilities</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                  <span class="n">nr_topics</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">20</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot; Further reduce the number of topics to nr_topics.</span>

<span class="sd">    The number of topics is further reduced by calculating the c-TF-IDF matrix</span>
<span class="sd">    of the documents and then reducing them by iteratively merging the least</span>
<span class="sd">    frequent topic with the most similar one based on their c-TF-IDF matrices.</span>
<span class="sd">    The topics, their sizes, and representations are updated.</span>

<span class="sd">    The reasoning for putting `docs`, `topics`, and `probs` as parameters is that</span>
<span class="sd">    these values are not saved within BERTopic on purpose. If you were to have a</span>
<span class="sd">    million documents, it seems very inefficient to save those in BERTopic</span>
<span class="sd">    instead of a dedicated database.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        docs: The docs you used when calling either `fit` or `fit_transform`</span>
<span class="sd">        topics: The topics that were returned when calling either `fit` or `fit_transform`</span>
<span class="sd">        probabilities: The probabilities that were returned when calling either `fit` or `fit_transform`</span>
<span class="sd">        nr_topics: The number of topics you want reduced to</span>

<span class="sd">    Returns:</span>
<span class="sd">        new_topics: Updated topics</span>
<span class="sd">        new_probabilities: Updated probabilities</span>

<span class="sd">    Usage:</span>

<span class="sd">    You can further reduce the topics by passing the documents with its</span>
<span class="sd">    topics and probabilities (if they were calculated):</span>

<span class="sd">    ```python</span>
<span class="sd">    new_topics, new_probs = topic_model.reduce_topics(docs, topics, probabilities, nr_topics=30)</span>
<span class="sd">    ```</span>

<span class="sd">    If probabilities were not calculated simply run the function without them:</span>

<span class="sd">    ```python</span>
<span class="sd">    new_topics, _= topic_model.reduce_topics(docs, topics, nr_topics=30)</span>
<span class="sd">    ```</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">nr_topics</span> <span class="o">=</span> <span class="n">nr_topics</span>
    <span class="n">documents</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s2">&quot;Document&quot;</span><span class="p">:</span> <span class="n">docs</span><span class="p">,</span> <span class="s2">&quot;Topic&quot;</span><span class="p">:</span> <span class="n">topics</span><span class="p">})</span>

    <span class="c1"># Reduce number of topics</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_extract_topics</span><span class="p">(</span><span class="n">documents</span><span class="p">)</span>
    <span class="n">documents</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reduce_topics</span><span class="p">(</span><span class="n">documents</span><span class="p">)</span>

    <span class="c1"># Extract topics and map probabilities</span>
    <span class="n">new_topics</span> <span class="o">=</span> <span class="n">documents</span><span class="o">.</span><span class="n">Topic</span><span class="o">.</span><span class="n">to_list</span><span class="p">()</span>
    <span class="n">new_probabilities</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_map_probabilities</span><span class="p">(</span><span class="n">probabilities</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">new_topics</span><span class="p">,</span> <span class="n">new_probabilities</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h2 id="bertopic._bertopic.BERTopic.save" class="doc doc-heading">
<code class="highlight language-python"><span class="n">save</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">,</span> <span class="n">save_embedding_model</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>


<a href="#bertopic._bertopic.BERTopic.save" class="headerlink" title="Permanent link">&para;</a></h2>

    <div class="doc doc-contents ">

      <p>Saves the model to the specified path</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>path</code></td>
        <td><code>str</code></td>
        <td><p>the location and name of the file you want to save</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>save_embedding_model</code></td>
        <td><code>bool</code></td>
        <td><p>Whether to save the embedding model in this class
                  as you might have selected a local model or one that
                  is downloaded automatically from the cloud.</p></td>
        <td><code>True</code></td>
      </tr>
  </tbody>
</table>      <p>Usage:</p>
<div class="highlight"><pre><span></span><code><span class="n">topic_model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s2">&quot;my_model&quot;</span><span class="p">)</span>
</code></pre></div>
<p>or if you do not want the embedding_model to be saved locally:</p>
<div class="highlight"><pre><span></span><code><span class="n">topic_model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s2">&quot;my_model&quot;</span><span class="p">,</span> <span class="n">save_embedding_model</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</code></pre></div>

        <details class="quote">
          <summary>Source code in <code>bertopic\_bertopic.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">save</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
         <span class="n">path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
         <span class="n">save_embedding_model</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot; Saves the model to the specified path</span>

<span class="sd">    Arguments:</span>
<span class="sd">        path: the location and name of the file you want to save</span>
<span class="sd">        save_embedding_model: Whether to save the embedding model in this class</span>
<span class="sd">                              as you might have selected a local model or one that</span>
<span class="sd">                              is downloaded automatically from the cloud.</span>

<span class="sd">    Usage:</span>

<span class="sd">    ```python</span>
<span class="sd">    topic_model.save(&quot;my_model&quot;)</span>
<span class="sd">    ```</span>

<span class="sd">    or if you do not want the embedding_model to be saved locally:</span>

<span class="sd">    ```python</span>
<span class="sd">    topic_model.save(&quot;my_model&quot;, save_embedding_model=False)</span>
<span class="sd">    ```</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="s1">&#39;wb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">save_embedding_model</span><span class="p">:</span>
            <span class="n">embedding_model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding_model</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">embedding_model</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="n">joblib</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">file</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">embedding_model</span> <span class="o">=</span> <span class="n">embedding_model</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">joblib</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">file</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h2 id="bertopic._bertopic.BERTopic.topics_over_time" class="doc doc-heading">
<code class="highlight language-python"><span class="n">topics_over_time</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">docs</span><span class="p">,</span> <span class="n">topics</span><span class="p">,</span> <span class="n">timestamps</span><span class="p">,</span> <span class="n">nr_bins</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">datetime_format</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">evolution_tuning</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">global_tuning</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>


<a href="#bertopic._bertopic.BERTopic.topics_over_time" class="headerlink" title="Permanent link">&para;</a></h2>

    <div class="doc doc-contents ">

      <p>Create topics over time</p>
<p>To create the topics over time, BERTopic needs to be already fitted once.
From the fitted models, the c-TF-IDF representations are calculate at
each timestamp t. Then, the c-TF-IDF representations at timestamp t are
averaged with the global c-TF-IDF representations in order to fine-tune the
local representations.</p>
<p>!!! note
    Make sure to use a limited number of unique timestamps (&lt;100) as the
    c-TF-IDF representation will be calculated at each single unique timestamp.
    Having a large number of unique timestamps can take some time to be calculated.
    Moreover, there aren't many use-cased where you would like to see the difference
    in topic representations over more than 100 different timestamps.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>docs</code></td>
        <td><code>List[str]</code></td>
        <td><p>The documents you used when calling either <code>fit</code> or <code>fit_transform</code></p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>topics</code></td>
        <td><code>List[int]</code></td>
        <td><p>The topics that were returned when calling either <code>fit</code> or <code>fit_transform</code></p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>timestamps</code></td>
        <td><code>Union[List[str], List[int]]</code></td>
        <td><p>The timestamp of each document. This can be either a list of strings or ints.
        If it is a list of strings, then the datetime format will be automatically
        inferred. If it is a list of ints, then the documents will be ordered by
        ascending order.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>nr_bins</code></td>
        <td><code>int</code></td>
        <td><p>The number of bins you want to create for the timestamps. The left interval will
     be chosen as the timestamp. An additional column will be created with the
     entire interval.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>datetime_format</code></td>
        <td><code>str</code></td>
        <td><p>The datetime format of the timestamps if they are strings, eg %d/%m/%Y.
             Set this to None if you want to have it automatically detect the format.
             See strftime documentation for more information on choices:
             https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>evolution_tuning</code></td>
        <td><code>bool</code></td>
        <td><p>Fine-tune each topic representation at timestamp t by averaging its
              c-TF-IDF matrix with the c-TF-IDF matrix at timestamp t-1. This creates
              evolutionary topic representations.</p></td>
        <td><code>True</code></td>
      </tr>
      <tr>
        <td><code>global_tuning</code></td>
        <td><code>bool</code></td>
        <td><p>Fine-tune each topic representation at timestamp t by averaging its c-TF-IDF matrix
       with the global c-TF-IDF matrix. Turn this off if you want to prevent words in
       topic representations that could not be found in the documents at timestamp t.</p></td>
        <td><code>True</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>DataFrame</code></td>
      <td><p>topics_over_time: A dataframe that contains the topic, words, and frequency of topic
                  at timestamp t.</p></td>
    </tr>
  </tbody>
</table>      <p>Usage:</p>
<p>The timestamps variable represent the timestamp of each document. If you have over
100 unique timestamps, it is advised to bin the timestamps as shown below:</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">bertopic</span> <span class="kn">import</span> <span class="n">BERTopic</span>
<span class="n">topic_model</span> <span class="o">=</span> <span class="n">BERTopic</span><span class="p">()</span>
<span class="n">topics</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">topic_model</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>
<span class="n">topics_over_time</span> <span class="o">=</span> <span class="n">topic_model</span><span class="o">.</span><span class="n">topics_over_time</span><span class="p">(</span><span class="n">docs</span><span class="p">,</span> <span class="n">topics</span><span class="p">,</span> <span class="n">timestamps</span><span class="p">,</span> <span class="n">nr_bins</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
</code></pre></div>

        <details class="quote">
          <summary>Source code in <code>bertopic\_bertopic.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">topics_over_time</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                     <span class="n">docs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
                     <span class="n">topics</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                     <span class="n">timestamps</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
                                       <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]],</span>
                     <span class="n">nr_bins</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                     <span class="n">datetime_format</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                     <span class="n">evolution_tuning</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
                     <span class="n">global_tuning</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot; Create topics over time</span>

<span class="sd">    To create the topics over time, BERTopic needs to be already fitted once.</span>
<span class="sd">    From the fitted models, the c-TF-IDF representations are calculate at</span>
<span class="sd">    each timestamp t. Then, the c-TF-IDF representations at timestamp t are</span>
<span class="sd">    averaged with the global c-TF-IDF representations in order to fine-tune the</span>
<span class="sd">    local representations.</span>

<span class="sd">    NOTE:</span>
<span class="sd">        Make sure to use a limited number of unique timestamps (&lt;100) as the</span>
<span class="sd">        c-TF-IDF representation will be calculated at each single unique timestamp.</span>
<span class="sd">        Having a large number of unique timestamps can take some time to be calculated.</span>
<span class="sd">        Moreover, there aren&#39;t many use-cased where you would like to see the difference</span>
<span class="sd">        in topic representations over more than 100 different timestamps.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        docs: The documents you used when calling either `fit` or `fit_transform`</span>
<span class="sd">        topics: The topics that were returned when calling either `fit` or `fit_transform`</span>
<span class="sd">        timestamps: The timestamp of each document. This can be either a list of strings or ints.</span>
<span class="sd">                    If it is a list of strings, then the datetime format will be automatically</span>
<span class="sd">                    inferred. If it is a list of ints, then the documents will be ordered by</span>
<span class="sd">                    ascending order.</span>
<span class="sd">        nr_bins: The number of bins you want to create for the timestamps. The left interval will</span>
<span class="sd">                 be chosen as the timestamp. An additional column will be created with the</span>
<span class="sd">                 entire interval.</span>
<span class="sd">        datetime_format: The datetime format of the timestamps if they are strings, eg %d/%m/%Y.</span>
<span class="sd">                         Set this to None if you want to have it automatically detect the format.</span>
<span class="sd">                         See strftime documentation for more information on choices:</span>
<span class="sd">                         https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior.</span>
<span class="sd">        evolution_tuning: Fine-tune each topic representation at timestamp t by averaging its</span>
<span class="sd">                          c-TF-IDF matrix with the c-TF-IDF matrix at timestamp t-1. This creates</span>
<span class="sd">                          evolutionary topic representations.</span>
<span class="sd">        global_tuning: Fine-tune each topic representation at timestamp t by averaging its c-TF-IDF matrix</span>
<span class="sd">                   with the global c-TF-IDF matrix. Turn this off if you want to prevent words in</span>
<span class="sd">                   topic representations that could not be found in the documents at timestamp t.</span>

<span class="sd">    Returns:</span>
<span class="sd">        topics_over_time: A dataframe that contains the topic, words, and frequency of topic</span>
<span class="sd">                          at timestamp t.</span>

<span class="sd">    Usage:</span>

<span class="sd">    The timestamps variable represent the timestamp of each document. If you have over</span>
<span class="sd">    100 unique timestamps, it is advised to bin the timestamps as shown below:</span>

<span class="sd">    ```python</span>
<span class="sd">    from bertopic import BERTopic</span>
<span class="sd">    topic_model = BERTopic()</span>
<span class="sd">    topics, _ = topic_model.fit_transform(docs)</span>
<span class="sd">    topics_over_time = topic_model.topics_over_time(docs, topics, timestamps, nr_bins=20)</span>
<span class="sd">    ```</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
    <span class="n">check_documents_type</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>
    <span class="n">documents</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s2">&quot;Document&quot;</span><span class="p">:</span> <span class="n">docs</span><span class="p">,</span> <span class="s2">&quot;Topic&quot;</span><span class="p">:</span> <span class="n">topics</span><span class="p">,</span> <span class="s2">&quot;Timestamps&quot;</span><span class="p">:</span> <span class="n">timestamps</span><span class="p">})</span>
    <span class="n">global_c_tf_idf</span> <span class="o">=</span> <span class="n">normalize</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">c_tf_idf</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">norm</span><span class="o">=</span><span class="s1">&#39;l1&#39;</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="n">all_topics</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">documents</span><span class="o">.</span><span class="n">Topic</span><span class="o">.</span><span class="n">unique</span><span class="p">()))</span>
    <span class="n">all_topics_indices</span> <span class="o">=</span> <span class="p">{</span><span class="n">topic</span><span class="p">:</span> <span class="n">index</span> <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">topic</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">all_topics</span><span class="p">)}</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">timestamps</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">str</span><span class="p">):</span>
        <span class="n">infer_datetime_format</span> <span class="o">=</span> <span class="kc">True</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">datetime_format</span> <span class="k">else</span> <span class="kc">False</span>
        <span class="n">documents</span><span class="p">[</span><span class="s2">&quot;Timestamps&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">to_datetime</span><span class="p">(</span><span class="n">documents</span><span class="p">[</span><span class="s2">&quot;Timestamps&quot;</span><span class="p">],</span>
                                                 <span class="n">infer_datetime_format</span><span class="o">=</span><span class="n">infer_datetime_format</span><span class="p">,</span>
                                                 <span class="nb">format</span><span class="o">=</span><span class="n">datetime_format</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">nr_bins</span><span class="p">:</span>
        <span class="n">documents</span><span class="p">[</span><span class="s2">&quot;Bins&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">cut</span><span class="p">(</span><span class="n">documents</span><span class="o">.</span><span class="n">Timestamps</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="n">nr_bins</span><span class="p">)</span>
        <span class="n">documents</span><span class="p">[</span><span class="s2">&quot;Timestamps&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">documents</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">row</span><span class="p">:</span> <span class="n">row</span><span class="o">.</span><span class="n">Bins</span><span class="o">.</span><span class="n">left</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Sort documents in chronological order</span>
    <span class="n">documents</span> <span class="o">=</span> <span class="n">documents</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="s2">&quot;Timestamps&quot;</span><span class="p">)</span>
    <span class="n">timestamps</span> <span class="o">=</span> <span class="n">documents</span><span class="o">.</span><span class="n">Timestamps</span><span class="o">.</span><span class="n">unique</span><span class="p">()</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">timestamps</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">100</span><span class="p">:</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;There are more than 100 unique timestamps (i.e., </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">timestamps</span><span class="p">)</span><span class="si">}</span><span class="s2">) &quot;</span>
                      <span class="s2">&quot;which significantly slows down the application. Consider setting `nr_bins` &quot;</span>
                      <span class="s2">&quot;to a value lower than 100 to speed up calculation. &quot;</span><span class="p">)</span>

    <span class="c1"># For each unique timestamp, create topic representations</span>
    <span class="n">topics_over_time</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">timestamp</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">enumerate</span><span class="p">(</span><span class="n">timestamps</span><span class="p">),</span> <span class="n">disable</span><span class="o">=</span><span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">):</span>

        <span class="c1"># Calculate c-TF-IDF representation for a specific timestamp</span>
        <span class="n">selection</span> <span class="o">=</span> <span class="n">documents</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">documents</span><span class="o">.</span><span class="n">Timestamps</span> <span class="o">==</span> <span class="n">timestamp</span><span class="p">,</span> <span class="p">:]</span>
        <span class="n">documents_per_topic</span> <span class="o">=</span> <span class="n">selection</span><span class="o">.</span><span class="n">groupby</span><span class="p">([</span><span class="s1">&#39;Topic&#39;</span><span class="p">],</span> <span class="n">as_index</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">agg</span><span class="p">({</span><span class="s1">&#39;Document&#39;</span><span class="p">:</span> <span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">,</span>
                                                                                <span class="s2">&quot;Timestamps&quot;</span><span class="p">:</span> <span class="s2">&quot;count&quot;</span><span class="p">})</span>
        <span class="n">c_tf_idf</span><span class="p">,</span> <span class="n">words</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_c_tf_idf</span><span class="p">(</span><span class="n">documents_per_topic</span><span class="p">,</span> <span class="n">m</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">selection</span><span class="p">),</span> <span class="n">fit</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">global_tuning</span> <span class="ow">or</span> <span class="n">evolution_tuning</span><span class="p">:</span>
            <span class="n">c_tf_idf</span> <span class="o">=</span> <span class="n">normalize</span><span class="p">(</span><span class="n">c_tf_idf</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">norm</span><span class="o">=</span><span class="s1">&#39;l1&#39;</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Fine-tune the c-TF-IDF matrix at timestamp t by averaging it with the c-TF-IDF</span>
        <span class="c1"># matrix at timestamp t-1</span>
        <span class="k">if</span> <span class="n">evolution_tuning</span> <span class="ow">and</span> <span class="n">index</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">current_topics</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">documents_per_topic</span><span class="o">.</span><span class="n">Topic</span><span class="o">.</span><span class="n">values</span><span class="p">))</span>
            <span class="n">overlapping_topics</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">previous_topics</span><span class="p">)</span><span class="o">.</span><span class="n">intersection</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">current_topics</span><span class="p">))))</span>

            <span class="n">current_overlap_idx</span> <span class="o">=</span> <span class="p">[</span><span class="n">current_topics</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">topic</span><span class="p">)</span> <span class="k">for</span> <span class="n">topic</span> <span class="ow">in</span> <span class="n">overlapping_topics</span><span class="p">]</span>
            <span class="n">previous_overlap_idx</span> <span class="o">=</span> <span class="p">[</span><span class="n">previous_topics</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">topic</span><span class="p">)</span> <span class="k">for</span> <span class="n">topic</span> <span class="ow">in</span> <span class="n">overlapping_topics</span><span class="p">]</span>

            <span class="n">c_tf_idf</span><span class="o">.</span><span class="n">tolil</span><span class="p">()[</span><span class="n">current_overlap_idx</span><span class="p">]</span> <span class="o">=</span> <span class="p">((</span><span class="n">c_tf_idf</span><span class="p">[</span><span class="n">current_overlap_idx</span><span class="p">]</span> <span class="o">+</span>
                                                      <span class="n">previous_c_tf_idf</span><span class="p">[</span><span class="n">previous_overlap_idx</span><span class="p">])</span> <span class="o">/</span> <span class="mf">2.0</span><span class="p">)</span><span class="o">.</span><span class="n">tolil</span><span class="p">()</span>

        <span class="c1"># Fine-tune the timestamp c-TF-IDF representation based on the global c-TF-IDF representation</span>
        <span class="c1"># by simply taking the average of the two</span>
        <span class="k">if</span> <span class="n">global_tuning</span><span class="p">:</span>
            <span class="n">selected_topics</span> <span class="o">=</span> <span class="p">[</span><span class="n">all_topics_indices</span><span class="p">[</span><span class="n">topic</span><span class="p">]</span> <span class="k">for</span> <span class="n">topic</span> <span class="ow">in</span> <span class="n">documents_per_topic</span><span class="o">.</span><span class="n">Topic</span><span class="o">.</span><span class="n">values</span><span class="p">]</span>
            <span class="n">c_tf_idf</span> <span class="o">=</span> <span class="p">(</span><span class="n">global_c_tf_idf</span><span class="p">[</span><span class="n">selected_topics</span><span class="p">]</span> <span class="o">+</span> <span class="n">c_tf_idf</span><span class="p">)</span> <span class="o">/</span> <span class="mf">2.0</span>

        <span class="c1"># Extract the words per topic</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">documents_per_topic</span><span class="o">.</span><span class="n">Topic</span><span class="o">.</span><span class="n">unique</span><span class="p">()))</span>
        <span class="n">words_per_topic</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_extract_words_per_topic</span><span class="p">(</span><span class="n">words</span><span class="p">,</span> <span class="n">c_tf_idf</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
        <span class="n">topic_frequency</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">documents_per_topic</span><span class="o">.</span><span class="n">Timestamps</span><span class="o">.</span><span class="n">values</span><span class="p">,</span>
                                    <span class="n">index</span><span class="o">=</span><span class="n">documents_per_topic</span><span class="o">.</span><span class="n">Topic</span><span class="p">)</span><span class="o">.</span><span class="n">to_dict</span><span class="p">()</span>

        <span class="c1"># Fill dataframe with results</span>
        <span class="n">topics_at_timestamp</span> <span class="o">=</span> <span class="p">[(</span><span class="n">topic</span><span class="p">,</span>
                                <span class="s2">&quot;, &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="n">words</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">words</span> <span class="ow">in</span> <span class="n">values</span><span class="p">][:</span><span class="mi">5</span><span class="p">]),</span>
                                <span class="n">topic_frequency</span><span class="p">[</span><span class="n">topic</span><span class="p">],</span>
                                <span class="n">timestamp</span><span class="p">)</span> <span class="k">for</span> <span class="n">topic</span><span class="p">,</span> <span class="n">values</span> <span class="ow">in</span> <span class="n">words_per_topic</span><span class="o">.</span><span class="n">items</span><span class="p">()]</span>
        <span class="n">topics_over_time</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">topics_at_timestamp</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">evolution_tuning</span><span class="p">:</span>
            <span class="n">previous_topics</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">documents_per_topic</span><span class="o">.</span><span class="n">Topic</span><span class="o">.</span><span class="n">values</span><span class="p">))</span>
            <span class="n">previous_c_tf_idf</span> <span class="o">=</span> <span class="n">c_tf_idf</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">topics_over_time</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Topic&quot;</span><span class="p">,</span> <span class="s2">&quot;Words&quot;</span><span class="p">,</span> <span class="s2">&quot;Frequency&quot;</span><span class="p">,</span> <span class="s2">&quot;Timestamp&quot;</span><span class="p">])</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h2 id="bertopic._bertopic.BERTopic.topics_per_class" class="doc doc-heading">
<code class="highlight language-python"><span class="n">topics_per_class</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">docs</span><span class="p">,</span> <span class="n">topics</span><span class="p">,</span> <span class="n">classes</span><span class="p">,</span> <span class="n">global_tuning</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>


<a href="#bertopic._bertopic.BERTopic.topics_per_class" class="headerlink" title="Permanent link">&para;</a></h2>

    <div class="doc doc-contents ">

      <p>Create topics per class</p>
<p>To create the topics per class, BERTopic needs to be already fitted once.
From the fitted models, the c-TF-IDF representations are calculate at
each class c. Then, the c-TF-IDF representations at class c are
averaged with the global c-TF-IDF representations in order to fine-tune the
local representations. This can be turned off if the pure representation is
needed.</p>
<p>!!! note
    Make sure to use a limited number of unique classes (&lt;100) as the
    c-TF-IDF representation will be calculated at each single unique class.
    Having a large number of unique classes can take some time to be calculated.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>docs</code></td>
        <td><code>List[str]</code></td>
        <td><p>The documents you used when calling either <code>fit</code> or <code>fit_transform</code></p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>topics</code></td>
        <td><code>List[int]</code></td>
        <td><p>The topics that were returned when calling either <code>fit</code> or <code>fit_transform</code></p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>classes</code></td>
        <td><code>Union[List[int], List[str]]</code></td>
        <td><p>The class of each document. This can be either a list of strings or ints.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>global_tuning</code></td>
        <td><code>bool</code></td>
        <td><p>Fine-tune each topic representation at timestamp t by averaging its c-TF-IDF matrix
       with the global c-TF-IDF matrix. Turn this off if you want to prevent words in
       topic representations that could not be found in the documents at timestamp t.</p></td>
        <td><code>True</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>DataFrame</code></td>
      <td><p>topics_per_class: A dataframe that contains the topic, words, and frequency of topics
                  for each class.</p></td>
    </tr>
  </tbody>
</table>      <p>Usage:</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">bertopic</span> <span class="kn">import</span> <span class="n">BERTopic</span>
<span class="n">topic_model</span> <span class="o">=</span> <span class="n">BERTopic</span><span class="p">()</span>
<span class="n">topics</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">topic_model</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>
<span class="n">topics_per_class</span> <span class="o">=</span> <span class="n">topic_model</span><span class="o">.</span><span class="n">topics_per_class</span><span class="p">(</span><span class="n">docs</span><span class="p">,</span> <span class="n">topics</span><span class="p">,</span> <span class="n">classes</span><span class="p">)</span>
</code></pre></div>

        <details class="quote">
          <summary>Source code in <code>bertopic\_bertopic.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">topics_per_class</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                     <span class="n">docs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
                     <span class="n">topics</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                     <span class="n">classes</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]],</span>
                     <span class="n">global_tuning</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot; Create topics per class</span>

<span class="sd">    To create the topics per class, BERTopic needs to be already fitted once.</span>
<span class="sd">    From the fitted models, the c-TF-IDF representations are calculate at</span>
<span class="sd">    each class c. Then, the c-TF-IDF representations at class c are</span>
<span class="sd">    averaged with the global c-TF-IDF representations in order to fine-tune the</span>
<span class="sd">    local representations. This can be turned off if the pure representation is</span>
<span class="sd">    needed.</span>

<span class="sd">    NOTE:</span>
<span class="sd">        Make sure to use a limited number of unique classes (&lt;100) as the</span>
<span class="sd">        c-TF-IDF representation will be calculated at each single unique class.</span>
<span class="sd">        Having a large number of unique classes can take some time to be calculated.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        docs: The documents you used when calling either `fit` or `fit_transform`</span>
<span class="sd">        topics: The topics that were returned when calling either `fit` or `fit_transform`</span>
<span class="sd">        classes: The class of each document. This can be either a list of strings or ints.</span>
<span class="sd">        global_tuning: Fine-tune each topic representation at timestamp t by averaging its c-TF-IDF matrix</span>
<span class="sd">                   with the global c-TF-IDF matrix. Turn this off if you want to prevent words in</span>
<span class="sd">                   topic representations that could not be found in the documents at timestamp t.</span>

<span class="sd">    Returns:</span>
<span class="sd">        topics_per_class: A dataframe that contains the topic, words, and frequency of topics</span>
<span class="sd">                          for each class.</span>

<span class="sd">    Usage:</span>

<span class="sd">    ```python</span>
<span class="sd">    from bertopic import BERTopic</span>
<span class="sd">    topic_model = BERTopic()</span>
<span class="sd">    topics, _ = topic_model.fit_transform(docs)</span>
<span class="sd">    topics_per_class = topic_model.topics_per_class(docs, topics, classes)</span>
<span class="sd">    ```</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">documents</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s2">&quot;Document&quot;</span><span class="p">:</span> <span class="n">docs</span><span class="p">,</span> <span class="s2">&quot;Topic&quot;</span><span class="p">:</span> <span class="n">topics</span><span class="p">,</span> <span class="s2">&quot;Class&quot;</span><span class="p">:</span> <span class="n">classes</span><span class="p">})</span>
    <span class="n">global_c_tf_idf</span> <span class="o">=</span> <span class="n">normalize</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">c_tf_idf</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">norm</span><span class="o">=</span><span class="s1">&#39;l1&#39;</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="c1"># For each unique timestamp, create topic representations</span>
    <span class="n">topics_per_class</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">class_</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">enumerate</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">classes</span><span class="p">)),</span> <span class="n">disable</span><span class="o">=</span><span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">):</span>

        <span class="c1"># Calculate c-TF-IDF representation for a specific timestamp</span>
        <span class="n">selection</span> <span class="o">=</span> <span class="n">documents</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">documents</span><span class="o">.</span><span class="n">Class</span> <span class="o">==</span> <span class="n">class_</span><span class="p">,</span> <span class="p">:]</span>
        <span class="n">documents_per_topic</span> <span class="o">=</span> <span class="n">selection</span><span class="o">.</span><span class="n">groupby</span><span class="p">([</span><span class="s1">&#39;Topic&#39;</span><span class="p">],</span> <span class="n">as_index</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">agg</span><span class="p">({</span><span class="s1">&#39;Document&#39;</span><span class="p">:</span> <span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">,</span>
                                                                                <span class="s2">&quot;Class&quot;</span><span class="p">:</span> <span class="s2">&quot;count&quot;</span><span class="p">})</span>
        <span class="n">c_tf_idf</span><span class="p">,</span> <span class="n">words</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_c_tf_idf</span><span class="p">(</span><span class="n">documents_per_topic</span><span class="p">,</span> <span class="n">m</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">selection</span><span class="p">),</span> <span class="n">fit</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Fine-tune the timestamp c-TF-IDF representation based on the global c-TF-IDF representation</span>
        <span class="c1"># by simply taking the average of the two</span>
        <span class="k">if</span> <span class="n">global_tuning</span><span class="p">:</span>
            <span class="n">c_tf_idf</span> <span class="o">=</span> <span class="n">normalize</span><span class="p">(</span><span class="n">c_tf_idf</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">norm</span><span class="o">=</span><span class="s1">&#39;l1&#39;</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
            <span class="n">c_tf_idf</span> <span class="o">=</span> <span class="p">(</span><span class="n">global_c_tf_idf</span><span class="p">[</span><span class="n">documents_per_topic</span><span class="o">.</span><span class="n">Topic</span><span class="o">.</span><span class="n">values</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">c_tf_idf</span><span class="p">)</span> <span class="o">/</span> <span class="mf">2.0</span>

        <span class="c1"># Extract the words per topic</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">documents_per_topic</span><span class="o">.</span><span class="n">Topic</span><span class="o">.</span><span class="n">unique</span><span class="p">()))</span>
        <span class="n">words_per_topic</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_extract_words_per_topic</span><span class="p">(</span><span class="n">words</span><span class="p">,</span> <span class="n">c_tf_idf</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
        <span class="n">topic_frequency</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">documents_per_topic</span><span class="o">.</span><span class="n">Class</span><span class="o">.</span><span class="n">values</span><span class="p">,</span>
                                    <span class="n">index</span><span class="o">=</span><span class="n">documents_per_topic</span><span class="o">.</span><span class="n">Topic</span><span class="p">)</span><span class="o">.</span><span class="n">to_dict</span><span class="p">()</span>

        <span class="c1"># Fill dataframe with results</span>
        <span class="n">topics_at_class</span> <span class="o">=</span> <span class="p">[(</span><span class="n">topic</span><span class="p">,</span>
                            <span class="s2">&quot;, &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="n">words</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">words</span> <span class="ow">in</span> <span class="n">values</span><span class="p">][:</span><span class="mi">5</span><span class="p">]),</span>
                            <span class="n">topic_frequency</span><span class="p">[</span><span class="n">topic</span><span class="p">],</span>
                            <span class="n">class_</span><span class="p">)</span> <span class="k">for</span> <span class="n">topic</span><span class="p">,</span> <span class="n">values</span> <span class="ow">in</span> <span class="n">words_per_topic</span><span class="o">.</span><span class="n">items</span><span class="p">()]</span>
        <span class="n">topics_per_class</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">topics_at_class</span><span class="p">)</span>

    <span class="n">topics_per_class</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">topics_per_class</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Topic&quot;</span><span class="p">,</span> <span class="s2">&quot;Words&quot;</span><span class="p">,</span> <span class="s2">&quot;Frequency&quot;</span><span class="p">,</span> <span class="s2">&quot;Class&quot;</span><span class="p">])</span>

    <span class="k">return</span> <span class="n">topics_per_class</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h2 id="bertopic._bertopic.BERTopic.transform" class="doc doc-heading">
<code class="highlight language-python"><span class="n">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">documents</span><span class="p">,</span> <span class="n">embeddings</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>


<a href="#bertopic._bertopic.BERTopic.transform" class="headerlink" title="Permanent link">&para;</a></h2>

    <div class="doc doc-contents ">

      <p>After having fit a model, use transform to predict new instances</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>documents</code></td>
        <td><code>Union[str, List[str]]</code></td>
        <td><p>A single document or a list of documents to fit on</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>embeddings</code></td>
        <td><code>ndarray</code></td>
        <td><p>Pre-trained document embeddings. These can be used
        instead of the sentence-transformer model.</p></td>
        <td><code>None</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Tuple[List[int], numpy.ndarray]</code></td>
      <td><p>predictions: Topic predictions for each documents
probabilities: The topic probability distribution which is returned by default.
               If <code>calculate_probabilities</code> in BERTopic is set to False, then the
               probabilities are not calculated to speed up computation and
               decrease memory usage.</p></td>
    </tr>
  </tbody>
</table>      <p>Usage:</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">bertopic</span> <span class="kn">import</span> <span class="n">BERTopic</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">fetch_20newsgroups</span>

<span class="n">docs</span> <span class="o">=</span> <span class="n">fetch_20newsgroups</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="s1">&#39;all&#39;</span><span class="p">)[</span><span class="s1">&#39;data&#39;</span><span class="p">]</span>
<span class="n">topic_model</span> <span class="o">=</span> <span class="n">BERTopic</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>
<span class="n">topics</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">topic_model</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>
</code></pre></div>
<p>If you want to use your own embeddings:</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">bertopic</span> <span class="kn">import</span> <span class="n">BERTopic</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">fetch_20newsgroups</span>
<span class="kn">from</span> <span class="nn">sentence_transformers</span> <span class="kn">import</span> <span class="n">SentenceTransformer</span>

<span class="c1"># Create embeddings</span>
<span class="n">docs</span> <span class="o">=</span> <span class="n">fetch_20newsgroups</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="s1">&#39;all&#39;</span><span class="p">)[</span><span class="s1">&#39;data&#39;</span><span class="p">]</span>
<span class="n">sentence_model</span> <span class="o">=</span> <span class="n">SentenceTransformer</span><span class="p">(</span><span class="s2">&quot;distilbert-base-nli-mean-tokens&quot;</span><span class="p">)</span>
<span class="n">embeddings</span> <span class="o">=</span> <span class="n">sentence_model</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">docs</span><span class="p">,</span> <span class="n">show_progress_bar</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Create topic model</span>
<span class="n">topic_model</span> <span class="o">=</span> <span class="n">BERTopic</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">docs</span><span class="p">,</span> <span class="n">embeddings</span><span class="p">)</span>
<span class="n">topics</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">topic_model</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">docs</span><span class="p">,</span> <span class="n">embeddings</span><span class="p">)</span>
</code></pre></div>

        <details class="quote">
          <summary>Source code in <code>bertopic\_bertopic.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
              <span class="n">documents</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]],</span>
              <span class="n">embeddings</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot; After having fit a model, use transform to predict new instances</span>

<span class="sd">    Arguments:</span>
<span class="sd">        documents: A single document or a list of documents to fit on</span>
<span class="sd">        embeddings: Pre-trained document embeddings. These can be used</span>
<span class="sd">                    instead of the sentence-transformer model.</span>

<span class="sd">    Returns:</span>
<span class="sd">        predictions: Topic predictions for each documents</span>
<span class="sd">        probabilities: The topic probability distribution which is returned by default.</span>
<span class="sd">                       If `calculate_probabilities` in BERTopic is set to False, then the</span>
<span class="sd">                       probabilities are not calculated to speed up computation and</span>
<span class="sd">                       decrease memory usage.</span>

<span class="sd">    Usage:</span>

<span class="sd">    ```python</span>
<span class="sd">    from bertopic import BERTopic</span>
<span class="sd">    from sklearn.datasets import fetch_20newsgroups</span>

<span class="sd">    docs = fetch_20newsgroups(subset=&#39;all&#39;)[&#39;data&#39;]</span>
<span class="sd">    topic_model = BERTopic().fit(docs)</span>
<span class="sd">    topics, _ = topic_model.transform(docs)</span>
<span class="sd">    ```</span>

<span class="sd">    If you want to use your own embeddings:</span>

<span class="sd">    ```python</span>
<span class="sd">    from bertopic import BERTopic</span>
<span class="sd">    from sklearn.datasets import fetch_20newsgroups</span>
<span class="sd">    from sentence_transformers import SentenceTransformer</span>

<span class="sd">    # Create embeddings</span>
<span class="sd">    docs = fetch_20newsgroups(subset=&#39;all&#39;)[&#39;data&#39;]</span>
<span class="sd">    sentence_model = SentenceTransformer(&quot;distilbert-base-nli-mean-tokens&quot;)</span>
<span class="sd">    embeddings = sentence_model.encode(docs, show_progress_bar=True)</span>

<span class="sd">    # Create topic model</span>
<span class="sd">    topic_model = BERTopic().fit(docs, embeddings)</span>
<span class="sd">    topics, _ = topic_model.transform(docs, embeddings)</span>
<span class="sd">    ```</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
    <span class="n">check_embeddings_shape</span><span class="p">(</span><span class="n">embeddings</span><span class="p">,</span> <span class="n">documents</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">documents</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
        <span class="n">documents</span> <span class="o">=</span> <span class="p">[</span><span class="n">documents</span><span class="p">]</span>

    <span class="k">if</span> <span class="n">embeddings</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_extract_embeddings</span><span class="p">(</span><span class="n">documents</span><span class="p">,</span>
                                              <span class="n">method</span><span class="o">=</span><span class="s2">&quot;document&quot;</span><span class="p">,</span>
                                              <span class="n">verbose</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">)</span>

    <span class="n">umap_embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">umap_model</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span>
    <span class="n">predictions</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">hdbscan</span><span class="o">.</span><span class="n">approximate_predict</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hdbscan_model</span><span class="p">,</span> <span class="n">umap_embeddings</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">calculate_probabilities</span><span class="p">:</span>
        <span class="n">probabilities</span> <span class="o">=</span> <span class="n">hdbscan</span><span class="o">.</span><span class="n">membership_vector</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hdbscan_model</span><span class="p">,</span> <span class="n">umap_embeddings</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">probabilities</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mapped_topics</span><span class="p">:</span>
        <span class="n">predictions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_map_predictions</span><span class="p">(</span><span class="n">predictions</span><span class="p">)</span>
        <span class="n">probabilities</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_map_probabilities</span><span class="p">(</span><span class="n">probabilities</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predictions</span><span class="p">,</span> <span class="n">probabilities</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h2 id="bertopic._bertopic.BERTopic.update_topics" class="doc doc-heading">
<code class="highlight language-python"><span class="n">update_topics</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">docs</span><span class="p">,</span> <span class="n">topics</span><span class="p">,</span> <span class="n">n_gram_range</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">vectorizer_model</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>


<a href="#bertopic._bertopic.BERTopic.update_topics" class="headerlink" title="Permanent link">&para;</a></h2>

    <div class="doc doc-contents ">

      <p>Updates the topic representation by recalculating c-TF-IDF with the new
parameters as defined in this function.</p>
<p>When you have trained a model and viewed the topics and the words that represent them,
you might not be satisfied with the representation. Perhaps you forgot to remove
stop_words or you want to try out a different n_gram_range. This function allows you
to update the topic representation after they have been formed.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>docs</code></td>
        <td><code>List[str]</code></td>
        <td><p>The documents you used when calling either <code>fit</code> or <code>fit_transform</code></p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>topics</code></td>
        <td><code>List[int]</code></td>
        <td><p>The topics that were returned when calling either <code>fit</code> or <code>fit_transform</code></p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>n_gram_range</code></td>
        <td><code>Tuple[int, int]</code></td>
        <td><p>The n-gram range for the CountVectorizer.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>vectorizer_model</code></td>
        <td><code>CountVectorizer</code></td>
        <td><p>Pass in your own CountVectorizer from scikit-learn</p></td>
        <td><code>None</code></td>
      </tr>
  </tbody>
</table>      <p>Usage:</p>
<p>In order to update the topic representation, you will need to first fit the topic
model and extract topics from them. Based on these, you can update the representation:</p>
<div class="highlight"><pre><span></span><code><span class="n">topic_model</span><span class="o">.</span><span class="n">update_topics</span><span class="p">(</span><span class="n">docs</span><span class="p">,</span> <span class="n">topics</span><span class="p">,</span> <span class="n">n_gram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
</code></pre></div>
<p>YOu can also use a custom vectorizer to update the representation:</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">CountVectorizer</span>
<span class="n">vectorizer_model</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">stop_words</span><span class="o">=</span><span class="s2">&quot;english&quot;</span><span class="p">)</span>
<span class="n">topic_model</span><span class="o">.</span><span class="n">update_topics</span><span class="p">(</span><span class="n">docs</span><span class="p">,</span> <span class="n">topics</span><span class="p">,</span> <span class="n">vectorizer_model</span><span class="o">=</span><span class="n">vectorizer_model</span><span class="p">)</span>
</code></pre></div>

        <details class="quote">
          <summary>Source code in <code>bertopic\_bertopic.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">update_topics</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                  <span class="n">docs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
                  <span class="n">topics</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                  <span class="n">n_gram_range</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                  <span class="n">vectorizer_model</span><span class="p">:</span> <span class="n">CountVectorizer</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot; Updates the topic representation by recalculating c-TF-IDF with the new</span>
<span class="sd">    parameters as defined in this function.</span>

<span class="sd">    When you have trained a model and viewed the topics and the words that represent them,</span>
<span class="sd">    you might not be satisfied with the representation. Perhaps you forgot to remove</span>
<span class="sd">    stop_words or you want to try out a different n_gram_range. This function allows you</span>
<span class="sd">    to update the topic representation after they have been formed.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        docs: The documents you used when calling either `fit` or `fit_transform`</span>
<span class="sd">        topics: The topics that were returned when calling either `fit` or `fit_transform`</span>
<span class="sd">        n_gram_range: The n-gram range for the CountVectorizer.</span>
<span class="sd">        vectorizer_model: Pass in your own CountVectorizer from scikit-learn</span>

<span class="sd">    Usage:</span>

<span class="sd">    In order to update the topic representation, you will need to first fit the topic</span>
<span class="sd">    model and extract topics from them. Based on these, you can update the representation:</span>

<span class="sd">    ```python</span>
<span class="sd">    topic_model.update_topics(docs, topics, n_gram_range=(2, 3))</span>
<span class="sd">    ```</span>

<span class="sd">    YOu can also use a custom vectorizer to update the representation:</span>

<span class="sd">    ```python</span>
<span class="sd">    from sklearn.feature_extraction.text import CountVectorizer</span>
<span class="sd">    vectorizer_model = CountVectorizer(ngram_range=(1, 2), stop_words=&quot;english&quot;)</span>
<span class="sd">    topic_model.update_topics(docs, topics, vectorizer_model=vectorizer_model)</span>
<span class="sd">    ```</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">n_gram_range</span><span class="p">:</span>
        <span class="n">n_gram_range</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_gram_range</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">vectorizer_model</span> <span class="o">=</span> <span class="n">vectorizer_model</span> <span class="ow">or</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">ngram_range</span><span class="o">=</span><span class="n">n_gram_range</span><span class="p">)</span>

    <span class="n">documents</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s2">&quot;Document&quot;</span><span class="p">:</span> <span class="n">docs</span><span class="p">,</span> <span class="s2">&quot;Topic&quot;</span><span class="p">:</span> <span class="n">topics</span><span class="p">})</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_extract_topics</span><span class="p">(</span><span class="n">documents</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h2 id="bertopic._bertopic.BERTopic.visualize_barchart" class="doc doc-heading">
<code class="highlight language-python"><span class="n">visualize_barchart</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">topics</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">top_n_topics</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">n_words</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">800</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">600</span><span class="p">)</span></code>


<a href="#bertopic._bertopic.BERTopic.visualize_barchart" class="headerlink" title="Permanent link">&para;</a></h2>

    <div class="doc doc-contents ">

      <p>Visualize a barchart of selected topics</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>topics</code></td>
        <td><code>List[int]</code></td>
        <td><p>A selection of topics to visualize.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>top_n_topics</code></td>
        <td><code>int</code></td>
        <td><p>Only select the top n most frequent topics.</p></td>
        <td><code>6</code></td>
      </tr>
      <tr>
        <td><code>n_words</code></td>
        <td><code>int</code></td>
        <td><p>Number of words to show in a topic</p></td>
        <td><code>5</code></td>
      </tr>
      <tr>
        <td><code>width</code></td>
        <td><code>int</code></td>
        <td><p>The width of the figure.</p></td>
        <td><code>800</code></td>
      </tr>
      <tr>
        <td><code>height</code></td>
        <td><code>int</code></td>
        <td><p>The height of the figure.</p></td>
        <td><code>600</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Figure</code></td>
      <td><p>fig: A plotly figure</p></td>
    </tr>
  </tbody>
</table>      <p>Usage:</p>
<p>To visualize the barchart of selected topics
simply run:</p>
<div class="highlight"><pre><span></span><code><span class="n">topic_model</span><span class="o">.</span><span class="n">visualize_barchart</span><span class="p">()</span>
</code></pre></div>
<p>Or if you want to save the resulting figure:</p>
<div class="highlight"><pre><span></span><code><span class="n">fig</span> <span class="o">=</span> <span class="n">topic_model</span><span class="o">.</span><span class="n">visualize_barchart</span><span class="p">()</span>
<span class="n">fig</span><span class="o">.</span><span class="n">write_html</span><span class="p">(</span><span class="s2">&quot;path/to/file.html&quot;</span><span class="p">)</span>
</code></pre></div>

        <details class="quote">
          <summary>Source code in <code>bertopic\_bertopic.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">visualize_barchart</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                       <span class="n">topics</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                       <span class="n">top_n_topics</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">6</span><span class="p">,</span>
                       <span class="n">n_words</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span>
                       <span class="n">width</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">800</span><span class="p">,</span>
                       <span class="n">height</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">600</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">go</span><span class="o">.</span><span class="n">Figure</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot; Visualize a barchart of selected topics</span>

<span class="sd">    Arguments:</span>
<span class="sd">        topics: A selection of topics to visualize.</span>
<span class="sd">        top_n_topics: Only select the top n most frequent topics.</span>
<span class="sd">        n_words: Number of words to show in a topic</span>
<span class="sd">        width: The width of the figure.</span>
<span class="sd">        height: The height of the figure.</span>

<span class="sd">    Returns:</span>
<span class="sd">        fig: A plotly figure</span>

<span class="sd">    Usage:</span>

<span class="sd">    To visualize the barchart of selected topics</span>
<span class="sd">    simply run:</span>

<span class="sd">    ```python</span>
<span class="sd">    topic_model.visualize_barchart()</span>
<span class="sd">    ```</span>

<span class="sd">    Or if you want to save the resulting figure:</span>

<span class="sd">    ```python</span>
<span class="sd">    fig = topic_model.visualize_barchart()</span>
<span class="sd">    fig.write_html(&quot;path/to/file.html&quot;)</span>
<span class="sd">    ```</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">plotting</span><span class="o">.</span><span class="n">visualize_barchart</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                                       <span class="n">topics</span><span class="o">=</span><span class="n">topics</span><span class="p">,</span>
                                       <span class="n">top_n_topics</span><span class="o">=</span><span class="n">top_n_topics</span><span class="p">,</span>
                                       <span class="n">n_words</span><span class="o">=</span><span class="n">n_words</span><span class="p">,</span>
                                       <span class="n">width</span><span class="o">=</span><span class="n">width</span><span class="p">,</span>
                                       <span class="n">height</span><span class="o">=</span><span class="n">height</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h2 id="bertopic._bertopic.BERTopic.visualize_distribution" class="doc doc-heading">
<code class="highlight language-python"><span class="n">visualize_distribution</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">probabilities</span><span class="p">,</span> <span class="n">min_probability</span><span class="o">=</span><span class="mf">0.015</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">800</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">600</span><span class="p">)</span></code>


<a href="#bertopic._bertopic.BERTopic.visualize_distribution" class="headerlink" title="Permanent link">&para;</a></h2>

    <div class="doc doc-contents ">

      <p>Visualize the distribution of topic probabilities</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>probabilities</code></td>
        <td><code>ndarray</code></td>
        <td><p>An array of probability scores</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>min_probability</code></td>
        <td><code>float</code></td>
        <td><p>The minimum probability score to visualize.
             All others are ignored.</p></td>
        <td><code>0.015</code></td>
      </tr>
      <tr>
        <td><code>width</code></td>
        <td><code>int</code></td>
        <td><p>The width of the figure.</p></td>
        <td><code>800</code></td>
      </tr>
      <tr>
        <td><code>height</code></td>
        <td><code>int</code></td>
        <td><p>The height of the figure.</p></td>
        <td><code>600</code></td>
      </tr>
  </tbody>
</table>      <p>Usage:</p>
<p>Make sure to fit the model before and only input the
probabilities of a single document:</p>
<div class="highlight"><pre><span></span><code><span class="n">topic_model</span><span class="o">.</span><span class="n">visualize_distribution</span><span class="p">(</span><span class="n">probabilities</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</code></pre></div>
<p>Or if you want to save the resulting figure:</p>
<div class="highlight"><pre><span></span><code><span class="n">fig</span> <span class="o">=</span> <span class="n">topic_model</span><span class="o">.</span><span class="n">visualize_distribution</span><span class="p">(</span><span class="n">probabilities</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">fig</span><span class="o">.</span><span class="n">write_html</span><span class="p">(</span><span class="s2">&quot;path/to/file.html&quot;</span><span class="p">)</span>
</code></pre></div>

        <details class="quote">
          <summary>Source code in <code>bertopic\_bertopic.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">visualize_distribution</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                           <span class="n">probabilities</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
                           <span class="n">min_probability</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.015</span><span class="p">,</span>
                           <span class="n">width</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">800</span><span class="p">,</span>
                           <span class="n">height</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">600</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">go</span><span class="o">.</span><span class="n">Figure</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot; Visualize the distribution of topic probabilities</span>

<span class="sd">    Arguments:</span>
<span class="sd">        probabilities: An array of probability scores</span>
<span class="sd">        min_probability: The minimum probability score to visualize.</span>
<span class="sd">                         All others are ignored.</span>
<span class="sd">        width: The width of the figure.</span>
<span class="sd">        height: The height of the figure.</span>

<span class="sd">    Usage:</span>

<span class="sd">    Make sure to fit the model before and only input the</span>
<span class="sd">    probabilities of a single document:</span>

<span class="sd">    ```python</span>
<span class="sd">    topic_model.visualize_distribution(probabilities[0])</span>
<span class="sd">    ```</span>

<span class="sd">    Or if you want to save the resulting figure:</span>

<span class="sd">    ```python</span>
<span class="sd">    fig = topic_model.visualize_distribution(probabilities[0])</span>
<span class="sd">    fig.write_html(&quot;path/to/file.html&quot;)</span>
<span class="sd">    ```</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">plotting</span><span class="o">.</span><span class="n">visualize_distribution</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                                           <span class="n">probabilities</span><span class="o">=</span><span class="n">probabilities</span><span class="p">,</span>
                                           <span class="n">min_probability</span><span class="o">=</span><span class="n">min_probability</span><span class="p">,</span>
                                           <span class="n">width</span><span class="o">=</span><span class="n">width</span><span class="p">,</span>
                                           <span class="n">height</span><span class="o">=</span><span class="n">height</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h2 id="bertopic._bertopic.BERTopic.visualize_heatmap" class="doc doc-heading">
<code class="highlight language-python"><span class="n">visualize_heatmap</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">topics</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">top_n_topics</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">n_clusters</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">800</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">800</span><span class="p">)</span></code>


<a href="#bertopic._bertopic.BERTopic.visualize_heatmap" class="headerlink" title="Permanent link">&para;</a></h2>

    <div class="doc doc-contents ">

      <p>Visualize a heatmap of the topic's similarity matrix</p>
<p>Based on the cosine similarity matrix between topic embeddings,
a heatmap is created showing the similarity between topics.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>topics</code></td>
        <td><code>List[int]</code></td>
        <td><p>A selection of topics to visualize.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>top_n_topics</code></td>
        <td><code>int</code></td>
        <td><p>Only select the top n most frequent topics.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>n_clusters</code></td>
        <td><code>int</code></td>
        <td><p>Create n clusters and order the similarity
        matrix by those clusters.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>width</code></td>
        <td><code>int</code></td>
        <td><p>The width of the figure.</p></td>
        <td><code>800</code></td>
      </tr>
      <tr>
        <td><code>height</code></td>
        <td><code>int</code></td>
        <td><p>The height of the figure.</p></td>
        <td><code>800</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Figure</code></td>
      <td><p>fig: A plotly figure</p></td>
    </tr>
  </tbody>
</table>      <p>Usage:</p>
<p>To visualize the similarity matrix of
topics simply run:</p>
<div class="highlight"><pre><span></span><code><span class="n">topic_model</span><span class="o">.</span><span class="n">visualize_heatmap</span><span class="p">()</span>
</code></pre></div>
<p>Or if you want to save the resulting figure:</p>
<div class="highlight"><pre><span></span><code><span class="n">fig</span> <span class="o">=</span> <span class="n">topic_model</span><span class="o">.</span><span class="n">visualize_heatmap</span><span class="p">()</span>
<span class="n">fig</span><span class="o">.</span><span class="n">write_html</span><span class="p">(</span><span class="s2">&quot;path/to/file.html&quot;</span><span class="p">)</span>
</code></pre></div>

        <details class="quote">
          <summary>Source code in <code>bertopic\_bertopic.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">visualize_heatmap</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                      <span class="n">topics</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                      <span class="n">top_n_topics</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                      <span class="n">n_clusters</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                      <span class="n">width</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">800</span><span class="p">,</span>
                      <span class="n">height</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">800</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">go</span><span class="o">.</span><span class="n">Figure</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot; Visualize a heatmap of the topic&#39;s similarity matrix</span>

<span class="sd">    Based on the cosine similarity matrix between topic embeddings,</span>
<span class="sd">    a heatmap is created showing the similarity between topics.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        topics: A selection of topics to visualize.</span>
<span class="sd">        top_n_topics: Only select the top n most frequent topics.</span>
<span class="sd">        n_clusters: Create n clusters and order the similarity</span>
<span class="sd">                    matrix by those clusters.</span>
<span class="sd">        width: The width of the figure.</span>
<span class="sd">        height: The height of the figure.</span>

<span class="sd">    Returns:</span>
<span class="sd">        fig: A plotly figure</span>

<span class="sd">    Usage:</span>

<span class="sd">    To visualize the similarity matrix of</span>
<span class="sd">    topics simply run:</span>

<span class="sd">    ```python</span>
<span class="sd">    topic_model.visualize_heatmap()</span>
<span class="sd">    ```</span>

<span class="sd">    Or if you want to save the resulting figure:</span>

<span class="sd">    ```python</span>
<span class="sd">    fig = topic_model.visualize_heatmap()</span>
<span class="sd">    fig.write_html(&quot;path/to/file.html&quot;)</span>
<span class="sd">    ```</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">plotting</span><span class="o">.</span><span class="n">visualize_heatmap</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                                      <span class="n">topics</span><span class="o">=</span><span class="n">topics</span><span class="p">,</span>
                                      <span class="n">top_n_topics</span><span class="o">=</span><span class="n">top_n_topics</span><span class="p">,</span>
                                      <span class="n">n_clusters</span><span class="o">=</span><span class="n">n_clusters</span><span class="p">,</span>
                                      <span class="n">width</span><span class="o">=</span><span class="n">width</span><span class="p">,</span>
                                      <span class="n">height</span><span class="o">=</span><span class="n">height</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h2 id="bertopic._bertopic.BERTopic.visualize_hierarchy" class="doc doc-heading">
<code class="highlight language-python"><span class="n">visualize_hierarchy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">orientation</span><span class="o">=</span><span class="s1">&#39;left&#39;</span><span class="p">,</span> <span class="n">topics</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">top_n_topics</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">600</span><span class="p">)</span></code>


<a href="#bertopic._bertopic.BERTopic.visualize_hierarchy" class="headerlink" title="Permanent link">&para;</a></h2>

    <div class="doc doc-contents ">

      <p>Visualize a hierarchical structure of the topics</p>
<p>A ward linkage function is used to perform the
hierarchical clustering based on the cosine distance
matrix between topic embeddings.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>orientation</code></td>
        <td><code>str</code></td>
        <td><p>The orientation of the figure.
         Either 'left' or 'bottom'</p></td>
        <td><code>&#39;left&#39;</code></td>
      </tr>
      <tr>
        <td><code>topics</code></td>
        <td><code>List[int]</code></td>
        <td><p>A selection of topics to visualize</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>top_n_topics</code></td>
        <td><code>int</code></td>
        <td><p>Only select the top n most frequent topics</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>width</code></td>
        <td><code>int</code></td>
        <td><p>The width of the figure.</p></td>
        <td><code>1000</code></td>
      </tr>
      <tr>
        <td><code>height</code></td>
        <td><code>int</code></td>
        <td><p>The height of the figure.</p></td>
        <td><code>600</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Figure</code></td>
      <td><p>fig: A plotly figure</p></td>
    </tr>
  </tbody>
</table>      <p>Usage:</p>
<p>To visualize the hierarchical structure of
topics simply run:</p>
<div class="highlight"><pre><span></span><code><span class="n">topic_model</span><span class="o">.</span><span class="n">visualize_hierarchy</span><span class="p">()</span>
</code></pre></div>
<p>Or if you want to save the resulting figure:</p>
<div class="highlight"><pre><span></span><code><span class="n">fig</span> <span class="o">=</span> <span class="n">topic_model</span><span class="o">.</span><span class="n">visualize_hierarchy</span><span class="p">()</span>
<span class="n">fig</span><span class="o">.</span><span class="n">write_html</span><span class="p">(</span><span class="s2">&quot;path/to/file.html&quot;</span><span class="p">)</span>
</code></pre></div>

        <details class="quote">
          <summary>Source code in <code>bertopic\_bertopic.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">visualize_hierarchy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                        <span class="n">orientation</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;left&quot;</span><span class="p">,</span>
                        <span class="n">topics</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                        <span class="n">top_n_topics</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                        <span class="n">width</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
                        <span class="n">height</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">600</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">go</span><span class="o">.</span><span class="n">Figure</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot; Visualize a hierarchical structure of the topics</span>

<span class="sd">    A ward linkage function is used to perform the</span>
<span class="sd">    hierarchical clustering based on the cosine distance</span>
<span class="sd">    matrix between topic embeddings.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        orientation: The orientation of the figure.</span>
<span class="sd">                     Either &#39;left&#39; or &#39;bottom&#39;</span>
<span class="sd">        topics: A selection of topics to visualize</span>
<span class="sd">        top_n_topics: Only select the top n most frequent topics</span>
<span class="sd">        width: The width of the figure.</span>
<span class="sd">        height: The height of the figure.</span>

<span class="sd">    Returns:</span>
<span class="sd">        fig: A plotly figure</span>

<span class="sd">    Usage:</span>

<span class="sd">    To visualize the hierarchical structure of</span>
<span class="sd">    topics simply run:</span>

<span class="sd">    ```python</span>
<span class="sd">    topic_model.visualize_hierarchy()</span>
<span class="sd">    ```</span>

<span class="sd">    Or if you want to save the resulting figure:</span>

<span class="sd">    ```python</span>
<span class="sd">    fig = topic_model.visualize_hierarchy()</span>
<span class="sd">    fig.write_html(&quot;path/to/file.html&quot;)</span>
<span class="sd">    ```</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">plotting</span><span class="o">.</span><span class="n">visualize_hierarchy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                                        <span class="n">orientation</span><span class="o">=</span><span class="n">orientation</span><span class="p">,</span>
                                        <span class="n">topics</span><span class="o">=</span><span class="n">topics</span><span class="p">,</span>
                                        <span class="n">top_n_topics</span><span class="o">=</span><span class="n">top_n_topics</span><span class="p">,</span>
                                        <span class="n">width</span><span class="o">=</span><span class="n">width</span><span class="p">,</span>
                                        <span class="n">height</span><span class="o">=</span><span class="n">height</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h2 id="bertopic._bertopic.BERTopic.visualize_term_rank" class="doc doc-heading">
<code class="highlight language-python"><span class="n">visualize_term_rank</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">topics</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">log_scale</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">800</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">500</span><span class="p">)</span></code>


<a href="#bertopic._bertopic.BERTopic.visualize_term_rank" class="headerlink" title="Permanent link">&para;</a></h2>

    <div class="doc doc-contents ">

      <p>Visualize the ranks of all terms across all topics</p>
<p>Each topic is represented by a set of words. These words, however,
do not all equally represent the topic. This visualization shows
how many words are needed to represent a topic and at which point
the beneficial effect of adding words starts to decline.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>topics</code></td>
        <td><code>List[int]</code></td>
        <td><p>A selection of topics to visualize. These will be colored
    red where all others will be colored black.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>log_scale</code></td>
        <td><code>bool</code></td>
        <td><p>Whether to represent the ranking on a log scale</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>width</code></td>
        <td><code>int</code></td>
        <td><p>The width of the figure.</p></td>
        <td><code>800</code></td>
      </tr>
      <tr>
        <td><code>height</code></td>
        <td><code>int</code></td>
        <td><p>The height of the figure.</p></td>
        <td><code>500</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Figure</code></td>
      <td><p>fig: A plotly figure</p></td>
    </tr>
  </tbody>
</table>      <p>Usage:</p>
<p>To visualize the ranks of all words across
all topics simply run:</p>
<div class="highlight"><pre><span></span><code><span class="n">topic_model</span><span class="o">.</span><span class="n">visualize_word_rank</span><span class="p">()</span>
</code></pre></div>
<p>Or if you want to save the resulting figure:</p>
<div class="highlight"><pre><span></span><code><span class="n">fig</span> <span class="o">=</span> <span class="n">topic_model</span><span class="o">.</span><span class="n">visualize_word_rank</span><span class="p">()</span>
<span class="n">fig</span><span class="o">.</span><span class="n">write_html</span><span class="p">(</span><span class="s2">&quot;path/to/file.html&quot;</span><span class="p">)</span>
</code></pre></div>
<p>Reference:</p>
<p>This visualization was heavily inspired by the
"Term Probability Decline" visualization found in an
analysis by the amazing <a href="https://tmtoolkit.readthedocs.io/">tmtoolkit</a>.
Reference to that specific analysis can be found
<a href="https://wzbsocialsciencecenter.github.io/tm_corona/tm_analysis.html">here</a>.</p>

        <details class="quote">
          <summary>Source code in <code>bertopic\_bertopic.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">visualize_term_rank</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                        <span class="n">topics</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                        <span class="n">log_scale</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                        <span class="n">width</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">800</span><span class="p">,</span>
                        <span class="n">height</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">500</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">go</span><span class="o">.</span><span class="n">Figure</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot; Visualize the ranks of all terms across all topics</span>

<span class="sd">    Each topic is represented by a set of words. These words, however,</span>
<span class="sd">    do not all equally represent the topic. This visualization shows</span>
<span class="sd">    how many words are needed to represent a topic and at which point</span>
<span class="sd">    the beneficial effect of adding words starts to decline.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        topics: A selection of topics to visualize. These will be colored</span>
<span class="sd">                red where all others will be colored black.</span>
<span class="sd">        log_scale: Whether to represent the ranking on a log scale</span>
<span class="sd">        width: The width of the figure.</span>
<span class="sd">        height: The height of the figure.</span>

<span class="sd">    Returns:</span>
<span class="sd">        fig: A plotly figure</span>

<span class="sd">    Usage:</span>

<span class="sd">    To visualize the ranks of all words across</span>
<span class="sd">    all topics simply run:</span>

<span class="sd">    ```python</span>
<span class="sd">    topic_model.visualize_word_rank()</span>
<span class="sd">    ```</span>

<span class="sd">    Or if you want to save the resulting figure:</span>

<span class="sd">    ```python</span>
<span class="sd">    fig = topic_model.visualize_word_rank()</span>
<span class="sd">    fig.write_html(&quot;path/to/file.html&quot;)</span>
<span class="sd">    ```</span>

<span class="sd">    Reference:</span>

<span class="sd">    This visualization was heavily inspired by the</span>
<span class="sd">    &quot;Term Probability Decline&quot; visualization found in an</span>
<span class="sd">    analysis by the amazing [tmtoolkit](https://tmtoolkit.readthedocs.io/).</span>
<span class="sd">    Reference to that specific analysis can be found</span>
<span class="sd">    [here](https://wzbsocialsciencecenter.github.io/tm_corona/tm_analysis.html).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">plotting</span><span class="o">.</span><span class="n">visualize_term_rank</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                                        <span class="n">topics</span><span class="o">=</span><span class="n">topics</span><span class="p">,</span>
                                        <span class="n">log_scale</span><span class="o">=</span><span class="n">log_scale</span><span class="p">,</span>
                                        <span class="n">width</span><span class="o">=</span><span class="n">width</span><span class="p">,</span>
                                        <span class="n">height</span><span class="o">=</span><span class="n">height</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h2 id="bertopic._bertopic.BERTopic.visualize_topics" class="doc doc-heading">
<code class="highlight language-python"><span class="n">visualize_topics</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">topics</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">top_n_topics</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">650</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">650</span><span class="p">)</span></code>


<a href="#bertopic._bertopic.BERTopic.visualize_topics" class="headerlink" title="Permanent link">&para;</a></h2>

    <div class="doc doc-contents ">

      <p>Visualize topics, their sizes, and their corresponding words</p>
<p>This visualization is highly inspired by LDAvis, a great visualization
technique typically reserved for LDA.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>topics</code></td>
        <td><code>List[int]</code></td>
        <td><p>A selection of topics to visualize</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>top_n_topics</code></td>
        <td><code>int</code></td>
        <td><p>Only select the top n most frequent topics</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>width</code></td>
        <td><code>int</code></td>
        <td><p>The width of the figure.</p></td>
        <td><code>650</code></td>
      </tr>
      <tr>
        <td><code>height</code></td>
        <td><code>int</code></td>
        <td><p>The height of the figure.</p></td>
        <td><code>650</code></td>
      </tr>
  </tbody>
</table>      <p>Usage:</p>
<p>To visualize the topics simply run:</p>
<div class="highlight"><pre><span></span><code><span class="n">topic_model</span><span class="o">.</span><span class="n">visualize_topics</span><span class="p">()</span>
</code></pre></div>
<p>Or if you want to save the resulting figure:</p>
<div class="highlight"><pre><span></span><code><span class="n">fig</span> <span class="o">=</span> <span class="n">topic_model</span><span class="o">.</span><span class="n">visualize_topics</span><span class="p">()</span>
<span class="n">fig</span><span class="o">.</span><span class="n">write_html</span><span class="p">(</span><span class="s2">&quot;path/to/file.html&quot;</span><span class="p">)</span>
</code></pre></div>

        <details class="quote">
          <summary>Source code in <code>bertopic\_bertopic.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">visualize_topics</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                     <span class="n">topics</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                     <span class="n">top_n_topics</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                     <span class="n">width</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">650</span><span class="p">,</span>
                     <span class="n">height</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">650</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">go</span><span class="o">.</span><span class="n">Figure</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot; Visualize topics, their sizes, and their corresponding words</span>

<span class="sd">    This visualization is highly inspired by LDAvis, a great visualization</span>
<span class="sd">    technique typically reserved for LDA.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        topics: A selection of topics to visualize</span>
<span class="sd">        top_n_topics: Only select the top n most frequent topics</span>
<span class="sd">        width: The width of the figure.</span>
<span class="sd">        height: The height of the figure.</span>

<span class="sd">    Usage:</span>

<span class="sd">    To visualize the topics simply run:</span>

<span class="sd">    ```python</span>
<span class="sd">    topic_model.visualize_topics()</span>
<span class="sd">    ```</span>

<span class="sd">    Or if you want to save the resulting figure:</span>

<span class="sd">    ```python</span>
<span class="sd">    fig = topic_model.visualize_topics()</span>
<span class="sd">    fig.write_html(&quot;path/to/file.html&quot;)</span>
<span class="sd">    ```</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">plotting</span><span class="o">.</span><span class="n">visualize_topics</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                                     <span class="n">topics</span><span class="o">=</span><span class="n">topics</span><span class="p">,</span>
                                     <span class="n">top_n_topics</span><span class="o">=</span><span class="n">top_n_topics</span><span class="p">,</span>
                                     <span class="n">width</span><span class="o">=</span><span class="n">width</span><span class="p">,</span>
                                     <span class="n">height</span><span class="o">=</span><span class="n">height</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h2 id="bertopic._bertopic.BERTopic.visualize_topics_over_time" class="doc doc-heading">
<code class="highlight language-python"><span class="n">visualize_topics_over_time</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">topics_over_time</span><span class="p">,</span> <span class="n">top_n_topics</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">topics</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">1250</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">450</span><span class="p">)</span></code>


<a href="#bertopic._bertopic.BERTopic.visualize_topics_over_time" class="headerlink" title="Permanent link">&para;</a></h2>

    <div class="doc doc-contents ">

      <p>Visualize topics over time</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>topics_over_time</code></td>
        <td><code>DataFrame</code></td>
        <td><p>The topics you would like to be visualized with the
              corresponding topic representation</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>top_n_topics</code></td>
        <td><code>int</code></td>
        <td><p>To visualize the most frequent topics instead of all</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>topics</code></td>
        <td><code>List[int]</code></td>
        <td><p>Select which topics you would like to be visualized</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>width</code></td>
        <td><code>int</code></td>
        <td><p>The width of the figure.</p></td>
        <td><code>1250</code></td>
      </tr>
      <tr>
        <td><code>height</code></td>
        <td><code>int</code></td>
        <td><p>The height of the figure.</p></td>
        <td><code>450</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Figure</code></td>
      <td><p>A plotly.graph_objects.Figure including all traces</p></td>
    </tr>
  </tbody>
</table>      <p>Usage:</p>
<p>To visualize the topics over time, simply run:</p>
<div class="highlight"><pre><span></span><code><span class="n">topics_over_time</span> <span class="o">=</span> <span class="n">topic_model</span><span class="o">.</span><span class="n">topics_over_time</span><span class="p">(</span><span class="n">docs</span><span class="p">,</span> <span class="n">topics</span><span class="p">,</span> <span class="n">timestamps</span><span class="p">)</span>
<span class="n">topic_model</span><span class="o">.</span><span class="n">visualize_topics_over_time</span><span class="p">(</span><span class="n">topics_over_time</span><span class="p">)</span>
</code></pre></div>
<p>Or if you want to save the resulting figure:</p>
<div class="highlight"><pre><span></span><code><span class="n">fig</span> <span class="o">=</span> <span class="n">topic_model</span><span class="o">.</span><span class="n">visualize_topics_over_time</span><span class="p">(</span><span class="n">topics_over_time</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">write_html</span><span class="p">(</span><span class="s2">&quot;path/to/file.html&quot;</span><span class="p">)</span>
</code></pre></div>

        <details class="quote">
          <summary>Source code in <code>bertopic\_bertopic.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">visualize_topics_over_time</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                               <span class="n">topics_over_time</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span>
                               <span class="n">top_n_topics</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                               <span class="n">topics</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                               <span class="n">width</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1250</span><span class="p">,</span>
                               <span class="n">height</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">450</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">go</span><span class="o">.</span><span class="n">Figure</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot; Visualize topics over time</span>

<span class="sd">    Arguments:</span>
<span class="sd">        topics_over_time: The topics you would like to be visualized with the</span>
<span class="sd">                          corresponding topic representation</span>
<span class="sd">        top_n_topics: To visualize the most frequent topics instead of all</span>
<span class="sd">        topics: Select which topics you would like to be visualized</span>
<span class="sd">        width: The width of the figure.</span>
<span class="sd">        height: The height of the figure.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A plotly.graph_objects.Figure including all traces</span>

<span class="sd">    Usage:</span>

<span class="sd">    To visualize the topics over time, simply run:</span>

<span class="sd">    ```python</span>
<span class="sd">    topics_over_time = topic_model.topics_over_time(docs, topics, timestamps)</span>
<span class="sd">    topic_model.visualize_topics_over_time(topics_over_time)</span>
<span class="sd">    ```</span>

<span class="sd">    Or if you want to save the resulting figure:</span>

<span class="sd">    ```python</span>
<span class="sd">    fig = topic_model.visualize_topics_over_time(topics_over_time)</span>
<span class="sd">    fig.write_html(&quot;path/to/file.html&quot;)</span>
<span class="sd">    ```</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">plotting</span><span class="o">.</span><span class="n">visualize_topics_over_time</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                                               <span class="n">topics_over_time</span><span class="o">=</span><span class="n">topics_over_time</span><span class="p">,</span>
                                               <span class="n">top_n_topics</span><span class="o">=</span><span class="n">top_n_topics</span><span class="p">,</span>
                                               <span class="n">topics</span><span class="o">=</span><span class="n">topics</span><span class="p">,</span>
                                               <span class="n">width</span><span class="o">=</span><span class="n">width</span><span class="p">,</span>
                                               <span class="n">height</span><span class="o">=</span><span class="n">height</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h2 id="bertopic._bertopic.BERTopic.visualize_topics_per_class" class="doc doc-heading">
<code class="highlight language-python"><span class="n">visualize_topics_per_class</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">topics_per_class</span><span class="p">,</span> <span class="n">top_n_topics</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">topics</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">1250</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">900</span><span class="p">)</span></code>


<a href="#bertopic._bertopic.BERTopic.visualize_topics_per_class" class="headerlink" title="Permanent link">&para;</a></h2>

    <div class="doc doc-contents ">

      <p>Visualize topics per class</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>topics_per_class</code></td>
        <td><code>DataFrame</code></td>
        <td><p>The topics you would like to be visualized with the
              corresponding topic representation</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>top_n_topics</code></td>
        <td><code>int</code></td>
        <td><p>To visualize the most frequent topics instead of all</p></td>
        <td><code>10</code></td>
      </tr>
      <tr>
        <td><code>topics</code></td>
        <td><code>List[int]</code></td>
        <td><p>Select which topics you would like to be visualized</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>width</code></td>
        <td><code>int</code></td>
        <td><p>The width of the figure.</p></td>
        <td><code>1250</code></td>
      </tr>
      <tr>
        <td><code>height</code></td>
        <td><code>int</code></td>
        <td><p>The height of the figure.</p></td>
        <td><code>900</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Figure</code></td>
      <td><p>A plotly.graph_objects.Figure including all traces</p></td>
    </tr>
  </tbody>
</table>      <p>Usage:</p>
<p>To visualize the topics per class, simply run:</p>
<div class="highlight"><pre><span></span><code><span class="n">topics_per_class</span> <span class="o">=</span> <span class="n">topic_model</span><span class="o">.</span><span class="n">topics_per_class</span><span class="p">(</span><span class="n">docs</span><span class="p">,</span> <span class="n">topics</span><span class="p">,</span> <span class="n">classes</span><span class="p">)</span>
<span class="n">topic_model</span><span class="o">.</span><span class="n">visualize_topics_per_class</span><span class="p">(</span><span class="n">topics_per_class</span><span class="p">)</span>
</code></pre></div>
<p>Or if you want to save the resulting figure:</p>
<div class="highlight"><pre><span></span><code><span class="n">fig</span> <span class="o">=</span> <span class="n">topic_model</span><span class="o">.</span><span class="n">visualize_topics_per_class</span><span class="p">(</span><span class="n">topics_per_class</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">write_html</span><span class="p">(</span><span class="s2">&quot;path/to/file.html&quot;</span><span class="p">)</span>
</code></pre></div>

        <details class="quote">
          <summary>Source code in <code>bertopic\_bertopic.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">visualize_topics_per_class</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                               <span class="n">topics_per_class</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span>
                               <span class="n">top_n_topics</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
                               <span class="n">topics</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                               <span class="n">width</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1250</span><span class="p">,</span>
                               <span class="n">height</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">900</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">go</span><span class="o">.</span><span class="n">Figure</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot; Visualize topics per class</span>

<span class="sd">    Arguments:</span>
<span class="sd">        topics_per_class: The topics you would like to be visualized with the</span>
<span class="sd">                          corresponding topic representation</span>
<span class="sd">        top_n_topics: To visualize the most frequent topics instead of all</span>
<span class="sd">        topics: Select which topics you would like to be visualized</span>
<span class="sd">        width: The width of the figure.</span>
<span class="sd">        height: The height of the figure.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A plotly.graph_objects.Figure including all traces</span>

<span class="sd">    Usage:</span>

<span class="sd">    To visualize the topics per class, simply run:</span>

<span class="sd">    ```python</span>
<span class="sd">    topics_per_class = topic_model.topics_per_class(docs, topics, classes)</span>
<span class="sd">    topic_model.visualize_topics_per_class(topics_per_class)</span>
<span class="sd">    ```</span>

<span class="sd">    Or if you want to save the resulting figure:</span>

<span class="sd">    ```python</span>
<span class="sd">    fig = topic_model.visualize_topics_per_class(topics_per_class)</span>
<span class="sd">    fig.write_html(&quot;path/to/file.html&quot;)</span>
<span class="sd">    ```</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">plotting</span><span class="o">.</span><span class="n">visualize_topics_per_class</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                                               <span class="n">topics_per_class</span><span class="o">=</span><span class="n">topics_per_class</span><span class="p">,</span>
                                               <span class="n">top_n_topics</span><span class="o">=</span><span class="n">top_n_topics</span><span class="p">,</span>
                                               <span class="n">topics</span><span class="o">=</span><span class="n">topics</span><span class="p">,</span>
                                               <span class="n">width</span><span class="o">=</span><span class="n">width</span><span class="p">,</span>
                                               <span class="n">height</span><span class="o">=</span><span class="n">height</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>
                
              
              
                


              
            </article>
          </div>
        </div>
        
      </main>
      
        
<footer class="md-footer">
  
    <nav class="md-footer__inner md-grid" aria-label="Footer">
      
        <a href="../tutorial/topicsovertime/topicsovertime.html" class="md-footer__link md-footer__link--prev" rel="prev">
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
          </div>
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Previous
              </span>
              Dynamic Topic Modeling
            </div>
          </div>
        </a>
      
      
        <a href="ctfidf.html" class="md-footer__link md-footer__link--next" rel="next">
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Next
              </span>
              cTFIDF
            </div>
          </div>
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4z"/></svg>
          </div>
        </a>
      
    </nav>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
          <div class="md-footer-copyright__highlight">
            Copyright &copy; 2021 Maintained by <a href="https://github.com/MaartenGr">Maarten</a>.
          </div>
        
        Made with
        <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
          Material for MkDocs
        </a>
        
      </div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    <script id="__config" type="application/json">{"base": "..", "features": [], "translations": {"clipboard.copy": "Copy to clipboard", "clipboard.copied": "Copied to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.placeholder": "Type to start searching", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.term.missing": "Missing"}, "search": "../assets/javascripts/workers/search.d351de03.min.js", "version": null}</script>
    
    
      <script src="../assets/javascripts/bundle.34eae1b6.min.js"></script>
      
    
  </body>
</html>