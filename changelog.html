
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Leveraging BERT and a class-based TF-IDF to create easily interpretable topics.">
      
      
        <meta name="author" content="Maarten P. Grootendorst">
      
      
        <link rel="canonical" href="https://maartengr.github.io/BERTopic/changelog.html">
      
      
        <link rel="prev" href="api/plotting/topics_per_class.html">
      
      
      
      <link rel="icon" href="icon.png">
      <meta name="generator" content="mkdocs-1.5.3, mkdocs-material-9.4.6">
    
    
      
        <title>Changelog - BERTopic</title>
      
    
    
      <link rel="stylesheet" href="assets/stylesheets/main.35e1ed30.min.css">
      
        
        <link rel="stylesheet" href="assets/stylesheets/palette.356b1318.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Ubuntu:300,300i,400,400i,700,700i%7CUbuntu+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Ubuntu";--md-code-font:"Ubuntu Mono"}</style>
      
    
    
      <link rel="stylesheet" href="assets/_mkdocstrings.css">
    
      <link rel="stylesheet" href="stylesheets/extra.css">
    
    <script>__md_scope=new URL(".",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
      
        <meta  property="og:type"  content="website" >
      
        <meta  property="og:title"  content="Changelog - BERTopic" >
      
        <meta  property="og:description"  content="Leveraging BERT and a class-based TF-IDF to create easily interpretable topics." >
      
        <meta  property="og:image"  content="https://maartengr.github.io/BERTopic/assets/images/social/changelog.png" >
      
        <meta  property="og:image:type"  content="image/png" >
      
        <meta  property="og:image:width"  content="1200" >
      
        <meta  property="og:image:height"  content="630" >
      
        <meta  property="og:url"  content="https://maartengr.github.io/BERTopic/changelog.html" >
      
        <meta  name="twitter:card"  content="summary_large_image" >
      
        <meta  name="twitter:title"  content="Changelog - BERTopic" >
      
        <meta  name="twitter:description"  content="Leveraging BERT and a class-based TF-IDF to create easily interpretable topics." >
      
        <meta  name="twitter:image"  content="https://maartengr.github.io/BERTopic/assets/images/social/changelog.png" >
      
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="black" data-md-color-primary="custom" data-md-color-accent="indigo">
  
    
    
      <script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#changelog" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="index.html" title="BERTopic" class="md-header__button md-logo" aria-label="BERTopic" data-md-component="logo">
      
  <img src="img/icon.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            BERTopic
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Changelog
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="black" data-md-color-primary="custom" data-md-color-accent="indigo"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_2" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 7a5 5 0 0 1 5 5 5 5 0 0 1-5 5 5 5 0 0 1-5-5 5 5 0 0 1 5-5m0 2a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0-7 2.39 3.42C13.65 5.15 12.84 5 12 5c-.84 0-1.65.15-2.39.42L12 2M3.34 7l4.16-.35A7.2 7.2 0 0 0 5.94 8.5c-.44.74-.69 1.5-.83 2.29L3.34 7m.02 10 1.76-3.77a7.131 7.131 0 0 0 2.38 4.14L3.36 17M20.65 7l-1.77 3.79a7.023 7.023 0 0 0-2.38-4.15l4.15.36m-.01 10-4.14.36c.59-.51 1.12-1.14 1.54-1.86.42-.73.69-1.5.83-2.29L20.64 17M12 22l-2.41-3.44c.74.27 1.55.44 2.41.44.82 0 1.63-.17 2.37-.44L12 22Z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="black" data-md-color-accent="indigo"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_2">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3 3.19.09m3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95 2.06.05m-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31Z"/></svg>
      </label>
    
  
</form>
      
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/MaartenGr/BERTopic" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.4.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    <li class="md-tabs__item">
      <a href="index.html" class="md-tabs__link">
        
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="algorithm/algorithm.html" class="md-tabs__link">
        
  
    
  
  The Algorithm

      </a>
    </li>
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="getting_started/quickstart/quickstart.html" class="md-tabs__link">
          
  
  Getting Started

        </a>
      </li>
    
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="faq.html" class="md-tabs__link">
        
  
    
  
  FAQ

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="usecases.html" class="md-tabs__link">
        
  
    
  
  Use Cases

      </a>
    </li>
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="api/bertopic.html" class="md-tabs__link">
          
  
  API

        </a>
      </li>
    
  

      
        
  
  
    
  
  
    <li class="md-tabs__item md-tabs__item--active">
      <a href="changelog.html" class="md-tabs__link">
        
  
    
  
  Changelog

      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
                
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" hidden>
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="index.html" title="BERTopic" class="md-nav__button md-logo" aria-label="BERTopic" data-md-component="logo">
      
  <img src="img/icon.png" alt="logo">

    </a>
    BERTopic
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/MaartenGr/BERTopic" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.4.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
    <li class="md-nav__item">
      <a href="index.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
    <li class="md-nav__item">
      <a href="algorithm/algorithm.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    The Algorithm
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="">
            
  
  <span class="md-ellipsis">
    Getting Started
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Getting Started
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="getting_started/quickstart/quickstart.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Quick Start
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="getting_started/serialization/serialization.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Serialization
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="getting_started/search/search.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Search Topics
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="getting_started/best_practices/best_practices.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Best Practices
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_5" >
        
          
          <label class="md-nav__link" for="__nav_3_5" id="__nav_3_5_label" tabindex="">
            
  
  <span class="md-ellipsis">
    In-depth
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_5">
            <span class="md-nav__icon md-icon"></span>
            In-depth
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_5_1" >
        
          
          <label class="md-nav__link" for="__nav_3_5_1" id="__nav_3_5_1_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Visualizations
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_5_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_5_1">
            <span class="md-nav__icon md-icon"></span>
            Visualizations
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="getting_started/visualization/visualize_topics.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Topics
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="getting_started/visualization/visualize_documents.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Documents
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="getting_started/visualization/visualize_terms.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Terms
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="getting_started/visualization/visualize_hierarchy.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Hierarchy
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_5_2" >
        
          
          <label class="md-nav__link" for="__nav_3_5_2" id="__nav_3_5_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Update Topics
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_5_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_5_2">
            <span class="md-nav__icon md-icon"></span>
            Update Topics
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="getting_started/topicreduction/topicreduction.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Topic Reduction
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="getting_started/topicrepresentation/topicrepresentation.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Update Topic Representations
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="getting_started/outlier_reduction/outlier_reduction.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Outlier reduction
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="getting_started/parameter%20tuning/parametertuning.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Parameter tuning
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="getting_started/tips_and_tricks/tips_and_tricks.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Tips & Tricks
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_6" >
        
          
          <label class="md-nav__link" for="__nav_3_6" id="__nav_3_6_label" tabindex="">
            
  
  <span class="md-ellipsis">
    Sub-models
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_6">
            <span class="md-nav__icon md-icon"></span>
            Sub-models
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="getting_started/embeddings/embeddings.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    1. Embeddings
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="getting_started/dim_reduction/dim_reduction.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2. Dimensionality Reduction
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="getting_started/clustering/clustering.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    3. Clustering
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="getting_started/vectorizers/vectorizers.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    4. Vectorizers
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="getting_started/ctfidf/ctfidf.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    5. c-TF-IDF
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_6_6" >
        
          
          <label class="md-nav__link" for="__nav_3_6_6" id="__nav_3_6_6_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    6. Fine-tune Topics
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_6_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_6_6">
            <span class="md-nav__icon md-icon"></span>
            6. Fine-tune Topics
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="getting_started/representation/representation.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    6A. Representation Models
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="getting_started/representation/llm.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    6B. LLM & Generative AI
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="getting_started/multiaspect/multiaspect.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    6C. Multiple Representations
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_7" >
        
          
          <label class="md-nav__link" for="__nav_3_7" id="__nav_3_7_label" tabindex="">
            
  
  <span class="md-ellipsis">
    Variations
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_7">
            <span class="md-nav__icon md-icon"></span>
            Variations
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="getting_started/topicsovertime/topicsovertime.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Dynamic Topic Modeling
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="getting_started/hierarchicaltopics/hierarchicaltopics.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Hierarchical Topic Modeling
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="getting_started/multimodal/multimodal.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Multimodal Topic Modeling
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="getting_started/online/online.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Online Topic Modeling
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="getting_started/merge/merge.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Merge Multiple Models
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_7_6" >
        
          
          <label class="md-nav__link" for="__nav_3_7_6" id="__nav_3_7_6_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    (semi)-supervised
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_7_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_7_6">
            <span class="md-nav__icon md-icon"></span>
            (semi)-supervised
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="getting_started/semisupervised/semisupervised.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Semi-supervised Topic Modeling
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="getting_started/supervised/supervised.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Supervised Topic Modeling
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="getting_started/manual/manual.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Manual Topic Modeling
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="getting_started/guided/guided.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Guided Topic Modeling
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="getting_started/zeroshot/zeroshot.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Zero-shot Topic Modeling
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="getting_started/distribution/distribution.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Topic Distributions
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="getting_started/topicsperclass/topicsperclass.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Topics per Class
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="getting_started/seed_words/seed_words.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Seed Words
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
    <li class="md-nav__item">
      <a href="faq.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    FAQ
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
    <li class="md-nav__item">
      <a href="usecases.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Use Cases
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="">
            
  
  <span class="md-ellipsis">
    API
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            API
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="api/bertopic.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    BERTopic
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6_2" >
        
          
          <label class="md-nav__link" for="__nav_6_2" id="__nav_6_2_label" tabindex="">
            
  
  <span class="md-ellipsis">
    Sub-models
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_6_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6_2">
            <span class="md-nav__icon md-icon"></span>
            Sub-models
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6_2_1" >
        
          
          <label class="md-nav__link" for="__nav_6_2_1" id="__nav_6_2_1_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Backends
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_6_2_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6_2_1">
            <span class="md-nav__icon md-icon"></span>
            Backends
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="api/backends/base.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Base
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="api/backends/word_doc.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Word Doc
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="api/backends/openai.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    OpenAI
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="api/backends/cohere.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Cohere
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6_2_2" >
        
          
          <label class="md-nav__link" for="__nav_6_2_2" id="__nav_6_2_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Dimensionality Reduction
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_6_2_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6_2_2">
            <span class="md-nav__icon md-icon"></span>
            Dimensionality Reduction
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="api/dimensionality/base.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Base
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6_2_3" >
        
          
          <label class="md-nav__link" for="__nav_6_2_3" id="__nav_6_2_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Clustering
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_6_2_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6_2_3">
            <span class="md-nav__icon md-icon"></span>
            Clustering
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="api/cluster/base.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Base
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6_2_4" >
        
          
          <label class="md-nav__link" for="__nav_6_2_4" id="__nav_6_2_4_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Vectorizers
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_6_2_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6_2_4">
            <span class="md-nav__icon md-icon"></span>
            Vectorizers
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="api/ctfidf.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    cTFIDF
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="api/onlinecv.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    OnlineCountVectorizer
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6_2_5" >
        
          
          <label class="md-nav__link" for="__nav_6_2_5" id="__nav_6_2_5_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Topic Representation
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_6_2_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6_2_5">
            <span class="md-nav__icon md-icon"></span>
            Topic Representation
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="api/representation/base.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Base
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="api/representation/mmr.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    MaximalMarginalRelevance
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="api/representation/keybert.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    KeyBERT
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="api/representation/pos.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    PartOfSpeech
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6_2_5_5" >
        
          
          <label class="md-nav__link" for="__nav_6_2_5_5" id="__nav_6_2_5_5_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Text Generation
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="4" aria-labelledby="__nav_6_2_5_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6_2_5_5">
            <span class="md-nav__icon md-icon"></span>
            Text Generation
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="api/representation/generation.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ðŸ¤— Transformers
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="api/representation/langchain.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    LangChain
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="api/representation/cohere.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Cohere
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="api/representation/openai.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    OpenAI
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="api/representation/zeroshot.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Zero-shot Classification
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6_3" >
        
          
          <label class="md-nav__link" for="__nav_6_3" id="__nav_6_3_label" tabindex="">
            
  
  <span class="md-ellipsis">
    Plotting
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_6_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6_3">
            <span class="md-nav__icon md-icon"></span>
            Plotting
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="api/plotting/barchart.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Barchart
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="api/plotting/documents.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Documents
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="api/plotting/dtm.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    DTM
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="api/plotting/hierarchical_documents.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Hierarchical documents
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="api/plotting/hierarchy.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Hierarchical topics
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="api/plotting/distribution.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Distribution
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="api/plotting/heatmap.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Heatmap
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="api/plotting/term.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Term Scores
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="api/plotting/topics.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Topics
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="api/plotting/topics_per_class.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Topics per Class
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    Changelog
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="changelog.html" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    Changelog
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#version-0160" class="md-nav__link">
    Version 0.16.0
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#version-0150" class="md-nav__link">
    Version 0.15.0
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#version-0141" class="md-nav__link">
    Version 0.14.1
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#version-0140" class="md-nav__link">
    Version 0.14.0
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#version-0130" class="md-nav__link">
    Version 0.13.0
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#version-0120" class="md-nav__link">
    Version 0.12.0
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#version-0110" class="md-nav__link">
    Version 0.11.0
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#version-0100" class="md-nav__link">
    Version 0.10.0
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#version-094" class="md-nav__link">
    Version 0.9.4
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#version-093" class="md-nav__link">
    Version 0.9.3
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#version-092" class="md-nav__link">
    Version 0.9.2
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#version-091" class="md-nav__link">
    Version 0.9.1
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#version-090" class="md-nav__link">
    Version 0.9.0
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#version-081" class="md-nav__link">
    Version 0.8.1
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#version-080" class="md-nav__link">
    Version 0.8.0
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#version-070" class="md-nav__link">
    Version 0.7.0
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#version-060" class="md-nav__link">
    Version 0.6.0
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#version-050" class="md-nav__link">
    Version 0.5.0
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#version-042" class="md-nav__link">
    Version 0.4.2
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#version-041" class="md-nav__link">
    Version 0.4.1
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#version-040" class="md-nav__link">
    Version 0.4.0
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#version-032" class="md-nav__link">
    Version 0.3.2
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#version-031" class="md-nav__link">
    Version 0.3.1
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#version-030" class="md-nav__link">
    Version 0.3.0
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#version-022" class="md-nav__link">
    Version 0.2.2
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#version-021" class="md-nav__link">
    Version 0.2.1
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#version-020" class="md-nav__link">
    Version 0.2.0
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#version-012" class="md-nav__link">
    Version 0.1.2
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#version-011" class="md-nav__link">
    Version 0.1.1
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#version-010" class="md-nav__link">
    Version 0.1.0
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
                
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#version-0160" class="md-nav__link">
    Version 0.16.0
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#version-0150" class="md-nav__link">
    Version 0.15.0
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#version-0141" class="md-nav__link">
    Version 0.14.1
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#version-0140" class="md-nav__link">
    Version 0.14.0
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#version-0130" class="md-nav__link">
    Version 0.13.0
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#version-0120" class="md-nav__link">
    Version 0.12.0
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#version-0110" class="md-nav__link">
    Version 0.11.0
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#version-0100" class="md-nav__link">
    Version 0.10.0
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#version-094" class="md-nav__link">
    Version 0.9.4
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#version-093" class="md-nav__link">
    Version 0.9.3
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#version-092" class="md-nav__link">
    Version 0.9.2
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#version-091" class="md-nav__link">
    Version 0.9.1
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#version-090" class="md-nav__link">
    Version 0.9.0
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#version-081" class="md-nav__link">
    Version 0.8.1
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#version-080" class="md-nav__link">
    Version 0.8.0
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#version-070" class="md-nav__link">
    Version 0.7.0
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#version-060" class="md-nav__link">
    Version 0.6.0
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#version-050" class="md-nav__link">
    Version 0.5.0
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#version-042" class="md-nav__link">
    Version 0.4.2
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#version-041" class="md-nav__link">
    Version 0.4.1
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#version-040" class="md-nav__link">
    Version 0.4.0
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#version-032" class="md-nav__link">
    Version 0.3.2
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#version-031" class="md-nav__link">
    Version 0.3.1
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#version-030" class="md-nav__link">
    Version 0.3.0
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#version-022" class="md-nav__link">
    Version 0.2.2
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#version-021" class="md-nav__link">
    Version 0.2.1
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#version-020" class="md-nav__link">
    Version 0.2.0
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#version-012" class="md-nav__link">
    Version 0.1.2
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#version-011" class="md-nav__link">
    Version 0.1.1
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#version-010" class="md-nav__link">
    Version 0.1.0
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
  


<h1 id="changelog">Changelog<a class="headerlink" href="#changelog" title="Permanent link">&para;</a></h1>
<h2 id="version-0160"><strong>Version 0.16.0</strong><a class="headerlink" href="#version-0160" title="Permanent link">&para;</a></h2>
<p><em>Release date: 26 November, 2023</em></p>
<h3><b>Highlights:</a></b></h3>

<ul>
<li>Merge pre-trained BERTopic models with <a href="https://maartengr.github.io/BERTopic/getting_started/merge/merge.html"><strong><code>.merge_models</code></strong></a><ul>
<li>Combine models with different representations together!</li>
<li>Use this for <em>incremental/online topic modeling</em> to detect new incoming topics</li>
<li>First step towards <em>federated learning</em> with BERTopic</li>
</ul>
</li>
<li><a href="https://maartengr.github.io/BERTopic/getting_started/zeroshot/zeroshot.html"><strong>Zero-shot</strong></a> Topic Modeling<ul>
<li>Use a predefined list of topics to assign documents</li>
<li>If needed, allows for further exploration of undefined topics</li>
</ul>
</li>
<li><a href="https://maartengr.github.io/BERTopic/getting_started/seed_words/seed_words.html"><strong>Seed (domain-specific) words</strong></a> with <code>ClassTfidfTransformer</code><ul>
<li>Make sure selected words are more likely to end up in the representation without influencing the clustering process</li>
</ul>
</li>
<li>Added params to <a href="https://maartengr.github.io/BERTopic/getting_started/representation/llm.html#truncating-documents"><strong>truncate documents</strong></a> to length when using LLMs</li>
<li>Added <a href="https://maartengr.github.io/BERTopic/getting_started/representation/llm.html#llamacpp"><strong>LlamaCPP</strong></a> as a representation model</li>
<li>LangChain: Support for <strong>LCEL Runnables</strong> by <a href="https://github.com/joshuasundance-swca">@joshuasundance-swca</a> in <a href="https://github.com/MaartenGr/BERTopic/pull/1586">#1586</a></li>
<li>Added <code>topics</code> parameter to <code>.topics_over_time</code> to select a subset of documents and topics</li>
<li>Documentation:<ul>
<li><a href="https://maartengr.github.io/BERTopic/getting_started/best_practices/best_practices.html">Best practices Guide</a></li>
<li><a href="https://maartengr.github.io/BERTopic/getting_started/representation/llm.html#llama-2">Llama 2 Tutorial</a></li>
<li><a href="https://maartengr.github.io/BERTopic/getting_started/representation/llm.html#zephyr-mistral-7b">Zephyr Tutorial</a></li>
<li>Improved <a href="https://maartengr.github.io/BERTopic/getting_started/embeddings/embeddings.html#sentence-transformers">embeddings guidance</a> (MTEB)</li>
<li>Improved logging throughout the package</li>
</ul>
</li>
<li>Added support for <strong>Cohere's Embed v3</strong>:
<div class="highlight"><pre><span></span><code><span class="n">cohere_model</span> <span class="o">=</span> <span class="n">CohereBackend</span><span class="p">(</span>
    <span class="n">client</span><span class="p">,</span>
    <span class="n">embedding_model</span><span class="o">=</span><span class="s2">&quot;embed-english-v3.0&quot;</span><span class="p">,</span>
    <span class="n">embed_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;input_type&quot;</span><span class="p">:</span> <span class="s2">&quot;clustering&quot;</span><span class="p">}</span>
<span class="p">)</span>
</code></pre></div></li>
</ul>
<h3><b>Fixes:</a></b></h3>

<ul>
<li>Fixed n-gram Keywords need delimiting in OpenAI() <a href="https://github.com/MaartenGr/BERTopic/issues/1546">#1546</a></li>
<li>Fixed OpenAI v1.0 issues <a href="https://github.com/MaartenGr/BERTopic/issues/1629">#1629</a></li>
<li>Improved documentation/logging to adress <a href="https://github.com/MaartenGr/BERTopic/issues/1589">#1589</a>, <a href="https://github.com/MaartenGr/BERTopic/issues/1591">#1591</a></li>
<li>Fixed engine support for Azure OpenAI embeddings <a href="https://github.com/MaartenGr/BERTopic/issues/1487">#1577</a></li>
<li>Fixed OpenAI Representation: KeyError: 'content' <a href="https://github.com/MaartenGr/BERTopic/issues/1570">#1570</a></li>
<li>Fixed Loading topic model with multiple topic aspects changes their format <a href="https://github.com/MaartenGr/BERTopic/issues/1487">#1487</a></li>
<li>Fix expired link in algorithm.md by <a href="https://github.com/burugaria7">@burugaria7</a> in <a href="https://github.com/MaartenGr/BERTopic/pull/1396">#1396</a></li>
<li>Fix guided topic modeling in cuML's UMAP by <a href="https://github.com/stevetracvc">@stevetracvc</a> in <a href="https://github.com/MaartenGr/BERTopic/pull/1326">#1326</a></li>
<li>OpenAI: Allow retrying on Service Unavailable errors by <a href="https://github.com/agamble">@agamble</a> in <a href="https://github.com/MaartenGr/BERTopic/pull/1407">#1407</a></li>
<li>Fixed parameter naming for HDBSCAN in best practices by <a href="https://github.com/rnckp">@rnckp</a> in <a href="https://github.com/MaartenGr/BERTopic/pull/1408">#1408</a></li>
<li>Fixed typo in tips_and_tricks.md by <a href="https://github.com/aronnoordhoek">@aronnoordhoek</a> in <a href="https://github.com/MaartenGr/BERTopic/pull/1446">#1446</a></li>
<li>Fix typos in documentation by <a href="https://github.com/bobchien">@bobchien</a> in <a href="https://github.com/MaartenGr/BERTopic/pull/1481">#1481</a></li>
<li>Fix IndexError when all outliers are removed by reduce_outliers by <a href="https://github.com/Aratako">@Aratako</a> in <a href="https://github.com/MaartenGr/BERTopic/pull/1466">#1466</a></li>
<li>Fix TypeError on reduce_outliers "probabilities" by <a href="https://github.com/ananaphasia">@ananaphasia</a> in <a href="https://github.com/MaartenGr/BERTopic/pull/1501">#1501</a></li>
<li>Add new line to fix markdown bullet point formatting by <a href="https://github.com/saeedesmaili">@saeedesmaili</a> in <a href="https://github.com/MaartenGr/BERTopic/pull/1519">#1519</a></li>
<li>Update typo in topicrepresentation.md by <a href="https://github.com/oliviercaron">@oliviercaron</a> in <a href="https://github.com/MaartenGr/BERTopic/pull/1537">#1537</a></li>
<li>Fix typo in FAQ by <a href="https://github.com/sandijou">@sandijou</a> in <a href="https://github.com/MaartenGr/BERTopic/pull/1542">#1542</a></li>
<li>Fixed typos in best practices documentation by <a href="https://github.com/poomkusa">@poomkusa</a> in <a href="https://github.com/MaartenGr/BERTopic/pull/1557">#1557</a></li>
<li>Correct TopicMapper doc example by <a href="https://github.com/chrisji">@chrisji</a> in <a href="https://github.com/MaartenGr/BERTopic/pull/1637">#1637</a></li>
<li>Fix typing in hierarchical_topics by <a href="https://github.com/dschwalm">@dschwalm</a> in <a href="https://github.com/MaartenGr/BERTopic/pull/1364">#1364</a></li>
<li>Fixed typing issue with treshold parameter in reduce_outliers by <a href="https://github.com/dschwalm">@dschwalm</a> in <a href="https://github.com/MaartenGr/BERTopic/pull/1380">#1380</a></li>
<li>Fix several typos by <a href="https://github.com/mertyyanik">@mertyyanik</a> in <a href="https://github.com/MaartenGr/BERTopic/pull/1307">#1307</a>
(#1307)</li>
<li>Fix inconsistent naming by <a href="https://github.com/rolanderdei">@rolanderdei</a> in <a href="https://github.com/MaartenGr/BERTopic/pull/1073">#1073</a></li>
</ul>
<h3><b><a href="https://maartengr.github.io/BERTopic/getting_started/merge/merge.html">Merge Pre-trained BERTopic Models</a></b></h3>

<p>The new <code>.merge_models</code> feature allows for any number of fitted BERTopic models to be merged. Doing so allows for a number of use cases:</p>
<ul>
<li><strong>Incremental topic modeling</strong> -- Continuously merge models together to detect whether new topics have appeared</li>
<li><strong>Federated Learning</strong> - Train BERTopic models on different clients and combine them on a central server</li>
<li><strong>Minimal compute</strong> - We can essentially batch the training process into multiple instances to reduce compute</li>
<li><strong>Different datasets</strong> - When you have different datasets that you want to train seperately on, for example with different languages, you can train each model separately and join them after training</li>
</ul>
<p>To demonstrate merging different topic models with BERTopic, we use the ArXiv paper abstracts to see which topics they generally contain.</p>
<p>First, we train three separate models on different parts of the data:</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">umap</span> <span class="kn">import</span> <span class="n">UMAP</span>
<span class="kn">from</span> <span class="nn">bertopic</span> <span class="kn">import</span> <span class="n">BERTopic</span>
<span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">&quot;CShorten/ML-ArXiv-Papers&quot;</span><span class="p">)[</span><span class="s2">&quot;train&quot;</span><span class="p">]</span>

<span class="c1"># Extract abstracts to train on and corresponding titles</span>
<span class="n">abstracts_1</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="s2">&quot;abstract&quot;</span><span class="p">][:</span><span class="mi">5_000</span><span class="p">]</span>
<span class="n">abstracts_2</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="s2">&quot;abstract&quot;</span><span class="p">][</span><span class="mi">5_000</span><span class="p">:</span><span class="mi">10_000</span><span class="p">]</span>
<span class="n">abstracts_3</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="s2">&quot;abstract&quot;</span><span class="p">][</span><span class="mi">10_000</span><span class="p">:</span><span class="mi">15_000</span><span class="p">]</span>

<span class="c1"># Create topic models</span>
<span class="n">umap_model</span> <span class="o">=</span> <span class="n">UMAP</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">n_components</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">min_dist</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="s1">&#39;cosine&#39;</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">topic_model_1</span> <span class="o">=</span> <span class="n">BERTopic</span><span class="p">(</span><span class="n">umap_model</span><span class="o">=</span><span class="n">umap_model</span><span class="p">,</span> <span class="n">min_topic_size</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">abstracts_1</span><span class="p">)</span>
<span class="n">topic_model_2</span> <span class="o">=</span> <span class="n">BERTopic</span><span class="p">(</span><span class="n">umap_model</span><span class="o">=</span><span class="n">umap_model</span><span class="p">,</span> <span class="n">min_topic_size</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">abstracts_2</span><span class="p">)</span>
<span class="n">topic_model_3</span> <span class="o">=</span> <span class="n">BERTopic</span><span class="p">(</span><span class="n">umap_model</span><span class="o">=</span><span class="n">umap_model</span><span class="p">,</span> <span class="n">min_topic_size</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">abstracts_3</span><span class="p">)</span>
</code></pre></div>
<p>Then, we can combine all three models into one with <code>.merge_models</code>:</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Combine all models into one</span>
<span class="n">merged_model</span> <span class="o">=</span> <span class="n">BERTopic</span><span class="o">.</span><span class="n">merge_models</span><span class="p">([</span><span class="n">topic_model_1</span><span class="p">,</span> <span class="n">topic_model_2</span><span class="p">,</span> <span class="n">topic_model_3</span><span class="p">])</span>
</code></pre></div>
<h3><b><a href="https://maartengr.github.io/BERTopic/getting_started/zeroshot/zeroshot.html">Zero-shot Topic Modeling</a></b></h3>
<p>Zeroshot Topic Modeling is a technique that allows you to find pre-defined topics in large amounts of documents. This method allows you to not only find those specific topics but also create new topics for documents that would not fit with your predefined topics. 
This allows for extensive flexibility as there are three scenario's to explore.</p>
<ul>
<li>No zeroshot topics were detected. This means that none of the documents would fit with the predefined topics and a regular BERTopic would be run. </li>
<li>Only zeroshot topics were detected. Here, we would not need to find additional topics since all original documents were assigned to one of the predefined topics.</li>
<li>Both zeroshot topics and clustered topics were detected. This means that some documents would fit with the predefined topics where others would not. For the latter, new topics were found.</li>
</ul>
<p><img alt="zeroshot" src="https://github.com/MaartenGr/BERTopic/assets/25746895/9cce6ee3-445f-440a-b93b-f8008578c839" /></p>
<p>In order to use zero-shot BERTopic, we create a list of topics that we want to assign to our documents. However, 
there may be several other topics that we know should be in the documents. The dataset that we use is small subset of ArXiv papers.
We know the data and believe there to be at least the following topics: <em>clustering</em>, <em>topic modeling</em>, and <em>large language models</em>. 
However, we are not sure whether other topics exist and want to explore those.</p>
<p>Using this feature is straightforward:</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>

<span class="kn">from</span> <span class="nn">bertopic</span> <span class="kn">import</span> <span class="n">BERTopic</span>
<span class="kn">from</span> <span class="nn">bertopic.representation</span> <span class="kn">import</span> <span class="n">KeyBERTInspired</span>

<span class="c1"># We select a subsample of 5000 abstracts from ArXiv</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">&quot;CShorten/ML-ArXiv-Papers&quot;</span><span class="p">)[</span><span class="s2">&quot;train&quot;</span><span class="p">]</span>
<span class="n">docs</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="s2">&quot;abstract&quot;</span><span class="p">][:</span><span class="mi">5_000</span><span class="p">]</span>

<span class="c1"># We define a number of topics that we know are in the documents</span>
<span class="n">zeroshot_topic_list</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Clustering&quot;</span><span class="p">,</span> <span class="s2">&quot;Topic Modeling&quot;</span><span class="p">,</span> <span class="s2">&quot;Large Language Models&quot;</span><span class="p">]</span>

<span class="c1"># We fit our model using the zero-shot topics</span>
<span class="c1"># and we define a minimum similarity. For each document,</span>
<span class="c1"># if the similarity does not exceed that value, it will be used</span>
<span class="c1"># for clustering instead.</span>
<span class="n">topic_model</span> <span class="o">=</span> <span class="n">BERTopic</span><span class="p">(</span>
    <span class="n">embedding_model</span><span class="o">=</span><span class="s2">&quot;thenlper/gte-small&quot;</span><span class="p">,</span> 
    <span class="n">min_topic_size</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span>
    <span class="n">zeroshot_topic_list</span><span class="o">=</span><span class="n">zeroshot_topic_list</span><span class="p">,</span>
    <span class="n">zeroshot_min_similarity</span><span class="o">=</span><span class="mf">.85</span><span class="p">,</span>
    <span class="n">representation_model</span><span class="o">=</span><span class="n">KeyBERTInspired</span><span class="p">()</span>
<span class="p">)</span>
<span class="n">topics</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">topic_model</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>
</code></pre></div>
<p>When we run <code>topic_model.get_topic_info()</code> you will see something like this:</p>
<p><img alt="zeroshot_output" src="https://github.com/MaartenGr/BERTopic/assets/25746895/1801e0a9-cda7-4d74-929f-e975fa67404b" /></p>
<h3><b><a href="https://maartengr.github.io/BERTopic/getting_started/seed_words/seed_words.html">Seed (Domain-specific) Words</a></b></h3>

<p>When performing Topic Modeling, you are often faced with data that you are familiar with to a certain extend or that speaks a very specific language. In those cases, topic modeling techniques might have difficulties capturing and representing the semantic nature of domain specific abbreviations, slang, short form, acronyms, etc. For example, the <em>"TNM"</em> classification is a method for identifying the stage of most cancers. The word <em>"TNM"</em> is an abbreviation and might not be correctly captured in generic embedding models. </p>
<p>To make sure that certain domain specific words are weighted higher and are more often used in topic representations, you can set any number of <code>seed_words</code> in the <code>bertopic.vectorizer.ClassTfidfTransformer</code>. To do so, let's take a look at an example. We have a dataset of article abstracts and want to perform some topic modeling. Since we might be familiar with the data, there are certain words that we know should be generally important. Let's assume that we have in-depth knowledge about reinforcement learning and know that words like "agent" and "robot" should be important in such a topic were it to be found. Using the <code>ClassTfidfTransformer</code>, we can define those <code>seed_words</code> and also choose by how much their values are multiplied. </p>
<p>The full example is then as follows:</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">umap</span> <span class="kn">import</span> <span class="n">UMAP</span>
<span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>
<span class="kn">from</span> <span class="nn">bertopic</span> <span class="kn">import</span> <span class="n">BERTopic</span>
<span class="kn">from</span> <span class="nn">bertopic.vectorizers</span> <span class="kn">import</span> <span class="n">ClassTfidfTransformer</span>

<span class="c1"># Let&#39;s take a subset of ArXiv abstracts as the training data</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">&quot;CShorten/ML-ArXiv-Papers&quot;</span><span class="p">)[</span><span class="s2">&quot;train&quot;</span><span class="p">]</span>
<span class="n">abstracts</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="s2">&quot;abstract&quot;</span><span class="p">][:</span><span class="mi">5_000</span><span class="p">]</span>

<span class="c1"># For illustration purposes, we make sure the output is fixed when running this code multiple times</span>
<span class="n">umap_model</span> <span class="o">=</span> <span class="n">UMAP</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">n_components</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">min_dist</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="s1">&#39;cosine&#39;</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># We can choose any number of seed words for which we want their representation</span>
<span class="c1"># to be strengthen. We increase the importance of these words as we want them to be more</span>
<span class="c1"># likely to end up in the topic representations.</span>
<span class="n">ctfidf_model</span> <span class="o">=</span> <span class="n">ClassTfidfTransformer</span><span class="p">(</span>
    <span class="n">seed_words</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;agent&quot;</span><span class="p">,</span> <span class="s2">&quot;robot&quot;</span><span class="p">,</span> <span class="s2">&quot;behavior&quot;</span><span class="p">,</span> <span class="s2">&quot;policies&quot;</span><span class="p">,</span> <span class="s2">&quot;environment&quot;</span><span class="p">],</span> 
    <span class="n">seed_multiplier</span><span class="o">=</span><span class="mi">2</span>
<span class="p">)</span>

<span class="c1"># We run the topic model with the seeded words</span>
<span class="n">topic_model</span> <span class="o">=</span> <span class="n">BERTopic</span><span class="p">(</span>
    <span class="n">umap_model</span><span class="o">=</span><span class="n">umap_model</span><span class="p">,</span>
    <span class="n">min_topic_size</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span>
    <span class="n">ctfidf_model</span><span class="o">=</span><span class="n">ctfidf_model</span><span class="p">,</span>
<span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">abstracts</span><span class="p">)</span>
</code></pre></div>
<h3><b><a href="https://maartengr.github.io/BERTopic/getting_started/representation/llm.html#truncating-documents">Truncate Documents in LLMs</a></b></h3>

<p>When using LLMs with BERTopic, we can truncate the input documents in <code>[DOCUMENTS]</code> in order to reduce the number of tokens that we have in our input prompt. To do so, all text generation modules have two parameters that we can tweak:</p>
<ul>
<li><code>doc_length</code> - The maximum length of each document. If a document is longer, it will be truncated. If None, the entire document is passed.</li>
<li><code>tokenizer</code> - The tokenizer used to calculate to split the document into segments used to count the length of a document. <ul>
<li>Options include <code>'char'</code>, <code>'whitespace'</code>, <code>'vectorizer'</code>, and a callable</li>
</ul>
</li>
</ul>
<p>This means that the definition of <code>doc_length</code> changes depending on what constitutes a token in the <code>tokenizer</code> parameter. If a token is a character, then <code>doc_length</code> refers to max length in characters. If a token is a word, then <code>doc_length</code> refers to the max length in words.</p>
<p>Let's illustrate this with an example. In the code below, we will use <a href="https://github.com/openai/tiktoken"><code>tiktoken</code></a> to count the number of tokens in each document and limit them to 100 tokens. All documents that have more than 100 tokens will be truncated.</p>
<p>We use <code>bertopic.representation.OpenAI</code> to represent our topics with nicely written labels. We specify that documents that we put in the prompt cannot exceed 100 tokens each. Since we will put 4 documents in the prompt, they will total roughly 400 tokens:</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">openai</span>
<span class="kn">import</span> <span class="nn">tiktoken</span>
<span class="kn">from</span> <span class="nn">bertopic.representation</span> <span class="kn">import</span> <span class="n">OpenAI</span>
<span class="kn">from</span> <span class="nn">bertopic</span> <span class="kn">import</span> <span class="n">BERTopic</span>

<span class="c1"># Tokenizer</span>
<span class="n">tokenizer</span><span class="o">=</span> <span class="n">tiktoken</span><span class="o">.</span><span class="n">encoding_for_model</span><span class="p">(</span><span class="s2">&quot;gpt-3.5-turbo&quot;</span><span class="p">)</span>

<span class="c1"># Create your representation model</span>
<span class="n">client</span> <span class="o">=</span> <span class="n">openai</span><span class="o">.</span><span class="n">OpenAI</span><span class="p">(</span><span class="n">api_key</span><span class="o">=</span><span class="s2">&quot;sk-...&quot;</span><span class="p">)</span>
<span class="n">representation_model</span> <span class="o">=</span> <span class="n">OpenAI</span><span class="p">(</span>
    <span class="n">client</span><span class="p">,</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-3.5-turbo&quot;</span><span class="p">,</span>
    <span class="n">delay_in_seconds</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> 
    <span class="n">chat</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">nr_docs</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">doc_length</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span>
<span class="p">)</span>

<span class="c1"># Use the representation model in BERTopic on top of the default pipeline</span>
<span class="n">topic_model</span> <span class="o">=</span> <span class="n">BERTopic</span><span class="p">(</span><span class="n">representation_model</span><span class="o">=</span><span class="n">representation_model</span><span class="p">)</span>
</code></pre></div>
<h2 id="version-0150"><strong>Version 0.15.0</strong><a class="headerlink" href="#version-0150" title="Permanent link">&para;</a></h2>
<p><em>Release date: 29 May, 2023</em></p>
<h3><b>Highlights:</a></b></h3>

<ul>
<li><a href="https://maartengr.github.io/BERTopic/getting_started/multimodal/multimodal.html"><strong>Multimodal</strong></a> Topic Modeling<ul>
<li>Train your topic modeling on text, images, or images and text!</li>
<li>Use the <code>bertopic.backend.MultiModalBackend</code> to embed images, text, both or even caption images!</li>
</ul>
</li>
<li><a href="https://maartengr.github.io/BERTopic/getting_started/multiaspect/multiaspect.html"><strong>Multi-Aspect</strong></a> Topic Modeling<ul>
<li>Create multiple topic representations simultaneously </li>
</ul>
</li>
<li>Improved <a href="https://maartengr.github.io/BERTopic/getting_started/serialization/serialization.html"><strong>Serialization</strong></a> options<ul>
<li>Push your model to the HuggingFace Hub with <code>.push_to_hf_hub</code></li>
<li>Safer, smaller and more flexible serialization options with <code>safetensors</code></li>
<li>Thanks to a great collaboration with HuggingFace and the authors of <a href="https://github.com/opinionscience/BERTransfer">BERTransfer</a>!</li>
</ul>
</li>
<li>Added new embedding models<ul>
<li>OpenAI: <code>bertopic.backend.OpenAIBackend</code></li>
<li>Cohere: <code>bertopic.backend.CohereBackend</code></li>
</ul>
</li>
<li>Added example of <a href="https://maartengr.github.io/BERTopic/getting_started/representation/representation.html#summarization">summarizing topics</a> with OpenAI's GPT-models</li>
<li>Added <code>nr_docs</code> and <code>diversity</code> parameters to OpenAI and Cohere representation models</li>
<li>Use <code>custom_labels="Aspect1"</code> to use the aspect labels for visualizations instead</li>
<li>Added cuML support for probability calculation in <code>.transform</code></li>
<li>Updated <strong>topic embeddings</strong><ul>
<li>Centroids by default and c-TF-IDF weighted embeddings for <code>partial_fit</code> and <code>.update_topics</code></li>
</ul>
</li>
<li>Added <code>exponential_backoff</code> parameter to <code>OpenAI</code> model</li>
</ul>
<h3><b>Fixes:</a></b></h3>

<ul>
<li>Fixed custom prompt not working in <code>TextGeneration</code> </li>
<li>Fixed <a href="https://github.com/MaartenGr/BERTopic/pull/1142">#1142</a></li>
<li>Add additional logic to handle cupy arrays by <a href="https://github.com/metasyn">@metasyn</a> in <a href="https://github.com/MaartenGr/BERTopic/pull/1179">#1179</a></li>
<li>Fix hierarchy viz and handle any form of distance matrix by <a href="https://github.com/elashrry">@elashrry</a> in <a href="https://github.com/MaartenGr/BERTopic/pull/1173">#1173</a></li>
<li>Updated languages list by <a href="https://github.com/sam9111">@sam9111</a> in <a href="https://github.com/MaartenGr/BERTopic/pull/1099">#1099</a></li>
<li>Added level_scale argument to visualize_hierarchical_documents by <a href="https://github.com/zilch42">@zilch42</a> in <a href="https://github.com/MaartenGr/BERTopic/pull/1106">#1106</a></li>
<li>Fix inconsistent naming by <a href="https://github.com/rolanderdei">@rolanderdei</a> in <a href="https://github.com/MaartenGr/BERTopic/pull/1073">#1073</a></li>
</ul>
<h3><b><a href="https://maartengr.github.io/BERTopic/getting_started/multimodal/multimodal.html">Multimodal Topic Modeling</a></b></h3>

<p>With v0.15, we can now perform multimodal topic modeling in BERTopic! The most basic example of multimodal topic modeling in BERTopic is when you have images that accompany your documents. This means that it is expected that each document has an image and vice versa. Instagram pictures, for example, almost always have some descriptions to them. </p>
<figure>
<p><img alt="Image title" src="getting_started/multimodal/images_and_text.svg" />
  </p>
<figcaption></figcaption>
</figure>
<p>In this example, we are going to use images from <code>flickr</code> that each have a caption accociated to it: </p>
<div class="highlight"><pre><span></span><code><span class="c1"># NOTE: This requires the `datasets` package which you can </span>
<span class="c1"># install with `pip install datasets`</span>
<span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>

<span class="n">ds</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">&quot;maderix/flickr_bw_rgb&quot;</span><span class="p">)</span>
<span class="n">images</span> <span class="o">=</span> <span class="n">ds</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">][</span><span class="s2">&quot;image&quot;</span><span class="p">]</span>
<span class="n">docs</span> <span class="o">=</span> <span class="n">ds</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">][</span><span class="s2">&quot;caption&quot;</span><span class="p">]</span>
</code></pre></div>
<p>The <code>docs</code> variable contains the captions for each image in <code>images</code>. We can now use these variables to run our multimodal example:</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">bertopic</span> <span class="kn">import</span> <span class="n">BERTopic</span>
<span class="kn">from</span> <span class="nn">bertopic.representation</span> <span class="kn">import</span> <span class="n">VisualRepresentation</span>

<span class="c1"># Additional ways of representing a topic</span>
<span class="n">visual_model</span> <span class="o">=</span> <span class="n">VisualRepresentation</span><span class="p">()</span>

<span class="c1"># Make sure to add the `visual_model` to a dictionary</span>
<span class="n">representation_model</span> <span class="o">=</span> <span class="p">{</span>
   <span class="s2">&quot;Visual_Aspect&quot;</span><span class="p">:</span>  <span class="n">visual_model</span><span class="p">,</span>
<span class="p">}</span>
<span class="n">topic_model</span> <span class="o">=</span> <span class="n">BERTopic</span><span class="p">(</span><span class="n">representation_model</span><span class="o">=</span><span class="n">representation_model</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div>
<p>We can now access our image representations for each topic with <code>topic_model.topic_aspects_["Visual_Aspect"]</code>.
If you want an overview of the topic images together with their textual representations in jupyter, you can run the following:</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">base64</span>
<span class="kn">from</span> <span class="nn">io</span> <span class="kn">import</span> <span class="n">BytesIO</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">HTML</span>

<span class="k">def</span> <span class="nf">image_base64</span><span class="p">(</span><span class="n">im</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">im</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
        <span class="n">im</span> <span class="o">=</span> <span class="n">get_thumbnail</span><span class="p">(</span><span class="n">im</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">BytesIO</span><span class="p">()</span> <span class="k">as</span> <span class="n">buffer</span><span class="p">:</span>
        <span class="n">im</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">buffer</span><span class="p">,</span> <span class="s1">&#39;jpeg&#39;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">base64</span><span class="o">.</span><span class="n">b64encode</span><span class="p">(</span><span class="n">buffer</span><span class="o">.</span><span class="n">getvalue</span><span class="p">())</span><span class="o">.</span><span class="n">decode</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">image_formatter</span><span class="p">(</span><span class="n">im</span><span class="p">):</span>
    <span class="k">return</span> <span class="sa">f</span><span class="s1">&#39;&lt;img src=&quot;data:image/jpeg;base64,</span><span class="si">{</span><span class="n">image_base64</span><span class="p">(</span><span class="n">im</span><span class="p">)</span><span class="si">}</span><span class="s1">&quot;&gt;&#39;</span>

<span class="c1"># Extract dataframe</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">topic_model</span><span class="o">.</span><span class="n">get_topic_info</span><span class="p">()</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s2">&quot;Representative_Docs&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s2">&quot;Name&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># Visualize the images</span>
<span class="n">HTML</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">to_html</span><span class="p">(</span><span class="n">formatters</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;Visual_Aspect&#39;</span><span class="p">:</span> <span class="n">image_formatter</span><span class="p">},</span> <span class="n">escape</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
</code></pre></div>
<p><img alt="images_and_text" src="https://github.com/MaartenGr/BERTopic/assets/25746895/3a741e2b-5810-4865-9664-0c6bb24ca3f9" /></p>
<h3><b><a href="https://maartengr.github.io/BERTopic/getting_started/multiaspect/multiaspect.html">Multi-aspect Topic Modeling</a></b></h3>

<p>In this new release, we introduce <code>multi-aspect topic modeling</code>! During the <code>.fit</code> or <code>.fit_transform</code> stages, you can now get multiple representations of a single topic. In practice, it works by generating and storing all kinds of different topic representations (see image below).</p>
<figure>
<p><img alt="Image title" src="getting_started/multiaspect/multiaspect.svg" />
  </p>
<figcaption></figcaption>
</figure>
<p>The approach is rather straightforward. We might want to represent our topics using a <code>PartOfSpeech</code> representation model but we might also want to try out <code>KeyBERTInspired</code> and compare those representation models. We can do this as follows:</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">bertopic.representation</span> <span class="kn">import</span> <span class="n">KeyBERTInspired</span>
<span class="kn">from</span> <span class="nn">bertopic.representation</span> <span class="kn">import</span> <span class="n">PartOfSpeech</span>
<span class="kn">from</span> <span class="nn">bertopic.representation</span> <span class="kn">import</span> <span class="n">MaximalMarginalRelevance</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">fetch_20newsgroups</span>

<span class="c1"># Documents to train on</span>
<span class="n">docs</span> <span class="o">=</span> <span class="n">fetch_20newsgroups</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="s1">&#39;all&#39;</span><span class="p">,</span>  <span class="n">remove</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;headers&#39;</span><span class="p">,</span> <span class="s1">&#39;footers&#39;</span><span class="p">,</span> <span class="s1">&#39;quotes&#39;</span><span class="p">))[</span><span class="s1">&#39;data&#39;</span><span class="p">]</span>

<span class="c1"># The main representation of a topic</span>
<span class="n">main_representation</span> <span class="o">=</span> <span class="n">KeyBERTInspired</span><span class="p">()</span>

<span class="c1"># Additional ways of representing a topic</span>
<span class="n">aspect_model1</span> <span class="o">=</span> <span class="n">PartOfSpeech</span><span class="p">(</span><span class="s2">&quot;en_core_web_sm&quot;</span><span class="p">)</span>
<span class="n">aspect_model2</span> <span class="o">=</span> <span class="p">[</span><span class="n">KeyBERTInspired</span><span class="p">(</span><span class="n">top_n_words</span><span class="o">=</span><span class="mi">30</span><span class="p">),</span> <span class="n">MaximalMarginalRelevance</span><span class="p">(</span><span class="n">diversity</span><span class="o">=</span><span class="mf">.5</span><span class="p">)]</span>

<span class="c1"># Add all models together to be run in a single `fit`</span>
<span class="n">representation_model</span> <span class="o">=</span> <span class="p">{</span>
   <span class="s2">&quot;Main&quot;</span><span class="p">:</span> <span class="n">main_representation</span><span class="p">,</span>
   <span class="s2">&quot;Aspect1&quot;</span><span class="p">:</span>  <span class="n">aspect_model1</span><span class="p">,</span>
   <span class="s2">&quot;Aspect2&quot;</span><span class="p">:</span>  <span class="n">aspect_model2</span> 
<span class="p">}</span>
<span class="n">topic_model</span> <span class="o">=</span> <span class="n">BERTopic</span><span class="p">(</span><span class="n">representation_model</span><span class="o">=</span><span class="n">representation_model</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>
</code></pre></div>
<p>As show above, to perform multi-aspect topic modeling, we make sure that <code>representation_model</code> is a dictionary where each representation model pipeline is defined. 
The main pipeline, that is used in most visualization options, is defined with the <code>"Main"</code> key. All other aspects can be defined however you want. In the example above, the two additional aspects that we are interested in are defined as <code>"Aspect1"</code> and <code>"Aspect2"</code>. </p>
<p>After we have fitted our model, we can access all representations with <code>topic_model.get_topic_info()</code>:</p>
<p><img src="getting_started/multiaspect/table.PNG">
<br></p>
<p>As you can see, there are a number of different representations for our topics that we can inspect. All aspects are found in <code>topic_model.topic_aspects_</code>. </p>
<h3><b><a href="https://maartengr.github.io/BERTopic/getting_started/serialization/serialization.html">Serialization</a></b></h3>

<p>Saving, loading, and sharing a BERTopic model can be done in several ways. With this new release, it is now  advised to go with <code>.safetensors</code> as that allows for a small, safe, and fast method for saving your BERTopic model. However, other formats, such as <code>.pickle</code> and pytorch <code>.bin</code> are also possible.</p>
<p>The methods are used as follows:</p>
<div class="highlight"><pre><span></span><code><span class="n">topic_model</span> <span class="o">=</span> <span class="n">BERTopic</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">my_docs</span><span class="p">)</span>

<span class="c1"># Method 1 - safetensors</span>
<span class="n">embedding_model</span> <span class="o">=</span> <span class="s2">&quot;sentence-transformers/all-MiniLM-L6-v2&quot;</span>
<span class="n">topic_model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s2">&quot;path/to/my/model_dir&quot;</span><span class="p">,</span> <span class="n">serialization</span><span class="o">=</span><span class="s2">&quot;safetensors&quot;</span><span class="p">,</span> <span class="n">save_ctfidf</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">save_embedding_model</span><span class="o">=</span><span class="n">embedding_model</span><span class="p">)</span>

<span class="c1"># Method 2 - pytorch</span>
<span class="n">embedding_model</span> <span class="o">=</span> <span class="s2">&quot;sentence-transformers/all-MiniLM-L6-v2&quot;</span>
<span class="n">topic_model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s2">&quot;path/to/my/model_dir&quot;</span><span class="p">,</span> <span class="n">serialization</span><span class="o">=</span><span class="s2">&quot;pytorch&quot;</span><span class="p">,</span> <span class="n">save_ctfidf</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">save_embedding_model</span><span class="o">=</span><span class="n">embedding_model</span><span class="p">)</span>

<span class="c1"># Method 3 - pickle</span>
<span class="n">topic_model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s2">&quot;my_model&quot;</span><span class="p">,</span> <span class="n">serialization</span><span class="o">=</span><span class="s2">&quot;pickle&quot;</span><span class="p">)</span>
</code></pre></div>
<p>Saving the topic modeling with <code>.safetensors</code> or <code>pytorch</code> has a number of advantages:</p>
<ul>
<li><code>.safetensors</code> is a relatively <strong>safe format</strong></li>
<li>The resulting model can be <strong>very small</strong> (often &lt; 20MB&gt;) since no sub-models need to be saved</li>
<li>Although version control is important, there is a bit more <strong>flexibility</strong> with respect to specific versions of packages</li>
<li>More easily used in <strong>production</strong></li>
<li><strong>Share</strong> models with the HuggingFace Hub</li>
</ul>
<p><br><br>
<img src="getting_started/serialization/serialization.png">
<br><br></p>
<p>The above image, a model trained on 100,000 documents, demonstrates the differences in sizes comparing <code>safetensors</code>, <code>pytorch</code>, and <code>pickle</code>. The difference in sizes can mostly be explained due to the efficient saving procedure and that the clustering and dimensionality reductions are not saved in safetensors/pytorch since inference can be done based on the topic embeddings. </p>
<h3><b><a href="https://maartengr.github.io/BERTopic/getting_started/serialization/serialization.html#huggingFace-hub">HuggingFace Hub</a></b></h3>

<p>When you have created a BERTopic model, you can easily share it with other through the HuggingFace Hub. First, you need to log in to your HuggingFace account:</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">huggingface_hub</span> <span class="kn">import</span> <span class="n">login</span>
<span class="n">login</span><span class="p">()</span>
</code></pre></div>
<p>When you have logged in to your HuggingFace account, you can save and upload the model as follows:</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">bertopic</span> <span class="kn">import</span> <span class="n">BERTopic</span>

<span class="c1"># Train model</span>
<span class="n">topic_model</span> <span class="o">=</span> <span class="n">BERTopic</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">my_docs</span><span class="p">)</span>

<span class="c1"># Push to HuggingFace Hub</span>
<span class="n">topic_model</span><span class="o">.</span><span class="n">push_to_hf_hub</span><span class="p">(</span>
    <span class="n">repo_id</span><span class="o">=</span><span class="s2">&quot;MaartenGr/BERTopic_ArXiv&quot;</span><span class="p">,</span>
    <span class="n">save_ctfidf</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>

<span class="c1"># Load from HuggingFace</span>
<span class="n">loaded_model</span> <span class="o">=</span> <span class="n">BERTopic</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;MaartenGr/BERTopic_ArXiv&quot;</span><span class="p">)</span>
</code></pre></div>
<h2 id="version-0141"><strong>Version 0.14.1</strong><a class="headerlink" href="#version-0141" title="Permanent link">&para;</a></h2>
<p><em>Release date: 2 March, 2023</em></p>
<h3><b>Highlights:</a></b></h3>

<ul>
<li>Use <a href="https://maartengr.github.io/BERTopic/getting_started/representation/representation.html#chatgpt"><strong>ChatGPT</strong></a> to create topic representations!:</li>
<li>Added <code>delay_in_seconds</code> parameter to OpenAI and Cohere representation models for throttling the API<ul>
<li>Setting this between 5 and 10 allows for trial users now to use more easily without hitting RateLimitErrors</li>
</ul>
</li>
<li>Fixed missing <code>title</code> param to visualization methods</li>
<li>Fixed probabilities not correctly aligning (<a href="https://github.com/MaartenGr/BERTopic/issues/1024">#1024</a>)</li>
<li>Fix typo in textgenerator  <a href="https://github.com/dkopljar27">@dkopljar27</a> in <a href="https://github.com/MaartenGr/BERTopic/pull/1002">#1002</a></li>
</ul>
<h3><b><a href="https://maartengr.github.io/BERTopic/getting_started/representation/representation.html#chatgpt">ChatGPT</a></b></h3>

<p>Within OpenAI's API, the ChatGPT models use a different API structure compared to the GPT-3 models. 
In order to use ChatGPT with BERTopic, we need to define the model and make sure to set <code>chat=True</code>:</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">openai</span>
<span class="kn">from</span> <span class="nn">bertopic</span> <span class="kn">import</span> <span class="n">BERTopic</span>
<span class="kn">from</span> <span class="nn">bertopic.representation</span> <span class="kn">import</span> <span class="n">OpenAI</span>

<span class="c1"># Create your representation model</span>
<span class="n">openai</span><span class="o">.</span><span class="n">api_key</span> <span class="o">=</span> <span class="n">MY_API_KEY</span>
<span class="n">representation_model</span> <span class="o">=</span> <span class="n">OpenAI</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-3.5-turbo&quot;</span><span class="p">,</span> <span class="n">delay_in_seconds</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">chat</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Use the representation model in BERTopic on top of the default pipeline</span>
<span class="n">topic_model</span> <span class="o">=</span> <span class="n">BERTopic</span><span class="p">(</span><span class="n">representation_model</span><span class="o">=</span><span class="n">representation_model</span><span class="p">)</span>
</code></pre></div>
<p>Prompting with ChatGPT is very satisfying and can be customized in BERTopic by using certain tags. 
There are currently two tags, namely <code>"[KEYWORDS]"</code> and <code>"[DOCUMENTS]"</code>. 
These tags indicate where in the prompt they are to be replaced with a topics keywords and top 4 most representative documents respectively. 
For example, if we have the following prompt:</p>
<div class="highlight"><pre><span></span><code><span class="n">prompt</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">I have topic that contains the following documents: </span><span class="se">\n</span><span class="s2">[DOCUMENTS]</span>
<span class="s2">The topic is described by the following keywords: [KEYWORDS]</span>

<span class="s2">Based on the information above, extract a short topic label in the following format:</span>
<span class="s2">topic: &lt;topic label&gt;</span>
<span class="s2">&quot;&quot;&quot;</span>
</code></pre></div>
<p>then that will be rendered as follows and passed to OpenAI's API:</p>
<div class="highlight"><pre><span></span><code><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">I have a topic that contains the following documents: </span>
<span class="sd">- Our videos are also made possible by your support on patreon.co.</span>
<span class="sd">- If you want to help us make more videos, you can do so on patreon.com or get one of our posters from our shop.</span>
<span class="sd">- If you want to help us make more videos, you can do so there.</span>
<span class="sd">- And if you want to support us in our endeavor to survive in the world of online video, and make more videos, you can do so on patreon.com.</span>

<span class="sd">The topic is described by the following keywords: videos video you our support want this us channel patreon make on we if facebook to patreoncom can for and more watch </span>

<span class="sd">Based on the information above, extract a short topic label in the following format:</span>
<span class="sd">topic: &lt;topic label&gt;</span>
<span class="sd">&quot;&quot;&quot;</span>
</code></pre></div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Whenever you create a custom prompt, it is important to add 
<div class="highlight"><pre><span></span><code>Based on the information above, extract a short topic label in the following format:
topic: &lt;topic label&gt;
</code></pre></div>
at the end of your prompt as BERTopic extracts everything that comes after <code>topic:</code>. Having 
said that, if <code>topic:</code> is not in the output, then it will simply extract the entire response, so 
feel free to experiment with the prompts. </p>
</div>
<h2 id="version-0140"><strong>Version 0.14.0</strong><a class="headerlink" href="#version-0140" title="Permanent link">&para;</a></h2>
<p><em>Release date: 14 February, 2023</em></p>
<h3><b>Highlights:</a></b></h3>

<ul>
<li>Fine-tune <a href="https://maartengr.github.io/BERTopic/getting_started/representation/representation.html">topic representations</a> with <code>bertopic.representation</code><ul>
<li>Diverse range of models, including KeyBERT, MMR, POS, Transformers, OpenAI, and more!'</li>
<li>Create your own prompts for text generation models, like GPT3:<ul>
<li>Use <code>"[KEYWORDS]"</code> and <code>"[DOCUMENTS]"</code> in the prompt to decide where the keywords and set of representative documents need to be inserted.</li>
</ul>
</li>
<li>Chain models to perform fine-grained fine-tuning</li>
<li>Create and customize your represention model</li>
</ul>
</li>
<li>Improved the topic reduction technique when using <code>nr_topics=int</code></li>
<li>Added <code>title</code> parameters for all graphs (<a href="https://github.com/MaartenGr/BERTopic/issues/800">#800</a>)</li>
</ul>
<h3><b>Fixes:</a></b></h3>

<ul>
<li>Improve documentation (<a href="https://github.com/MaartenGr/BERTopic/issues/837">#837</a>, <a href="https://github.com/MaartenGr/BERTopic/issues/769">#769</a>, <a href="https://github.com/MaartenGr/BERTopic/issues/954">#954</a>, <a href="https://github.com/MaartenGr/BERTopic/issues/912">#912</a>, <a href="https://github.com/MaartenGr/BERTopic/issues/911">#911</a>)</li>
<li>Bump pyyaml (<a href="https://github.com/MaartenGr/BERTopic/issues/903">#903</a>)</li>
<li>Fix large number of representative docs (<a href="https://github.com/MaartenGr/BERTopic/issues/965">#965</a>)</li>
<li>Prevent stochastisch behavior in <code>.visualize_topics</code> (<a href="https://github.com/MaartenGr/BERTopic/issues/952">#952</a>)</li>
<li>Add custom labels parameter to <code>.visualize_topics</code> (<a href="https://github.com/MaartenGr/BERTopic/issues/976">#976</a>)</li>
<li>Fix cuML HDBSCAN type checks by <a href="https://github.com/FelSiq">@FelSiq</a> in <a href="https://github.com/MaartenGr/BERTopic/pull/981">#981</a></li>
</ul>
<h3><b>API Changes:</a></b></h3>
<ul>
<li>The <code>diversity</code> parameter was removed in favor of <code>bertopic.representation.MaximalMarginalRelevance</code></li>
<li>The <code>representation_model</code> parameter was added to <code>bertopic.BERTopic</code></li>
</ul>
<p><br>  </p>
<h3><b><a href="https://maartengr.github.io/BERTopic/getting_started/representation/representation.html#keybertinspired">Representation Models</a></b></h3>

<p>Fine-tune the c-TF-IDF representation with a variety of models. Whether that is through a KeyBERT-Inspired model or GPT-3, the choice is up to you! </p>
<iframe width="1200" height="500" src="https://user-images.githubusercontent.com/25746895/218417067-a81cc179-9055-49ba-a2b0-f2c1db535159.mp4
" title="BERTopic Overview" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

<p><br>  </p>
<h3><b><a href="https://maartengr.github.io/BERTopic/getting_started/representation/representation.html#keybertinspired">KeyBERTInspired</a></b></h3>

<p>The algorithm follows some principles of <a href="https://github.com/MaartenGr/KeyBERT">KeyBERT</a> but does some optimization in order to speed up inference. Usage is straightforward:</p>
<p><img alt="keybertinspired" src="https://user-images.githubusercontent.com/25746895/216336376-d2c4e5d6-6cf7-435c-904c-fc195aae7dcd.svg" /></p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">bertopic.representation</span> <span class="kn">import</span> <span class="n">KeyBERTInspired</span>
<span class="kn">from</span> <span class="nn">bertopic</span> <span class="kn">import</span> <span class="n">BERTopic</span>

<span class="c1"># Create your representation model</span>
<span class="n">representation_model</span> <span class="o">=</span> <span class="n">KeyBERTInspired</span><span class="p">()</span>

<span class="c1"># Use the representation model in BERTopic on top of the default pipeline</span>
<span class="n">topic_model</span> <span class="o">=</span> <span class="n">BERTopic</span><span class="p">(</span><span class="n">representation_model</span><span class="o">=</span><span class="n">representation_model</span><span class="p">)</span>
</code></pre></div>
<p><img alt="keybert" src="https://user-images.githubusercontent.com/25746895/218417161-bfd5980e-43c7-498a-904a-b6018ba58d45.svg" /></p>
<h3><b><a href="https://maartengr.github.io/BERTopic/getting_started/representation/representation.html#partofspeech">PartOfSpeech</a></b></h3>

<p>Our candidate topics, as extracted with c-TF-IDF, do not take into account a keyword's part of speech as extracting noun-phrases from all documents can be computationally quite expensive. Instead, we can leverage c-TF-IDF to perform part of speech on a subset of keywords and documents that best represent a topic. </p>
<p><img alt="partofspeech" src="https://user-images.githubusercontent.com/25746895/216336534-48ff400e-72e1-4c50-9030-414576bac01e.svg" /></p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">bertopic.representation</span> <span class="kn">import</span> <span class="n">PartOfSpeech</span>
<span class="kn">from</span> <span class="nn">bertopic</span> <span class="kn">import</span> <span class="n">BERTopic</span>

<span class="c1"># Create your representation model</span>
<span class="n">representation_model</span> <span class="o">=</span> <span class="n">PartOfSpeech</span><span class="p">(</span><span class="s2">&quot;en_core_web_sm&quot;</span><span class="p">)</span>

<span class="c1"># Use the representation model in BERTopic on top of the default pipeline</span>
<span class="n">topic_model</span> <span class="o">=</span> <span class="n">BERTopic</span><span class="p">(</span><span class="n">representation_model</span><span class="o">=</span><span class="n">representation_model</span><span class="p">)</span>
</code></pre></div>
<p><img alt="pos" src="https://user-images.githubusercontent.com/25746895/218417198-41c19b5c-251f-43c1-bfe2-0a480731565a.svg" /></p>
<h3><b><a href="https://maartengr.github.io/BERTopic/getting_started/representation/representation.html#maximalmarginalrelevance">MaximalMarginalRelevance</a></b></h3>

<p>When we calculate the weights of keywords, we typically do not consider whether we already have similar keywords in our topic. Words like "car" and "cars" 
essentially represent the same information and often redundant. We can use <code>MaximalMarginalRelevance</code> to improve diversity of our candidate topics:</p>
<p><img alt="mmr" src="https://user-images.githubusercontent.com/25746895/216336697-558f1409-8da3-4076-a21b-d87eec583ac7.svg" /></p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">bertopic.representation</span> <span class="kn">import</span> <span class="n">MaximalMarginalRelevance</span>
<span class="kn">from</span> <span class="nn">bertopic</span> <span class="kn">import</span> <span class="n">BERTopic</span>

<span class="c1"># Create your representation model</span>
<span class="n">representation_model</span> <span class="o">=</span> <span class="n">MaximalMarginalRelevance</span><span class="p">(</span><span class="n">diversity</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="c1"># Use the representation model in BERTopic on top of the default pipeline</span>
<span class="n">topic_model</span> <span class="o">=</span> <span class="n">BERTopic</span><span class="p">(</span><span class="n">representation_model</span><span class="o">=</span><span class="n">representation_model</span><span class="p">)</span>
</code></pre></div>
<p><img alt="mmr (1)" src="https://user-images.githubusercontent.com/25746895/218417234-88b145e2-7293-43c0-888c-36abe469a48a.svg" /></p>
<h3><b><a href="https://maartengr.github.io/BERTopic/getting_started/representation/representation.html#zero-shot-classification">Zero-Shot Classification</a></b></h3>

<p>To perform zero-shot classification, we feed the model with the keywords as generated through c-TF-IDF and a set of candidate labels. If, for a certain topic, we find a similar enough label, then it is assigned. If not, then we keep the original c-TF-IDF keywords. </p>
<p>We use it in BERTopic as follows:</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">bertopic.representation</span> <span class="kn">import</span> <span class="n">ZeroShotClassification</span>
<span class="kn">from</span> <span class="nn">bertopic</span> <span class="kn">import</span> <span class="n">BERTopic</span>

<span class="c1"># Create your representation model</span>
<span class="n">candidate_topics</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;space and nasa&quot;</span><span class="p">,</span> <span class="s2">&quot;bicycles&quot;</span><span class="p">,</span> <span class="s2">&quot;sports&quot;</span><span class="p">]</span>
<span class="n">representation_model</span> <span class="o">=</span> <span class="n">ZeroShotClassification</span><span class="p">(</span><span class="n">candidate_topics</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="s2">&quot;facebook/bart-large-mnli&quot;</span><span class="p">)</span>

<span class="c1"># Use the representation model in BERTopic on top of the default pipeline</span>
<span class="n">topic_model</span> <span class="o">=</span> <span class="n">BERTopic</span><span class="p">(</span><span class="n">representation_model</span><span class="o">=</span><span class="n">representation_model</span><span class="p">)</span>
</code></pre></div>
<p><img alt="zero" src="https://user-images.githubusercontent.com/25746895/218417276-dcef3519-acba-4792-8601-45dc7ed39488.svg" /></p>
<h3><b><a href="https://maartengr.github.io/BERTopic/getting_started/representation/representation.html#transformers">Text Generation: ðŸ¤— Transformers</a></b></h3>

<p>Nearly every week, there are new and improved models released on the ðŸ¤— <a href="https://huggingface.co/models">Model Hub</a> that, with some creativity, allow for 
further fine-tuning of our c-TF-IDF based topics. These models range from text generation to zero-classification. In BERTopic, wrappers around these 
methods are created as a way to support whatever might be released in the future. </p>
<p>Using a GPT-like model from the huggingface hub is rather straightforward:</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">bertopic.representation</span> <span class="kn">import</span> <span class="n">TextGeneration</span>
<span class="kn">from</span> <span class="nn">bertopic</span> <span class="kn">import</span> <span class="n">BERTopic</span>

<span class="c1"># Create your representation model</span>
<span class="n">representation_model</span> <span class="o">=</span> <span class="n">TextGeneration</span><span class="p">(</span><span class="s1">&#39;gpt2&#39;</span><span class="p">)</span>

<span class="c1"># Use the representation model in BERTopic on top of the default pipeline</span>
<span class="n">topic_model</span> <span class="o">=</span> <span class="n">BERTopic</span><span class="p">(</span><span class="n">representation_model</span><span class="o">=</span><span class="n">representation_model</span><span class="p">)</span>
</code></pre></div>
<p><img alt="hf" src="https://user-images.githubusercontent.com/25746895/218417310-2b0eabc7-296d-499d-888b-0ab48a65a2fb.svg" /></p>
<h3><b><a href="https://maartengr.github.io/BERTopic/getting_started/representation/representation.html#cohere">Text Generation: Cohere</a></b></h3>

<p>Instead of using a language model from ðŸ¤— transformers, we can use external APIs instead that 
do the work for you. Here, we can use <a href="https://docs.cohere.ai/">Cohere</a> to extract our topic labels from the candidate documents and keywords.
To use this, you will need to install cohere first:</p>
<div class="highlight"><pre><span></span><code>pip<span class="w"> </span>install<span class="w"> </span>cohere
</code></pre></div>
<p>Then, get yourself an API key and use Cohere's API as follows:</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">cohere</span>
<span class="kn">from</span> <span class="nn">bertopic.representation</span> <span class="kn">import</span> <span class="n">Cohere</span>
<span class="kn">from</span> <span class="nn">bertopic</span> <span class="kn">import</span> <span class="n">BERTopic</span>

<span class="c1"># Create your representation model</span>
<span class="n">co</span> <span class="o">=</span> <span class="n">cohere</span><span class="o">.</span><span class="n">Client</span><span class="p">(</span><span class="n">my_api_key</span><span class="p">)</span>
<span class="n">representation_model</span> <span class="o">=</span> <span class="n">Cohere</span><span class="p">(</span><span class="n">co</span><span class="p">)</span>

<span class="c1"># Use the representation model in BERTopic on top of the default pipeline</span>
<span class="n">topic_model</span> <span class="o">=</span> <span class="n">BERTopic</span><span class="p">(</span><span class="n">representation_model</span><span class="o">=</span><span class="n">representation_model</span><span class="p">)</span>
</code></pre></div>
<p><img alt="cohere" src="https://user-images.githubusercontent.com/25746895/218417337-294cb52a-93c9-4fd5-b981-29b40e4f0c1e.svg" /></p>
<h3><b><a href="https://maartengr.github.io/BERTopic/getting_started/representation/representation.html#openai">Text Generation: OpenAI</a></b></h3>

<p>Instead of using a language model from ðŸ¤— transformers, we can use external APIs instead that 
do the work for you. Here, we can use <a href="https://openai.com/api/">OpenAI</a> to extract our topic labels from the candidate documents and keywords.
To use this, you will need to install openai first:</p>
<div class="highlight"><pre><span></span><code>pip install openai
</code></pre></div>
<p>Then, get yourself an API key and use OpenAI's API as follows:</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">openai</span>
<span class="kn">from</span> <span class="nn">bertopic.representation</span> <span class="kn">import</span> <span class="n">OpenAI</span>
<span class="kn">from</span> <span class="nn">bertopic</span> <span class="kn">import</span> <span class="n">BERTopic</span>

<span class="c1"># Create your representation model</span>
<span class="n">openai</span><span class="o">.</span><span class="n">api_key</span> <span class="o">=</span> <span class="n">MY_API_KEY</span>
<span class="n">representation_model</span> <span class="o">=</span> <span class="n">OpenAI</span><span class="p">()</span>

<span class="c1"># Use the representation model in BERTopic on top of the default pipeline</span>
<span class="n">topic_model</span> <span class="o">=</span> <span class="n">BERTopic</span><span class="p">(</span><span class="n">representation_model</span><span class="o">=</span><span class="n">representation_model</span><span class="p">)</span>
</code></pre></div>
<p><img alt="openai" src="https://user-images.githubusercontent.com/25746895/218417357-cf8c0fab-4450-43d3-b4fd-219ed276d870.svg" /></p>
<h3><b><a href="https://maartengr.github.io/BERTopic/getting_started/representation/representation.html#langchain">Text Generation: LangChain</a></b></h3>

<p><a href="https://github.com/hwchase17/langchain">Langchain</a> is a package that helps users with chaining large language models.
In BERTopic, we can leverage this package in order to more efficiently combine external knowledge. Here, this 
external knowledge are the most representative documents in each topic. </p>
<p>To use langchain, you will need to install the langchain package first. Additionally, you will need an underlying LLM to support langchain,
like openai:</p>
<div class="highlight"><pre><span></span><code>pip<span class="w"> </span>install<span class="w"> </span>langchain,<span class="w"> </span>openai
</code></pre></div>
<p>Then, you can create your chain as follows:</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">langchain.chains.question_answering</span> <span class="kn">import</span> <span class="n">load_qa_chain</span>
<span class="kn">from</span> <span class="nn">langchain.llms</span> <span class="kn">import</span> <span class="n">OpenAI</span>
<span class="n">chain</span> <span class="o">=</span> <span class="n">load_qa_chain</span><span class="p">(</span><span class="n">OpenAI</span><span class="p">(</span><span class="n">temperature</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">openai_api_key</span><span class="o">=</span><span class="n">MY_API_KEY</span><span class="p">),</span> <span class="n">chain_type</span><span class="o">=</span><span class="s2">&quot;stuff&quot;</span><span class="p">)</span>
</code></pre></div>
<p>Finally, you can pass the chain to BERTopic as follows:</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">bertopic.representation</span> <span class="kn">import</span> <span class="n">LangChain</span>

<span class="c1"># Create your representation model</span>
<span class="n">representation_model</span> <span class="o">=</span> <span class="n">LangChain</span><span class="p">(</span><span class="n">chain</span><span class="p">)</span>

<span class="c1"># Use the representation model in BERTopic on top of the default pipeline</span>
<span class="n">topic_model</span> <span class="o">=</span> <span class="n">BERTopic</span><span class="p">(</span><span class="n">representation_model</span><span class="o">=</span><span class="n">representation_model</span><span class="p">)</span>
</code></pre></div>
<h2 id="version-0130"><strong>Version 0.13.0</strong><a class="headerlink" href="#version-0130" title="Permanent link">&para;</a></h2>
<p><em>Release date: 4 January, 2023</em></p>
<h3><b>Highlights:</a></b></h3>

<ul>
<li>Calculate <a href="https://maartengr.github.io/BERTopic/getting_started/distribution/distribution.html">topic distributions</a> with <code>.approximate_distribution</code> regardless of the cluster model used<ul>
<li>Generates topic distributions on a document- and token-levels</li>
<li>Can be used for any document regardless of its size!</li>
</ul>
</li>
<li><a href="https://maartengr.github.io/BERTopic/getting_started/supervised/supervised.html">Fully supervised BERTopic</a><ul>
<li>You can now use a classification model for the clustering step instead to create a fully supervised topic model</li>
</ul>
</li>
<li><a href="https://maartengr.github.io/BERTopic/getting_started/manual/manual.html">Manual topic modeling</a><ul>
<li>Generate topic representations from labels directly</li>
<li>Allows for skipping the embedding and clustering steps in order to go directly to the topic representation step</li>
</ul>
</li>
<li><a href="https://maartengr.github.io/BERTopic/getting_started/outlier_reduction/outlier_reduction.html">Reduce outliers</a> with 4 different strategies using <code>.reduce_outliers</code></li>
<li>Install BERTopic without <code>SentenceTransformers</code> for a <a href="https://maartengr.github.io/BERTopic/getting_started/tips_and_tricks/tips_and_tricks.html#lightweight-installation">lightweight package</a>:<ul>
<li><code>pip install --no-deps bertopic</code></li>
<li><code>pip install --upgrade numpy hdbscan umap-learn pandas scikit-learn tqdm plotly pyyaml</code></li>
</ul>
</li>
<li>Get meta data of trained documents such as topics and probabilities using <code>.get_document_info(docs)</code></li>
<li>Added more support for cuML's HDBSCAN<ul>
<li>Calculate and predict probabilities during <code>fit_transform</code>  and <code>transform</code> respectively</li>
<li>This should give a major speed-up when setting <code>calculate_probabilities=True</code></li>
</ul>
</li>
<li>More images to the documentation and a lot of changes/updates/clarifications</li>
<li>Get representative documents for non-HDBSCAN models by comparing document and topic c-TF-IDF representations </li>
<li>Sklearn Pipeline <a href="https://maartengr.github.io/BERTopic/getting_started/embeddings/embeddings.html#scikit-learn-embeddings">Embedder</a> by <a href="https://github.com/koaning">@koaning</a> in <a href="https://github.com/MaartenGr/BERTopic/pull/791">#791</a></li>
</ul>
<h3><b>Fixes:</a></b></h3>

<ul>
<li>Improve <code>.partial_fit</code> documentation (<a href="https://github.com/MaartenGr/BERTopic/issues/837">#837</a>)</li>
<li>Fixed scipy linkage usage (<a href="https://github.com/MaartenGr/BERTopic/issues/807">#807</a>)</li>
<li>Fixed shifted heatmap (<a href="https://github.com/MaartenGr/BERTopic/issues/782">#782</a>)</li>
<li>Fixed SpaCy backend (<a href="https://github.com/MaartenGr/BERTopic/issues/744">#744</a>)</li>
<li>Fixed representative docs with small clusters (&lt;3) (<a href="https://github.com/MaartenGr/BERTopic/issues/703">#703</a>)</li>
<li>Typo fixed by <a href="https://github.com/timpal0l">@timpal0l</a> in <a href="https://github.com/MaartenGr/BERTopic/pull/734">#734</a></li>
<li>Typo fixed by <a href="https://github.com/timpal0l">@srulikbd</a> in <a href="https://github.com/MaartenGr/BERTopic/pull/842">#842</a></li>
<li>Correcting iframe urls by <a href="https://github.com/Mustapha-AJEGHRIR">@Mustapha-AJEGHRIR</a> in <a href="https://github.com/MaartenGr/BERTopic/pull/798">#798</a></li>
<li>Refactor embedding methods by <a href="https://github.com/zachschillaci27">@zachschillaci27</a> in <a href="https://github.com/MaartenGr/BERTopic/pull/855">#855</a></li>
<li>Added diversity parameter to update_topics() function by <a href="https://github.com/anubhabdaserrr">@anubhabdaserrr</a> in <a href="https://github.com/MaartenGr/BERTopic/pull/887">#887</a></li>
</ul>
<h3><b><a href="https://maartengr.github.io/BERTopic/algorithm/algorithm.html">Documentation</a></b></h3>

<p>Personally, I believe that documentation can be seen as a feature and is an often underestimated aspect of open-source. So I went a bit overboardðŸ˜…... and created an animation about the three pillars of BERTopic using Manim. There are many other visualizations added, one of each variation of BERTopic, and many smaller changes. </p>
<iframe width="1200" height="500" src="https://user-images.githubusercontent.com/25746895/205490350-cd9833e7-9cd5-44fa-8752-407d748de633.mp4
" title="BERTopic Overview" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

<h3><b><a href="https://maartengr.github.io/BERTopic/getting_started/distribution/distribution.html">Topic Distributions</a></b></h3>

<p>The difficulty with a cluster-based topic modeling technique is that it does not directly consider that documents may contain multiple topics. With the new release, we can now model the distributions of topics! We even consider that a single word might be related to multiple topics. If a document is a mixture of topics, what is preventing a single word to be the same? </p>
<p>To do so, we approximate the distribution of topics in a document by calculating and summing the similarities of tokensets (achieved by applying a sliding window) with the topics:</p>
<div class="highlight"><pre><span></span><code><span class="c1"># After fitting your model run the following for either your trained documents or even unseen documents</span>
<span class="n">topic_distr</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">topic_model</span><span class="o">.</span><span class="n">approximate_distribution</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>
</code></pre></div>
<p>To calculate and visualize the topic distributions in a document on a token-level, we can run the following:</p>
<div class="highlight"><pre><span></span><code><span class="c1"># We need to calculate the topic distributions on a token level</span>
<span class="n">topic_distr</span><span class="p">,</span> <span class="n">topic_token_distr</span> <span class="o">=</span> <span class="n">topic_model</span><span class="o">.</span><span class="n">approximate_distribution</span><span class="p">(</span><span class="n">docs</span><span class="p">,</span> <span class="n">calculate_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Create a visualization using a styled dataframe if Jinja2 is installed</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">topic_model</span><span class="o">.</span><span class="n">visualize_approximate_distribution</span><span class="p">(</span><span class="n">docs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">topic_token_distr</span><span class="p">[</span><span class="mi">0</span><span class="p">]);</span> <span class="n">df</span>
</code></pre></div>
<h3><b><a href="https://maartengr.github.io/BERTopic/getting_started/supervised/supervised.html">Supervised Topic Modeling</a></b></h3>

<p>BERTopic now supports fully-supervised classification! Instead of using a clustering algorithm, like HDBSCAN, we can replace it with a classifier, like Logistic Regression:</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">bertopic</span> <span class="kn">import</span> <span class="n">BERTopic</span>
<span class="kn">from</span> <span class="nn">bertopic.dimensionality</span> <span class="kn">import</span> <span class="n">BaseDimensionalityReduction</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">fetch_20newsgroups</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>

<span class="c1"># Get labeled data</span>
<span class="n">data</span><span class="o">=</span> <span class="n">fetch_20newsgroups</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="s1">&#39;all&#39;</span><span class="p">,</span>  <span class="n">remove</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;headers&#39;</span><span class="p">,</span> <span class="s1">&#39;footers&#39;</span><span class="p">,</span> <span class="s1">&#39;quotes&#39;</span><span class="p">))</span>
<span class="n">docs</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;data&#39;</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;target&#39;</span><span class="p">]</span>

<span class="c1"># Allows us to skip over the dimensionality reduction step</span>
<span class="n">empty_dimensionality_model</span> <span class="o">=</span> <span class="n">BaseDimensionalityReduction</span><span class="p">()</span>

<span class="c1"># Create a classifier to be used instead of the cluster model</span>
<span class="n">clf</span><span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">()</span>

<span class="c1"># Create a fully supervised BERTopic instance</span>
<span class="n">topic_model</span><span class="o">=</span> <span class="n">BERTopic</span><span class="p">(</span>
        <span class="n">umap_model</span><span class="o">=</span><span class="n">empty_dimensionality_model</span><span class="p">,</span>
        <span class="n">hdbscan_model</span><span class="o">=</span><span class="n">clf</span>
<span class="p">)</span>
<span class="n">topics</span><span class="p">,</span> <span class="n">probs</span> <span class="o">=</span> <span class="n">topic_model</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">docs</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>
</code></pre></div>
<h3><b><a href="https://maartengr.github.io/BERTopic/getting_started/manual/manual.html">Manual Topic Modeling</a></b></h3>

<p>When you already have a bunch of labels and simply want to extract topic representations from them, you might not need to actually learn how those can predicted. We can bypass the <code>embeddings -&gt; dimensionality reduction -&gt; clustering</code> steps and go straight to the c-TF-IDF representation of our labels:</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">bertopic</span> <span class="kn">import</span> <span class="n">BERTopic</span>
<span class="kn">from</span> <span class="nn">bertopic.backend</span> <span class="kn">import</span> <span class="n">BaseEmbedder</span>
<span class="kn">from</span> <span class="nn">bertopic.cluster</span> <span class="kn">import</span> <span class="n">BaseCluster</span>
<span class="kn">from</span> <span class="nn">bertopic.dimensionality</span> <span class="kn">import</span> <span class="n">BaseDimensionalityReduction</span>

<span class="c1"># Prepare our empty sub-models and reduce frequent words while we are at it.</span>
<span class="n">empty_embedding_model</span> <span class="o">=</span> <span class="n">BaseEmbedder</span><span class="p">()</span>
<span class="n">empty_dimensionality_model</span> <span class="o">=</span> <span class="n">BaseDimensionalityReduction</span><span class="p">()</span>
<span class="n">empty_cluster_model</span> <span class="o">=</span> <span class="n">BaseCluster</span><span class="p">()</span>

<span class="c1"># Fit BERTopic without actually performing any clustering</span>
<span class="n">topic_model</span><span class="o">=</span> <span class="n">BERTopic</span><span class="p">(</span>
        <span class="n">embedding_model</span><span class="o">=</span><span class="n">empty_embedding_model</span><span class="p">,</span>
        <span class="n">umap_model</span><span class="o">=</span><span class="n">empty_dimensionality_model</span><span class="p">,</span>
        <span class="n">hdbscan_model</span><span class="o">=</span><span class="n">empty_cluster_model</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">topics</span><span class="p">,</span> <span class="n">probs</span> <span class="o">=</span> <span class="n">topic_model</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">docs</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>
</code></pre></div>
<h3><b><a href="https://maartengr.github.io/BERTopic/getting_started/outlier_reduction/outlier_reduction.html">Outlier Reduction</a></b></h3>

<p>Outlier reduction is an frequently-discussed topic in BERTopic as its default cluster model, HDBSCAN, has a tendency to generate many outliers. This often helps in the topic representation steps, as we do not consider documents that are less relevant, but you might want to still assign those outliers to actual topics. In the modular philosophy of BERTopic, keeping training times in mind, it is now possible to perform outlier reduction <strong>after</strong> having trained your topic model. This allows for ease of iteration and prevents having to train BERTopic many times to find the parameters you are searching for. There are 4 different strategies that you can use, so make sure to check out the <a href="https://maartengr.github.io/BERTopic/getting_started/outlier_reduction/outlier_reduction.html">documentation</a>!</p>
<p>Using it is rather straightforward:</p>
<div class="highlight"><pre><span></span><code><span class="n">new_topics</span> <span class="o">=</span> <span class="n">topic_model</span><span class="o">.</span><span class="n">reduce_outliers</span><span class="p">(</span><span class="n">docs</span><span class="p">,</span> <span class="n">topics</span><span class="p">)</span>
</code></pre></div>
<h3><b><a href="https://maartengr.github.io/BERTopic/getting_started/tips_and_tricks/tips_and_tricks.html#lightweight-installation">Lightweight BERTopic</a></b></h3>

<p>The default embedding model in BERTopic is one of the amazing sentence-transformers models, namely <code>"all-MiniLM-L6-v2"</code>. Although this model performs well out of the box, it typically needs a GPU to transform the documents into embeddings in a reasonable time. Moreover, the installation requires <code>pytorch</code> which often results in a rather large environment, memory-wise. </p>
<p>Fortunately, it is possible to install BERTopic without <code>sentence-transformers</code> and use it as a lightweight solution instead. The installation can be done as follows:</p>
<div class="highlight"><pre><span></span><code>pip<span class="w"> </span>install<span class="w"> </span>--no-deps<span class="w"> </span>bertopic
pip<span class="w"> </span>install<span class="w"> </span>--upgrade<span class="w"> </span>numpy<span class="w"> </span>hdbscan<span class="w"> </span>umap-learn<span class="w"> </span>pandas<span class="w"> </span>scikit-learn<span class="w"> </span>tqdm<span class="w"> </span>plotly<span class="w"> </span>pyyaml
</code></pre></div>
<p>Then, we can use BERTopic without <code>sentence-transformers</code> as follows using a CPU-based embedding technique:</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">make_pipeline</span>
<span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">TruncatedSVD</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">TfidfVectorizer</span>

<span class="n">pipe</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span>
    <span class="n">TfidfVectorizer</span><span class="p">(),</span>
    <span class="n">TruncatedSVD</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="p">)</span>

<span class="n">topic_model</span> <span class="o">=</span> <span class="n">BERTopic</span><span class="p">(</span><span class="n">embedding_model</span><span class="o">=</span><span class="n">pipe</span><span class="p">)</span>
</code></pre></div>
<p>As a result, the entire package and resulting model can be run quickly on the CPU and no GPU is necessary!</p>
<h3><b><a href="https://maartengr.github.io/BERTopic/api/bertopic.html#bertopic._bertopic.BERTopic.get_document_info">Document Information</a></b></h3>

<p>Get information about the documents on which the topic was trained including the documents themselves, their respective topics, the name of each topic, the top n words of each topic, whether it is a representative document, and the probability of the clustering if the cluster model supports it. There are also options to include other metadata, such as the topic distributions or the x and y coordinates of the reduced embeddings that you can learn more about <a href="https://maartengr.github.io/BERTopic/api/bertopic.html#bertopic._bertopic.BERTopic.get_document_info">here</a>.</p>
<p>To get the document info, you will only need to pass the documents on which the topic model was trained:</p>
<div class="highlight"><pre><span></span><code><span class="o">&gt;&gt;&gt;</span> <span class="n">topic_model</span><span class="o">.</span><span class="n">get_document_info</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>

<span class="n">Document</span>                               <span class="n">Topic</span>    <span class="n">Name</span>                        <span class="n">Top_n_words</span>                     <span class="n">Probability</span>    <span class="o">...</span>
<span class="n">I</span> <span class="n">am</span> <span class="n">sure</span> <span class="n">some</span> <span class="n">bashers</span> <span class="n">of</span> <span class="n">Pens</span><span class="o">...</span>       <span class="mi">0</span>       <span class="mi">0</span><span class="n">_game_team_games_season</span>    <span class="n">game</span> <span class="o">-</span> <span class="n">team</span> <span class="o">-</span> <span class="n">games</span><span class="o">...</span>          <span class="mf">0.200010</span>       <span class="o">...</span>
<span class="n">My</span> <span class="n">brother</span> <span class="ow">is</span> <span class="ow">in</span> <span class="n">the</span> <span class="n">market</span> <span class="k">for</span><span class="o">...</span>      <span class="o">-</span><span class="mi">1</span>     <span class="o">-</span><span class="mi">1</span><span class="n">_can_your_will_any</span>         <span class="n">can</span> <span class="o">-</span> <span class="n">your</span> <span class="o">-</span> <span class="n">will</span><span class="o">...</span>            <span class="mf">0.420668</span>       <span class="o">...</span>
<span class="n">Finally</span> <span class="n">you</span> <span class="n">said</span> <span class="n">what</span> <span class="n">you</span> <span class="n">dream</span><span class="o">...</span>      <span class="o">-</span><span class="mi">1</span>     <span class="o">-</span><span class="mi">1</span><span class="n">_can_your_will_any</span>         <span class="n">can</span> <span class="o">-</span> <span class="n">your</span> <span class="o">-</span> <span class="n">will</span><span class="o">...</span>            <span class="mf">0.807259</span>       <span class="o">...</span>
<span class="n">Think</span><span class="err">!</span> <span class="n">It</span> <span class="ow">is</span> <span class="n">the</span> <span class="n">SCSI</span> <span class="n">card</span> <span class="n">doing</span><span class="o">...</span>     <span class="mi">49</span>     <span class="mi">49</span><span class="n">_windows_drive_dos_file</span>    <span class="n">windows</span> <span class="o">-</span> <span class="n">drive</span> <span class="o">-</span> <span class="n">docs</span><span class="o">...</span>       <span class="mf">0.071746</span>       <span class="o">...</span>
<span class="mi">1</span><span class="p">)</span> <span class="n">I</span> <span class="n">have</span> <span class="n">an</span> <span class="n">old</span> <span class="n">Jasmine</span> <span class="n">drive</span><span class="o">...</span>       <span class="mi">49</span>     <span class="mi">49</span><span class="n">_windows_drive_dos_file</span>    <span class="n">windows</span> <span class="o">-</span> <span class="n">drive</span> <span class="o">-</span> <span class="n">docs</span><span class="o">...</span>       <span class="mf">0.038983</span>       <span class="o">...</span>
</code></pre></div>
<h2 id="version-0120"><strong>Version 0.12.0</strong><a class="headerlink" href="#version-0120" title="Permanent link">&para;</a></h2>
<p><em>Release date: 5 September, 2022</em></p>
<p><strong>Highlights</strong>:</p>
<ul>
<li>Perform <a href="https://maartengr.github.io/BERTopic/getting_started/online/online.html">online/incremental topic modeling</a> with <code>.partial_fit</code></li>
<li>Expose <a href="https://maartengr.github.io/BERTopic/getting_started/ctfidf/ctfidf.html">c-TF-IDF model</a> for customization with <code>bertopic.vectorizers.ClassTfidfTransformer</code><ul>
<li>The parameters <code>bm25_weighting</code> and <code>reduce_frequent_words</code> were added to potentially improve representations:</li>
</ul>
</li>
<li>Expose attributes for easier access to internal data</li>
<li>Major changes to the <a href="https://maartengr.github.io/BERTopic/algorithm/algorithm.html">Algorithm</a> page of the documentation, which now contains three overviews of the algorithm:<ul>
<li><a href="https://maartengr.github.io/BERTopic/algorithm/algorithm.html#visual-overview">Visualize Overview</a></li>
<li><a href="https://maartengr.github.io/BERTopic/algorithm/algorithm.html#code-overview">Code Overview</a></li>
<li><a href="https://maartengr.github.io/BERTopic/algorithm/algorithm.html#detailed-overview">Detailed Overview</a></li>
</ul>
</li>
<li>Added an <a href="https://maartengr.github.io/BERTopic/getting_started/tips_and_tricks/tips_and_tricks.html#keybert-bertopic">example</a> of combining BERTopic with KeyBERT</li>
<li>Added many tests with the intention of making development a bit more stable</li>
</ul>
<p><strong>Fixes</strong>: </p>
<ul>
<li>Fixed iteratively merging topics (<a href="https://github.com/MaartenGr/BERTopic/issues/632">#632</a> and (<a href="https://github.com/MaartenGr/BERTopic/issues/648">#648</a>)</li>
<li>Fixed 0th topic not showing up in visualizations (<a href="https://github.com/MaartenGr/BERTopic/issues/667">#667</a>)</li>
<li>Fixed lowercasing not being optional (<a href="https://github.com/MaartenGr/BERTopic/issues/682">#682</a>)</li>
<li>Fixed spelling (<a href="https://github.com/MaartenGr/BERTopic/issues/664">#664</a> and (<a href="https://github.com/MaartenGr/BERTopic/issues/673">#673</a>)</li>
<li>Fixed 0th topic not shown in <code>.get_topic_info</code> by <a href="https://github.com/oxymor0n">@oxymor0n</a> in <a href="https://github.com/MaartenGr/BERTopic/pull/660">#660</a></li>
<li>Fixed spelling by <a href="https://github.com/domenicrosati">@domenicrosati</a> in <a href="https://github.com/MaartenGr/BERTopic/pull/674">#674</a></li>
<li>Add custom labels and title options to barchart <a href="https://github.com/leloykun">@leloykun</a> in <a href="https://github.com/MaartenGr/BERTopic/pull/694">#694</a></li>
</ul>
<p><strong>Online/incremental topic modeling</strong>:</p>
<p>Online topic modeling (sometimes called "incremental topic modeling") is the ability to learn incrementally from a mini-batch of instances. Essentially, it is a way to update your topic model with data on which it was not trained on before. In Scikit-Learn, this technique is often modeled through a <code>.partial_fit</code> function, which is also used in BERTopic. </p>
<p>At a minimum, the cluster model needs to support a <code>.partial_fit</code> function in order to use this feature. The default HDBSCAN model will not work as it does not support online updating. </p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">fetch_20newsgroups</span>
<span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">MiniBatchKMeans</span>
<span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">IncrementalPCA</span>
<span class="kn">from</span> <span class="nn">bertopic.vectorizers</span> <span class="kn">import</span> <span class="n">OnlineCountVectorizer</span>
<span class="kn">from</span> <span class="nn">bertopic</span> <span class="kn">import</span> <span class="n">BERTopic</span>

<span class="c1"># Prepare documents</span>
<span class="n">all_docs</span> <span class="o">=</span> <span class="n">fetch_20newsgroups</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="n">subset</span><span class="p">,</span>  <span class="n">remove</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;headers&#39;</span><span class="p">,</span> <span class="s1">&#39;footers&#39;</span><span class="p">,</span> <span class="s1">&#39;quotes&#39;</span><span class="p">))[</span><span class="s2">&quot;data&quot;</span><span class="p">]</span>
<span class="n">doc_chunks</span> <span class="o">=</span> <span class="p">[</span><span class="n">all_docs</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="mi">1000</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">all_docs</span><span class="p">),</span> <span class="mi">1000</span><span class="p">)]</span>

<span class="c1"># Prepare sub-models that support online learning</span>
<span class="n">umap_model</span> <span class="o">=</span> <span class="n">IncrementalPCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">cluster_model</span> <span class="o">=</span> <span class="n">MiniBatchKMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">vectorizer_model</span> <span class="o">=</span> <span class="n">OnlineCountVectorizer</span><span class="p">(</span><span class="n">stop_words</span><span class="o">=</span><span class="s2">&quot;english&quot;</span><span class="p">,</span> <span class="n">decay</span><span class="o">=</span><span class="mf">.01</span><span class="p">)</span>

<span class="n">topic_model</span> <span class="o">=</span> <span class="n">BERTopic</span><span class="p">(</span><span class="n">umap_model</span><span class="o">=</span><span class="n">umap_model</span><span class="p">,</span>
                       <span class="n">hdbscan_model</span><span class="o">=</span><span class="n">cluster_model</span><span class="p">,</span>
                       <span class="n">vectorizer_model</span><span class="o">=</span><span class="n">vectorizer_model</span><span class="p">)</span>

<span class="c1"># Incrementally fit the topic model by training on 1000 documents at a time</span>
<span class="k">for</span> <span class="n">docs</span> <span class="ow">in</span> <span class="n">doc_chunks</span><span class="p">:</span>
    <span class="n">topic_model</span><span class="o">.</span><span class="n">partial_fit</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>
</code></pre></div>
<p>Only the topics for the most recent batch of documents are tracked. If you want to be using online topic modeling, not for a streaming setting but merely for low-memory use cases, then it is advised to also update the <code>.topics_</code> attribute as variations such as hierarchical topic modeling will not work afterward:</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Incrementally fit the topic model by training on 1000 documents at a time and track the topics in each iteration</span>
<span class="n">topics</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">docs</span> <span class="ow">in</span> <span class="n">doc_chunks</span><span class="p">:</span>
    <span class="n">topic_model</span><span class="o">.</span><span class="n">partial_fit</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>
    <span class="n">topics</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">topic_model</span><span class="o">.</span><span class="n">topics_</span><span class="p">)</span>

<span class="n">topic_model</span><span class="o">.</span><span class="n">topics_</span> <span class="o">=</span> <span class="n">topics</span>
</code></pre></div>
<p><strong>c-TF-IDF</strong>:</p>
<p>Explicitly define, use, and adjust the <code>ClassTfidfTransformer</code> with new parameters, <code>bm25_weighting</code> and <code>reduce_frequent_words</code>, to potentially improve the topic representation: </p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">bertopic</span> <span class="kn">import</span> <span class="n">BERTopic</span>
<span class="kn">from</span> <span class="nn">bertopic.vectorizers</span> <span class="kn">import</span> <span class="n">ClassTfidfTransformer</span>

<span class="n">ctfidf_model</span> <span class="o">=</span> <span class="n">ClassTfidfTransformer</span><span class="p">(</span><span class="n">bm25_weighting</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">topic_model</span> <span class="o">=</span> <span class="n">BERTopic</span><span class="p">(</span><span class="n">ctfidf_model</span><span class="o">=</span><span class="n">ctfidf_model</span><span class="p">)</span>
</code></pre></div>
<p><strong>Attributes</strong>:</p>
<p>After having fitted your BERTopic instance, you can use the following attributes to have quick access to certain information, such as the topic assignment for each document in <code>topic_model.topics_</code>. </p>
<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>topics_</td>
<td>List[int]</td>
<td>The topics that are generated for each document after training or updating the topic model. The most recent topics are tracked.</td>
</tr>
<tr>
<td>probabilities_</td>
<td>List[float]</td>
<td>The probability of the assigned topic per document. These are only calculated if a HDBSCAN model is used for the clustering step. When <code>calculate_probabilities=True</code>, then it is the probabilities of all topics per document.</td>
</tr>
<tr>
<td>topic_sizes_</td>
<td>Mapping[int, int]</td>
<td>The size of each topic.</td>
</tr>
<tr>
<td>topic_mapper_</td>
<td>TopicMapper</td>
<td>A class for tracking topics and their mappings anytime they are merged, reduced, added, or removed.</td>
</tr>
<tr>
<td>topic_representations_</td>
<td>Mapping[int, Tuple[int, float]]</td>
<td>The top <em>n</em> terms per topic and their respective c-TF-IDF values.</td>
</tr>
<tr>
<td>c_tf_idf_</td>
<td>csr_matrix</td>
<td>The topic-term matrix as calculated through c-TF-IDF. To access its respective words, run <code>.vectorizer_model.get_feature_names()</code> or <code>.vectorizer_model.get_feature_names_out()</code></td>
</tr>
<tr>
<td>topic_labels_</td>
<td>Mapping[int, str]</td>
<td>The default labels for each topic.</td>
</tr>
<tr>
<td>custom_labels_</td>
<td>List[str]</td>
<td>Custom labels for each topic as generated through <code>.set_topic_labels</code>.</td>
</tr>
<tr>
<td>topic_embeddings_</td>
<td>np.ndarray</td>
<td>The embeddings for each topic. It is calculated by taking the weighted average of word embeddings in a topic based on their c-TF-IDF values.</td>
</tr>
<tr>
<td>representative_docs_</td>
<td>Mapping[int, str]</td>
<td>The representative documents for each topic if HDBSCAN is used.</td>
</tr>
</tbody>
</table>
<h2 id="version-0110"><strong>Version 0.11.0</strong><a class="headerlink" href="#version-0110" title="Permanent link">&para;</a></h2>
<p><em>Release date: 11 July, 2022</em></p>
<p><strong>Highlights</strong>:</p>
<ul>
<li>Perform <a href="https://maartengr.github.io/BERTopic/getting_started/hierarchicaltopics/hierarchicaltopics.html">hierarchical topic modeling</a> with <code>.hierarchical_topics</code></li>
</ul>
<div class="highlight"><pre><span></span><code><span class="n">hierarchical_topics</span> <span class="o">=</span> <span class="n">topic_model</span><span class="o">.</span><span class="n">hierarchical_topics</span><span class="p">(</span><span class="n">docs</span><span class="p">,</span> <span class="n">topics</span><span class="p">)</span> 
</code></pre></div>
<ul>
<li>Visualize <a href="https://maartengr.github.io/BERTopic/getting_started/hierarchicaltopics/hierarchicaltopics.html#visualizations">hierarchical topic representations</a> with <code>.visualize_hierarchy</code></li>
</ul>
<div class="highlight"><pre><span></span><code><span class="n">topic_model</span><span class="o">.</span><span class="n">visualize_hierarchy</span><span class="p">(</span><span class="n">hierarchical_topics</span><span class="o">=</span><span class="n">hierarchical_topics</span><span class="p">)</span>
</code></pre></div>
<ul>
<li>Extract a <a href="https://maartengr.github.io/BERTopic/getting_started/hierarchicaltopics/hierarchicaltopics.html#visualizations">text-based hierarchical topic representation</a> with <code>.get_topic_tree</code></li>
</ul>
<div class="highlight"><pre><span></span><code><span class="n">tree</span> <span class="o">=</span> <span class="n">topic_model</span><span class="o">.</span><span class="n">get_topic_tree</span><span class="p">(</span><span class="n">hierarchical_topics</span><span class="p">)</span>
</code></pre></div>
<ul>
<li>Visualize <a href="https://maartengr.github.io/BERTopic/getting_started/visualization/visualization.html#visualize-documents">2D documents</a> with <code>.visualize_documents()</code></li>
</ul>
<div class="highlight"><pre><span></span><code><span class="c1"># Use input embeddings</span>
<span class="n">topic_model</span><span class="o">.</span><span class="n">visualize_documents</span><span class="p">(</span><span class="n">docs</span><span class="p">,</span> <span class="n">embeddings</span><span class="o">=</span><span class="n">embeddings</span><span class="p">)</span>

<span class="c1"># or use 2D reduced embeddings through a method of your own (e.g., PCA, t-SNE, UMAP, etc.)</span>
<span class="n">reduced_embeddings</span> <span class="o">=</span> <span class="n">UMAP</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">min_dist</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="s1">&#39;cosine&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span>
<span class="n">topic_model</span><span class="o">.</span><span class="n">visualize_documents</span><span class="p">(</span><span class="n">docs</span><span class="p">,</span> <span class="n">reduced_embeddings</span><span class="o">=</span><span class="n">reduced_embeddings</span><span class="p">)</span>
</code></pre></div>
<ul>
<li>Visualize <a href="https://maartengr.github.io/BERTopic/getting_started/visualization/visualization.html#visualize-hierarchical-documents">2D hierarchical documents</a> with <code>.visualize_hierarchical_documents()</code> </li>
</ul>
<div class="highlight"><pre><span></span><code><span class="c1"># Run the visualization with the original embeddings</span>
<span class="n">topic_model</span><span class="o">.</span><span class="n">visualize_hierarchical_documents</span><span class="p">(</span><span class="n">docs</span><span class="p">,</span> <span class="n">hierarchical_topics</span><span class="p">,</span> <span class="n">embeddings</span><span class="o">=</span><span class="n">embeddings</span><span class="p">)</span>

<span class="c1"># Or, if you have reduced the original embeddings already which speed things up quite a bit:</span>
<span class="n">reduced_embeddings</span> <span class="o">=</span> <span class="n">UMAP</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">min_dist</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="s1">&#39;cosine&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span>
<span class="n">topic_model</span><span class="o">.</span><span class="n">visualize_hierarchical_documents</span><span class="p">(</span><span class="n">docs</span><span class="p">,</span> <span class="n">hierarchical_topics</span><span class="p">,</span> <span class="n">reduced_embeddings</span><span class="o">=</span><span class="n">reduced_embeddings</span><span class="p">)</span>
</code></pre></div>
<ul>
<li>Create <a href="https://maartengr.github.io/BERTopic/getting_started/topicrepresentation/topicrepresentation.html#custom-labels">custom labels</a> to the topics throughout most visualizations</li>
</ul>
<div class="highlight"><pre><span></span><code><span class="c1"># Generate topic labels</span>
<span class="n">topic_labels</span> <span class="o">=</span> <span class="n">topic_model</span><span class="o">.</span><span class="n">generate_topic_labels</span><span class="p">(</span><span class="n">nr_words</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">topic_prefix</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">word_length</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">separator</span><span class="o">=</span><span class="s2">&quot;, &quot;</span><span class="p">)</span>

<span class="c1"># Set them internally in BERTopic</span>
<span class="n">topic_model</span><span class="o">.</span><span class="n">set_topic_labels</span><span class="p">(</span><span class="n">topics_labels</span><span class="p">)</span>
</code></pre></div>
<ul>
<li>Manually <a href="https://maartengr.github.io/BERTopic/getting_started/hierarchicaltopics/hierarchicaltopics.html#merge-topics">merge topics</a> with <code>.merge_topics()</code></li>
</ul>
<div class="highlight"><pre><span></span><code><span class="c1"># Merge topics 1, 2, and 3</span>
<span class="n">topics_to_merge</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
<span class="n">topic_model</span><span class="o">.</span><span class="n">merge_topics</span><span class="p">(</span><span class="n">docs</span><span class="p">,</span> <span class="n">topics</span><span class="p">,</span> <span class="n">topics_to_merge</span><span class="p">)</span>

<span class="c1"># Merge topics 1 and 2, and separately merge topics 3 and 4</span>
<span class="n">topics_to_merge</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]]</span>
<span class="n">topic_model</span><span class="o">.</span><span class="n">merge_topics</span><span class="p">(</span><span class="n">docs</span><span class="p">,</span> <span class="n">topics</span><span class="p">,</span> <span class="n">topics_to_merge</span><span class="p">)</span>
</code></pre></div>
<ul>
<li>Added example for finding similar topics between two models in the <a href="https://maartengr.github.io/BERTopic/getting_started/tips_and_tricks/tips_and_tricks.html">tips &amp; tricks</a> page</li>
<li>Add multi-modal example in the <a href="https://maartengr.github.io/BERTopic/getting_started/tips_and_tricks/tips_and_tricks.html">tips &amp; tricks</a> page</li>
<li>Added native <a href="https://maartengr.github.io/BERTopic/getting_started/embeddings/embeddings.html#hugging-face-transformers">Hugging Face transformers</a> support </li>
</ul>
<p><strong>Fixes</strong>: </p>
<ul>
<li>Fix support for k-Means in <code>.visualize_heatmap</code> (<a href="https://github.com/MaartenGr/BERTopic/issues/532">#532</a>)</li>
<li>Fix missing topic 0 in <code>.visualize_topics</code> (<a href="https://github.com/MaartenGr/BERTopic/issues/533">#533</a>)</li>
<li>Fix inconsistencies in <code>.get_topic_info</code> (<a href="https://github.com/MaartenGr/BERTopic/issues/572">#572</a>) and (<a href="https://github.com/MaartenGr/BERTopic/issues/581">#581</a>)</li>
<li>Add <code>optimal_ordering</code> parameter to <code>.visualize_hierarchy</code> by <a href="https://github.com/rafaelvalero">@rafaelvalero</a> in <a href="https://github.com/MaartenGr/BERTopic/pull/390">#390</a></li>
<li>Fix RuntimeError when used as sklearn estimator by <a href="https://github.com/simonfelding">@simonfelding</a> in <a href="https://github.com/MaartenGr/BERTopic/pull/448">#448</a></li>
<li>Fix typo in visualization documentation by <a href="https://github.com/dwhdai">@dwhdai</a> in <a href="https://github.com/MaartenGr/BERTopic/pull/475">#475</a></li>
<li>Fix typo in docstrings by <a href="https://github.com/xwwwwww">@xwwwwww</a> in <a href="https://github.com/MaartenGr/BERTopic/pull/549">#549</a></li>
<li>Support higher Flair versions</li>
</ul>
<h2 id="version-0100"><strong>Version 0.10.0</strong><a class="headerlink" href="#version-0100" title="Permanent link">&para;</a></h2>
<p><em>Release date: 30 April, 2022</em></p>
<p><strong>Highlights</strong>: </p>
<ul>
<li>Use any dimensionality reduction technique instead of UMAP:</li>
</ul>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">bertopic</span> <span class="kn">import</span> <span class="n">BERTopic</span>
<span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>

<span class="n">dim_model</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">topic_model</span> <span class="o">=</span> <span class="n">BERTopic</span><span class="p">(</span><span class="n">umap_model</span><span class="o">=</span><span class="n">dim_model</span><span class="p">)</span>
</code></pre></div>
<ul>
<li>Use any clustering technique instead of HDBSCAN:</li>
</ul>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">bertopic</span> <span class="kn">import</span> <span class="n">BERTopic</span>
<span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">KMeans</span>

<span class="n">cluster_model</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="n">topic_model</span> <span class="o">=</span> <span class="n">BERTopic</span><span class="p">(</span><span class="n">hdbscan_model</span><span class="o">=</span><span class="n">cluster_model</span><span class="p">)</span>
</code></pre></div>
<p><strong>Documentation</strong>: </p>
<ul>
<li>Add a CountVectorizer page with tips and tricks on how to create topic representations that fit your use case</li>
<li>Added pages on how to use other dimensionality reduction and clustering algorithms</li>
<li>Additional instructions on how to reduce outliers in the FAQ:</li>
</ul>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="n">probability_threshold</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">new_topics</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">prob</span><span class="p">)</span> <span class="k">if</span> <span class="nb">max</span><span class="p">(</span><span class="n">prob</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">probability_threshold</span> <span class="k">else</span> <span class="o">-</span><span class="mi">1</span> <span class="k">for</span> <span class="n">prob</span> <span class="ow">in</span> <span class="n">probs</span><span class="p">]</span> 
</code></pre></div>
<p><strong>Fixes</strong>: </p>
<ul>
<li>Fixed <code>None</code> being returned for probabilities when transforming unseen documents</li>
<li>Replaced all instances of <code>arg:</code> with <code>Arguments:</code> for consistency</li>
<li>Before saving a fitted BERTopic instance, we remove the stopwords in the fitted CountVectorizer model as it can get quite large due to the number of words that end in stopwords if <code>min_df</code> is set to a value larger than 1</li>
<li>Set <code>"hdbscan&gt;=0.8.28"</code> to prevent numpy issues</li>
<li>Although this was already fixed by the new release of HDBSCAN, it is technically still possible to install 0.8.27 with BERTopic which leads to these numpy issues</li>
<li>Update gensim dependency to <code>&gt;=4.0.0</code> (<a href="https://github.com/MaartenGr/BERTopic/issues/371">#371</a>)</li>
<li>Fix topic 0 not appearing in visualizations (<a href="https://github.com/MaartenGr/BERTopic/issues/472">#472</a>)</li>
<li>Fix (<a href="https://github.com/MaartenGr/BERTopic/issues/506">#506</a>)</li>
<li>Fix (<a href="https://github.com/MaartenGr/BERTopic/issues/429">#429</a>)</li>
<li>Fix typo in DTM documentation by <a href="https://github.com/hp0404">@hp0404</a> in <a href="https://github.com/MaartenGr/BERTopic/pull/386">#386</a></li>
</ul>
<h2 id="version-094"><strong>Version 0.9.4</strong><a class="headerlink" href="#version-094" title="Permanent link">&para;</a></h2>
<p><em>Release date: 14 December, 2021</em></p>
<p>A number of fixes, documentation updates, and small features:</p>
<ul>
<li>Expose diversity parameter<ul>
<li>Use <code>BERTopic(diversity=0.1)</code> to change how diverse the words in a topic representation are (ranges from 0 to 1)</li>
</ul>
</li>
<li>Improve stability of topic reduction by only computing the cosine similarity within c-TF-IDF and not the topic embeddings</li>
<li>Added property to c-TF-IDF that all IDF values should be positive (<a href="https://github.com/MaartenGr/BERTopic/issues/351">#351</a>)</li>
<li>Improve stability of <code>.visualize_barchart()</code> and <code>.visualize_hierarchy()</code></li>
<li>Major <a href="https://maartengr.github.io/BERTopic/">documentation</a> overhaul (mkdocs, tutorials, FAQ, images, etc. ) (<a href="https://github.com/MaartenGr/BERTopic/issues/330">#330</a>)</li>
<li>Drop python 3.6 (<a href="https://github.com/MaartenGr/BERTopic/issues/333">#333</a>)</li>
<li>Relax plotly dependency (<a href="https://github.com/MaartenGr/BERTopic/issues/88">#88</a>)</li>
<li>Additional logging for <code>.transform</code> (<a href="https://github.com/MaartenGr/BERTopic/issues/356">#356</a>)</li>
</ul>
<h2 id="version-093"><strong>Version 0.9.3</strong><a class="headerlink" href="#version-093" title="Permanent link">&para;</a></h2>
<p><em>Release date:  17 October, 2021</em></p>
<ul>
<li>Fix <a href="https://github.com/MaartenGr/BERTopic/issues/282">#282</a><ul>
<li>As it turns out the old implementation of topic mapping was still found in the <code>transform</code> function</li>
</ul>
</li>
<li>Fix <a href="https://github.com/MaartenGr/BERTopic/issues/285">#285</a><ul>
<li>Fix getting all representative docs</li>
</ul>
</li>
<li>Fix <a href="https://github.com/MaartenGr/BERTopic/issues/288">#288</a><ul>
<li>A recent issue with the package <code>pyyaml</code> that can be found in Google Colab</li>
</ul>
</li>
</ul>
<h2 id="version-092"><strong>Version 0.9.2</strong><a class="headerlink" href="#version-092" title="Permanent link">&para;</a></h2>
<p><em>Release date:  12 October, 2021</em></p>
<p>A release focused on algorithmic optimization and fixing several issues:</p>
<p><strong>Highlights</strong>:  </p>
<ul>
<li>Update the non-multilingual paraphrase-<em> models to the all-</em> models due to improved <a href="https://www.sbert.net/docs/pretrained_models.html">performance</a></li>
<li>Reduce necessary RAM in c-TF-IDF top 30 word <a href="https://stackoverflow.com/questions/49207275/finding-the-top-n-values-in-a-row-of-a-scipy-sparse-matrix">extraction</a></li>
</ul>
<p><strong>Fixes</strong>:  </p>
<ul>
<li>Fix topic mapping<ul>
<li>When reducing the number of topics, these need to be mapped to the correct input/output which had some issues in the previous version</li>
<li>A new class was created as a way to track these mappings regardless of how many times they were executed</li>
<li>In other words, you can iteratively reduce the number of topics after training the model without the need to continuously train the model</li>
</ul>
</li>
<li>Fix typo in embeddings page (<a href="https://github.com/MaartenGr/BERTopic/issues/200">#200</a>) </li>
<li>Fix link in README (<a href="https://github.com/MaartenGr/BERTopic/issues/233">#233</a>)</li>
<li>Fix documentation <code>.visualize_term_rank()</code> (<a href="https://github.com/MaartenGr/BERTopic/issues/253">#253</a>) </li>
<li>Fix getting correct representative docs (<a href="https://github.com/MaartenGr/BERTopic/issues/258">#258</a>)</li>
<li>Update <a href="https://maartengr.github.io/BERTopic/faq.html#i-am-facing-memory-issues-help">memory FAQ</a> with <a href="https://github.com/MaartenGr/BERTopic/issues/151">HDBSCAN pr</a></li>
</ul>
<h2 id="version-091"><strong>Version 0.9.1</strong><a class="headerlink" href="#version-091" title="Permanent link">&para;</a></h2>
<p><em>Release date:  1 September, 2021</em></p>
<p>A release focused on fixing several issues:</p>
<p><strong>Fixes</strong>:  </p>
<ul>
<li>Fix TypeError when auto-reducing topics (<a href="https://github.com/MaartenGr/BERTopic/issues/210">#210</a>) </li>
<li>Fix mapping representative docs when reducing topics (<a href="https://github.com/MaartenGr/BERTopic/issues/208">#208</a>)</li>
<li>Fix visualization issues with probabilities (<a href="https://github.com/MaartenGr/BERTopic/issues/205">#205</a>)</li>
<li>Fix missing <code>normalize_frequency</code> param in plots (<a href="https://github.com/MaartenGr/BERTopic/issues/208">#213</a>)</li>
</ul>
<h2 id="version-090"><strong>Version 0.9.0</strong><a class="headerlink" href="#version-090" title="Permanent link">&para;</a></h2>
<p><em>Release date:  9 August, 2021</em></p>
<p><strong>Highlights</strong>:  </p>
<ul>
<li>Implemented a <a href="https://maartengr.github.io/BERTopic/getting_started/guided/guided.html"><strong>Guided BERTopic</strong></a> -&gt; Use seeds to steer the Topic Modeling</li>
<li>Get the most representative documents per topic: <code>topic_model.get_representative_docs(topic=1)</code><ul>
<li>This allows users to see which documents are good representations of a topic and better understand the topics that were created</li>
</ul>
</li>
<li>Added <code>normalize_frequency</code> parameter to <code>visualize_topics_per_class</code> and <code>visualize_topics_over_time</code> in order to better compare the relative topic frequencies between topics</li>
<li>Return flat probabilities as default, only calculate the probabilities of all topics per document if <code>calculate_probabilities</code> is True</li>
<li>Added several FAQs</li>
</ul>
<p><strong>Fixes</strong>:  </p>
<ul>
<li>Fix loading pre-trained BERTopic model</li>
<li>Fix mapping of probabilities</li>
<li>Fix <a href="https://github.com/MaartenGr/BERTopic/issues/190">#190</a></li>
</ul>
<p><strong>Guided BERTopic</strong>:    </p>
<p>Guided BERTopic works in two ways: </p>
<p>First, we create embeddings for each seeded topics by joining them and passing them through the document embedder. 
These embeddings will be compared with the existing document embeddings through cosine similarity and assigned a label. 
If the document is most similar to a seeded topic, then it will get that topic's label. 
If it is most similar to the average document embedding, it will get the -1 label. 
These labels are then passed through UMAP to create a semi-supervised approach that should nudge the topic creation to the seeded topics. </p>
<p>Second, we take all words in <code>seed_topic_list</code> and assign them a multiplier larger than 1. 
Those multipliers will be used to increase the IDF values of the words across all topics thereby increasing 
the likelihood that a seeded topic word will appear in a topic. This does, however, also increase the chance of an 
irrelevant topic having unrelated words. In practice, this should not be an issue since the IDF value is likely to 
remain low regardless of the multiplier. The multiplier is now a fixed value but may change to something more elegant, 
like taking the distribution of IDF values and its position into account when defining the multiplier. </p>
<div class="highlight"><pre><span></span><code><span class="n">seed_topic_list</span> <span class="o">=</span> <span class="p">[[</span><span class="s2">&quot;company&quot;</span><span class="p">,</span> <span class="s2">&quot;billion&quot;</span><span class="p">,</span> <span class="s2">&quot;quarter&quot;</span><span class="p">,</span> <span class="s2">&quot;shrs&quot;</span><span class="p">,</span> <span class="s2">&quot;earnings&quot;</span><span class="p">],</span>
                   <span class="p">[</span><span class="s2">&quot;acquisition&quot;</span><span class="p">,</span> <span class="s2">&quot;procurement&quot;</span><span class="p">,</span> <span class="s2">&quot;merge&quot;</span><span class="p">],</span>
                   <span class="p">[</span><span class="s2">&quot;exchange&quot;</span><span class="p">,</span> <span class="s2">&quot;currency&quot;</span><span class="p">,</span> <span class="s2">&quot;trading&quot;</span><span class="p">,</span> <span class="s2">&quot;rate&quot;</span><span class="p">,</span> <span class="s2">&quot;euro&quot;</span><span class="p">],</span>
                   <span class="p">[</span><span class="s2">&quot;grain&quot;</span><span class="p">,</span> <span class="s2">&quot;wheat&quot;</span><span class="p">,</span> <span class="s2">&quot;corn&quot;</span><span class="p">],</span>
                   <span class="p">[</span><span class="s2">&quot;coffee&quot;</span><span class="p">,</span> <span class="s2">&quot;cocoa&quot;</span><span class="p">],</span>
                   <span class="p">[</span><span class="s2">&quot;natural&quot;</span><span class="p">,</span> <span class="s2">&quot;gas&quot;</span><span class="p">,</span> <span class="s2">&quot;oil&quot;</span><span class="p">,</span> <span class="s2">&quot;fuel&quot;</span><span class="p">,</span> <span class="s2">&quot;products&quot;</span><span class="p">,</span> <span class="s2">&quot;petrol&quot;</span><span class="p">]]</span>

<span class="n">topic_model</span> <span class="o">=</span> <span class="n">BERTopic</span><span class="p">(</span><span class="n">seed_topic_list</span><span class="o">=</span><span class="n">seed_topic_list</span><span class="p">)</span>
<span class="n">topics</span><span class="p">,</span> <span class="n">probs</span> <span class="o">=</span> <span class="n">topic_model</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>
</code></pre></div>
<h2 id="version-081"><strong>Version 0.8.1</strong><a class="headerlink" href="#version-081" title="Permanent link">&para;</a></h2>
<p><em>Release date:  8 June, 2021</em></p>
<p><strong>Highlights</strong>:  </p>
<ul>
<li>Improved models:<ul>
<li>For English documents the default is now: <code>"paraphrase-MiniLM-L6-v2"</code> </li>
<li>For Non-English or multi-lingual documents the default is now: <code>"paraphrase-multilingual-MiniLM-L12-v2"</code> </li>
<li>Both models show not only great performance but are much faster!  </li>
</ul>
</li>
<li>Add interactive visualizations to the <code>plotting</code> API documentation</li>
</ul>
<p>For better performance, please use the following models:  </p>
<ul>
<li>English: <code>"paraphrase-mpnet-base-v2"</code></li>
<li>Non-English or multi-lingual: <code>"paraphrase-multilingual-mpnet-base-v2"</code></li>
</ul>
<p><strong>Fixes</strong>:   </p>
<ul>
<li>Improved unit testing for more stability</li>
<li>Set transformers version for Flair</li>
</ul>
<h2 id="version-080"><strong>Version 0.8.0</strong><a class="headerlink" href="#version-080" title="Permanent link">&para;</a></h2>
<p><em>Release date:  31 May, 2021</em></p>
<p><strong>Highlights</strong>:  </p>
<ul>
<li>Additional visualizations:<ul>
<li>Topic Hierarchy: <code>topic_model.visualize_hierarchy()</code> </li>
<li>Topic Similarity Heatmap: <code>topic_model.visualize_heatmap()</code> </li>
<li>Topic Representation Barchart: <code>topic_model.visualize_barchart()</code> </li>
<li>Term Score Decline: <code>topic_model.visualize_term_rank()</code> </li>
</ul>
</li>
<li>Created <code>bertopic.plotting</code> library to easily extend visualizations</li>
<li>Improved automatic topic reduction by using HDBSCAN to detect similar topics</li>
<li>Sort topic ids by their frequency. -1 is the outlier class and contains typically the most documents. After that 0 is the largest  topic, 1 the second largest, etc. </li>
</ul>
<p><strong>Fixes</strong>:   </p>
<ul>
<li>Fix typo <a href="https://github.com/MaartenGr/BERTopic/pull/113">#113</a>, <a href="https://github.com/MaartenGr/BERTopic/pull/117">#117</a></li>
<li>Fix <a href="https://github.com/MaartenGr/BERTopic/issues/121">#121</a> by removing <a href="https://github.com/MaartenGr/BERTopic/blob/5c6cf22776fafaaff728370781a5d33727d3dc8f/bertopic/_bertopic.py#L359-L360">these</a> two lines</li>
<li>Fix mapping of topics after reduction (it now excludes 0) (<a href="https://github.com/MaartenGr/BERTopic/issues/103">#103</a>)</li>
</ul>
<h2 id="version-070"><strong>Version 0.7.0</strong><a class="headerlink" href="#version-070" title="Permanent link">&para;</a></h2>
<p><em>Release date:  26 April, 2021</em>  </p>
<p>The two main features are <strong>(semi-)supervised topic modeling</strong> 
and several <strong>backends</strong> to use instead of Flair and SentenceTransformers!</p>
<p><strong>Highlights</strong>:</p>
<ul>
<li>(semi-)supervised topic modeling by leveraging supervised options in UMAP<ul>
<li><code>model.fit(docs, y=target_classes)</code></li>
</ul>
</li>
<li>Backends:<ul>
<li>Added Spacy, Gensim, USE (TFHub)</li>
<li>Use a different backend for document embeddings and word embeddings</li>
<li>Create your own backends with <code>bertopic.backend.BaseEmbedder</code></li>
<li>Click <a href="https://maartengr.github.io/BERTopic/getting_started/embeddings/embeddings.html">here</a> for an overview of all new backends</li>
</ul>
</li>
<li>Calculate and visualize topics per class<ul>
<li>Calculate: <code>topics_per_class = topic_model.topics_per_class(docs, topics, classes)</code></li>
<li>Visualize: <code>topic_model.visualize_topics_per_class(topics_per_class)</code></li>
</ul>
</li>
<li>Several tutorials were updated and added:</li>
</ul>
<table>
<thead>
<tr>
<th>Name</th>
<th>Link</th>
</tr>
</thead>
<tbody>
<tr>
<td>Topic Modeling with BERTopic</td>
<td><a href="https://colab.research.google.com/drive/1FieRA9fLdkQEGDIMYl0I3MCjSUKVF8C-?usp=sharing"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></td>
</tr>
<tr>
<td>(Custom) Embedding Models in BERTopic</td>
<td><a href="https://colab.research.google.com/drive/18arPPe50szvcCp_Y6xS56H2tY0m-RLqv?usp=sharing"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></td>
</tr>
<tr>
<td>Advanced Customization in BERTopic</td>
<td><a href="https://colab.research.google.com/drive/1ClTYut039t-LDtlcd-oQAdXWgcsSGTw9?usp=sharing"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></td>
</tr>
<tr>
<td>(semi-)Supervised Topic Modeling with BERTopic</td>
<td><a href="https://colab.research.google.com/drive/1bxizKzv5vfxJEB29sntU__ZC7PBSIPaQ?usp=sharing"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></td>
</tr>
<tr>
<td>Dynamic Topic Modeling with Trump's Tweets</td>
<td><a href="https://colab.research.google.com/drive/1un8ooI-7ZNlRoK0maVkYhmNRl0XGK88f?usp=sharing"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></td>
</tr>
</tbody>
</table>
<p><strong>Fixes</strong>:  </p>
<ul>
<li>Fixed issues with Torch req</li>
<li>Prevent saving term frequency matrix in CTFIDF class</li>
<li>Fixed DTM not working when reducing topics (<a href="https://github.com/MaartenGr/BERTopic/issues/96">#96</a>)</li>
<li>Moved visualization dependencies to base BERTopic<ul>
<li><code>pip install bertopic[visualization]</code> becomes <code>pip install bertopic</code></li>
</ul>
</li>
<li>Allow precomputed embeddings in bertopic.find_topics() (<a href="https://github.com/MaartenGr/BERTopic/issues/79">#79</a>):</li>
</ul>
<div class="highlight"><pre><span></span><code><span class="n">model</span> <span class="o">=</span> <span class="n">BERTopic</span><span class="p">(</span><span class="n">embedding_model</span><span class="o">=</span><span class="n">my_embedding_model</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">docs</span><span class="p">,</span> <span class="n">my_precomputed_embeddings</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">find_topics</span><span class="p">(</span><span class="n">search_term</span><span class="p">)</span>
</code></pre></div>
<h2 id="version-060"><strong>Version 0.6.0</strong><a class="headerlink" href="#version-060" title="Permanent link">&para;</a></h2>
<p><em>Release date:  1 March, 2021</em></p>
<p><strong>Highlights</strong>:</p>
<ul>
<li>DTM: Added a basic dynamic topic modeling technique based on the global c-TF-IDF representation <ul>
<li><code>model.topics_over_time(docs, timestamps, global_tuning=True)</code></li>
</ul>
</li>
<li>DTM: Option to evolve topics based on t-1 c-TF-IDF representation which results in evolving topics over time<ul>
<li>Only uses topics at t-1 and skips evolution if there is a gap</li>
<li><code>model.topics_over_time(docs, timestamps, evolution_tuning=True)</code></li>
</ul>
</li>
<li>DTM: Function to visualize topics over time <ul>
<li><code>model.visualize_topics_over_time(topics_over_time)</code></li>
</ul>
</li>
<li>DTM: Add binning of timestamps  <ul>
<li><code>model.topics_over_time(docs, timestamps, nr_bins=10)</code></li>
</ul>
</li>
<li>Add function get general information about topics (id, frequency, name, etc.) <ul>
<li><code>get_topic_info()</code></li>
</ul>
</li>
<li>Improved stability of c-TF-IDF by taking the average number of words across all topics instead of the number of documents</li>
</ul>
<p><strong>Fixes</strong>:</p>
<ul>
<li><code>_map_probabilities()</code> does not take into account that there is no probability of the outlier class and the probabilities are mutated instead of copied (#63, #64)</li>
</ul>
<h2 id="version-050"><strong>Version 0.5.0</strong><a class="headerlink" href="#version-050" title="Permanent link">&para;</a></h2>
<p><em>Release date:  8 Februari, 2021</em></p>
<p><strong>Highlights</strong>:</p>
<ul>
<li>Add <code>Flair</code> to allow for more (custom) token/document embeddings, including ðŸ¤— transformers </li>
<li>Option to use custom UMAP, HDBSCAN, and CountVectorizer</li>
<li>Added <code>low_memory</code> parameter to reduce memory during computation</li>
<li>Improved verbosity (shows progress bar)</li>
<li>Return the figure of <code>visualize_topics()</code></li>
<li>Expose all parameters with a single function: <code>get_params()</code></li>
</ul>
<p><strong>Fixes</strong>:</p>
<ul>
<li>To simplify the API, the parameters stop_words and n_neighbors were removed. These can still be used when a custom UMAP or CountVectorizer is used.</li>
<li>Set <code>calculate_probabilities</code> to False as a default. Calculating probabilities with HDBSCAN significantly increases computation time and memory usage. Better to remove calculating probabilities or only allow it by manually turning this on.</li>
<li>Use the newest version of <code>sentence-transformers</code> as it speeds ups encoding significantly</li>
</ul>
<h2 id="version-042"><strong>Version 0.4.2</strong><a class="headerlink" href="#version-042" title="Permanent link">&para;</a></h2>
<p><em>Release date:  10 Januari, 2021</em></p>
<p><strong>Fixes</strong>:  </p>
<ul>
<li>Selecting <code>embedding_model</code> did not work when <code>language</code> was also used. This led to the user needing 
to set <code>language</code> to None before being able to use <code>embedding_model</code>. Fixed by using <code>embedding_model</code> when 
<code>language</code> is used (as a default parameter).</li>
</ul>
<h2 id="version-041"><strong>Version 0.4.1</strong><a class="headerlink" href="#version-041" title="Permanent link">&para;</a></h2>
<p><em>Release date:  07 Januari, 2021</em></p>
<p><strong>Fixes</strong>:  </p>
<ul>
<li>Simple fix by lowering the languages variable to match the lowered input language.</li>
</ul>
<h2 id="version-040"><strong>Version 0.4.0</strong><a class="headerlink" href="#version-040" title="Permanent link">&para;</a></h2>
<p><em>Release date:  21 December, 2020</em></p>
<p><strong>Highlights</strong>:  </p>
<ul>
<li>Visualize Topics similar to <a href="https://github.com/cpsievert/LDAvis">LDAvis</a></li>
<li>Added option to reduce topics after training</li>
<li>Added option to update topic representation after training</li>
<li>Added option to search topics using a search term</li>
<li>Significantly improved the stability of generating clusters</li>
<li>Finetune the topic words by selecting the most coherent words with the highest c-TF-IDF values </li>
<li>More extensive tutorials in the documentation</li>
</ul>
<p><strong>Notable Changes</strong>:  </p>
<ul>
<li>Option to select language instead of sentence-transformers models to minimize the complexity of using BERTopic</li>
<li>Improved logging (remove duplicates) </li>
<li>Check if BERTopic is fitted </li>
<li>Added TF-IDF as an embedder instead of transformer models (see tutorial)</li>
<li>Numpy for Python 3.6 will be dropped and was therefore removed from the workflow.</li>
<li>Preprocess text before passing it through c-TF-IDF</li>
<li>Merged <code>get_topics_freq()</code> with <code>get_topic_freq()</code> </li>
</ul>
<p><strong>Fixes</strong>:  </p>
<ul>
<li>Fix error handling topic probabilities</li>
</ul>
<h2 id="version-032"><strong>Version 0.3.2</strong><a class="headerlink" href="#version-032" title="Permanent link">&para;</a></h2>
<p><em>Release date:  16 November, 2020</em></p>
<p><strong>Highlights</strong>:</p>
<ul>
<li>Fixed a bug with the topic reduction method that seems to reduce the number of topics but not to the nr_topics as defined in the class. Since this was, to a certain extend, breaking the topic reduction method a new release was necessary.</li>
</ul>
<h2 id="version-031"><strong>Version 0.3.1</strong><a class="headerlink" href="#version-031" title="Permanent link">&para;</a></h2>
<p><em>Release date:  4 November, 2020</em></p>
<p><strong>Highlights</strong>:</p>
<ul>
<li>Adding the option to use custom embeddings or embeddings that you generated beforehand with whatever package you'd like to use. This allows users to further customize BERTopic to their liking.</li>
</ul>
<h2 id="version-030"><strong>Version 0.3.0</strong><a class="headerlink" href="#version-030" title="Permanent link">&para;</a></h2>
<p><em>Release date:  29 October, 2020</em></p>
<p><strong>Highlights</strong>:</p>
<ul>
<li>transform() and fit_transform() now also return the topic probability distributions</li>
<li>Added visualize_distribution() which visualizes the topic probability distribution for a single document</li>
</ul>
<h2 id="version-022"><strong>Version 0.2.2</strong><a class="headerlink" href="#version-022" title="Permanent link">&para;</a></h2>
<p><em>Release date:  17 October, 2020</em></p>
<p><strong>Highlights</strong>:</p>
<ul>
<li>Fixed n_gram_range not being used</li>
<li>Added option for using stopwords</li>
</ul>
<h2 id="version-021"><strong>Version 0.2.1</strong><a class="headerlink" href="#version-021" title="Permanent link">&para;</a></h2>
<p><em>Release date:  11 October, 2020</em></p>
<p><strong>Highlights</strong>:</p>
<ul>
<li>Improved the calculation of the class-based TF-IDF procedure by limiting the calculation to sparse matrices. This prevents out-of-memory problems when faced with large datasets.</li>
</ul>
<h2 id="version-020"><strong>Version 0.2.0</strong><a class="headerlink" href="#version-020" title="Permanent link">&para;</a></h2>
<p><em>Release date:  11 October, 2020</em></p>
<p><strong>Highlights</strong>:</p>
<ul>
<li>Changed c-TF-IDF procedure such that it implements a version of scikit-learns procedure. This should also speed up the calculation of the sparse matrix and prevent memory errors.</li>
<li>Added automated unit tests  </li>
</ul>
<h2 id="version-012"><strong>Version 0.1.2</strong><a class="headerlink" href="#version-012" title="Permanent link">&para;</a></h2>
<p><em>Release date:  1 October, 2020</em></p>
<p><strong>Highlights</strong>:</p>
<ul>
<li>When transforming new documents, self.mapped_topics seemed to be missing. Added to the init.</li>
</ul>
<h2 id="version-011"><strong>Version 0.1.1</strong><a class="headerlink" href="#version-011" title="Permanent link">&para;</a></h2>
<p><em>Release date:  24 September, 2020</em></p>
<p><strong>Highlights</strong>:</p>
<ul>
<li>Fixed requirements --&gt; Issue with pytorch</li>
<li>Update documentation</li>
</ul>
<h2 id="version-010"><strong>Version 0.1.0</strong><a class="headerlink" href="#version-010" title="Permanent link">&para;</a></h2>
<p><em>Release date:  24 September, 2020</em></p>
<p><strong>Highlights</strong>:  </p>
<ul>
<li>First release of <code>BERTopic</code></li>
<li>Added parameters for UMAP and HDBSCAN</li>
<li>Option to choose sentence-transformer model</li>
<li>Method for transforming unseen documents</li>
<li>Save and load trained models (UMAP and HDBSCAN)</li>
<li>Extract topics and their sizes</li>
</ul>
<p><strong>Notable Changes</strong>:  </p>
<ul>
<li>Optimized c-TF-IDF</li>
<li>Improved documentation</li>
<li>Improved topic reduction</li>
</ul>



  



                
              </article>
            </div>
          
          
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright &copy; 2023 Maintained by <a href="https://github.com/MaartenGr">Maarten</a>.
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": ".", "features": ["navigation.tabs", "navigation.sections", "navigation.instant", "navigation.top", "navigation.tracking", "toc.follow", "content.code.copy"], "search": "assets/javascripts/workers/search.f886a092.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="assets/javascripts/bundle.aecac24b.min.js"></script>
      
    
  </body>
</html>