{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "from bertopic import BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== BERTopic Delete Topics Test Results ===\n",
      "\n",
      "Initializing and fitting BERTopic model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-31 21:49:21,445 - BERTopic - Embedding - Transforming documents to embeddings.\n",
      "Batches: 100%|██████████| 16/16 [00:12<00:00,  1.29it/s]\n",
      "2025-03-31 21:49:34,425 - BERTopic - Embedding - Completed ✓\n",
      "2025-03-31 21:49:34,426 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-03-31 21:49:35,290 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-03-31 21:49:35,291 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-03-31 21:49:35,307 - BERTopic - Cluster - Completed ✓\n",
      "2025-03-31 21:49:35,309 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-03-31 21:49:35,493 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial State:\n",
      "Total topics: 73\n",
      "Topic sizes (top 5): {0: 11, 1: 11, 2: 10, 3: 10, 4: 9}\n",
      "Matrix shapes - c_tf_idf: (74, 19877), embeddings: (74, 384)\n",
      "\n",
      "Deleting topics: [3, 4, 6]\n",
      "Original sizes of topics to delete: [10, 9, 9]\n",
      "\n",
      "✓ Topic deletion completed\n",
      "\n",
      "=== Validation Results ===\n",
      "\n",
      "1. Topic Counts:\n",
      "   Before: 73 topics\n",
      "   After: 70 topics\n",
      "   Expected: 70 topics\n",
      "\n",
      "2. Size-based Ordering:\n",
      "   Topic IDs by size: [0, 1, 2, 4, 3, 5, 8, 6, 7, 13, 10, 9, 12, 11, 17, 15, 16, 14, 20, 21, 22, 18, 19, 23, 26, 28, 24, 27, 25, 29, 35, 36, 41, 37, 31, 30, 40, 38, 34, 33, 32, 42, 39, 43, 45, 50, 51, 49, 52, 53, 44, 48, 57, 47, 54, 55, 56, 46, 58, 61, 60, 59, 62, 63, 64, 65, 66, 67, 68, 69]\n",
      "   Sizes: [11, 11, 10, 9, 9, 9, 8, 8, 8, 7, 7, 7, 7, 7, 6, 6, 6, 6, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "   Correctly ordered by size: False\n",
      "   WARNING: Topics not properly ordered by size!\n",
      "\n",
      "3. Matrix Shapes:\n",
      "   c_tf_idf - Before: (74, 19877), After: (71, 19877)\n",
      "   embeddings - Before: (74, 384), After: (71, 384)\n",
      "\n",
      "4. Topic Representations:\n",
      "   Before: 74 representations\n",
      "   After: 71 representations\n",
      "\n",
      "5. Outlier Topic (-1):\n",
      "   Present in topics_: True\n",
      "   Present in sizes: True\n",
      "   Present in representations: True\n",
      "\n",
      "6. Topic Deletion and Reordering:\n",
      "   Expected topic count: 70\n",
      "   Actual topic count: 70\n",
      "   Sequential topic numbering: True\n",
      "   Sizes match: True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "\n",
    "def create_topic_model():\n",
    "    \"\"\"Create and fit a BERTopic model\"\"\"\n",
    "    # Create sample data\n",
    "    docs = fetch_20newsgroups(subset='all')['data'][:500]\n",
    "    \n",
    "    # Initialize BERTopic with specific models\n",
    "    umap = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine', low_memory=True, random_state=42)\n",
    "    hdbscan_model = HDBSCAN(min_cluster_size=2, metric='euclidean', cluster_selection_method='leaf', prediction_data=True)\n",
    "    topic_model = BERTopic(umap_model=umap, hdbscan_model=hdbscan_model, embedding_model='all-MiniLM-L6-v2', verbose=True)\n",
    "    \n",
    "    # Fit the model\n",
    "    topics, probs = topic_model.fit_transform(docs)\n",
    "    return topic_model\n",
    "\n",
    "def run_deletion_tests():\n",
    "    \"\"\"Run comprehensive tests for topic deletion and print detailed comparisons\"\"\"\n",
    "    print(\"\\n=== BERTopic Delete Topics Test Results ===\\n\")\n",
    "    \n",
    "    # Setup model\n",
    "    print(\"Initializing and fitting BERTopic model...\")\n",
    "    topic_model = create_topic_model()\n",
    "    \n",
    "    # Record initial state\n",
    "    initial_state = {\n",
    "        'topic_sizes': topic_model.topic_sizes_.copy(),\n",
    "        'topics_set': set(topic_model.topics_),\n",
    "        'c_tf_idf_shape': topic_model.c_tf_idf_.shape,\n",
    "        'embeddings_shape': topic_model.topic_embeddings_.shape,\n",
    "        'representations_count': len(topic_model.topic_representations_),\n",
    "    }\n",
    "    \n",
    "    # Get topics sorted by size (excluding -1)\n",
    "    sorted_topics = sorted(\n",
    "        [(topic, size) for topic, size in initial_state['topic_sizes'].items() if topic != -1],\n",
    "        key=lambda x: x[1],\n",
    "        reverse=True\n",
    "    )\n",
    "    \n",
    "    print(\"\\nInitial State:\")\n",
    "    print(f\"Total topics: {len(sorted_topics)}\")\n",
    "    print(f\"Topic sizes (top 5): {dict(sorted_topics[:5])}\")\n",
    "    print(f\"Matrix shapes - c_tf_idf: {initial_state['c_tf_idf_shape']}, embeddings: {initial_state['embeddings_shape']}\")\n",
    "    \n",
    "    # Select topics to delete (4th, 5th, 6th largest)\n",
    "    topics_to_delete = [item[0] for item in sorted_topics[3:6]]\n",
    "    print(f\"\\nDeleting topics: {topics_to_delete}\")\n",
    "    print(f\"Original sizes of topics to delete: {[initial_state['topic_sizes'][t] for t in topics_to_delete]}\")\n",
    "    \n",
    "    # Perform deletion\n",
    "    try:\n",
    "        topic_model.delete_topics(topics_to_delete)\n",
    "        print(\"\\n✓ Topic deletion completed\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Error during topic deletion: {str(e)}\")\n",
    "        return\n",
    "    \n",
    "    # Analyze results\n",
    "    print(\"\\n=== Validation Results ===\\n\")\n",
    "    \n",
    "    # 1. Check topic counts\n",
    "    new_topics = set(topic_model.topics_) - {-1}\n",
    "    print(f\"1. Topic Counts:\")\n",
    "    print(f\"   Before: {len(sorted_topics)} topics\")\n",
    "    print(f\"   After: {len(new_topics)} topics\")\n",
    "    print(f\"   Expected: {len(sorted_topics) - len(topics_to_delete)} topics\")\n",
    "    \n",
    "    # 2. Check size ordering\n",
    "    new_sorted_topics = sorted(\n",
    "        [(topic, size) for topic, size in topic_model.topic_sizes_.items() if topic != -1],\n",
    "        key=lambda x: x[1],\n",
    "        reverse=True\n",
    "    )\n",
    "    \n",
    "    print(\"\\n2. Size-based Ordering:\")\n",
    "    print(f\"   Topic IDs by size: {[t[0] for t in new_sorted_topics]}\")\n",
    "    print(f\"   Sizes: {[t[1] for t in new_sorted_topics]}\")\n",
    "    is_ordered = all(i == t[0] for i, t in enumerate(new_sorted_topics))\n",
    "    print(f\"   Correctly ordered by size: {is_ordered}\")\n",
    "    if not is_ordered:\n",
    "        print(\"   WARNING: Topics not properly ordered by size!\")\n",
    "    \n",
    "    # 3. Check matrix shapes\n",
    "    print(\"\\n3. Matrix Shapes:\")\n",
    "    print(f\"   c_tf_idf - Before: {initial_state['c_tf_idf_shape']}, After: {topic_model.c_tf_idf_.shape}\")\n",
    "    print(f\"   embeddings - Before: {initial_state['embeddings_shape']}, After: {topic_model.topic_embeddings_.shape}\")\n",
    "    \n",
    "    # 4. Check representations\n",
    "    print(\"\\n4. Topic Representations:\")\n",
    "    print(f\"   Before: {initial_state['representations_count']} representations\")\n",
    "    print(f\"   After: {len(topic_model.topic_representations_)} representations\")\n",
    "    \n",
    "    # 5. Check outlier topic\n",
    "    print(\"\\n5. Outlier Topic (-1):\")\n",
    "    print(f\"   Present in topics_: {-1 in topic_model.topics_}\")\n",
    "    print(f\"   Present in sizes: {-1 in topic_model.topic_sizes_}\")\n",
    "    print(f\"   Present in representations: {-1 in topic_model.topic_representations_}\")\n",
    "    \n",
    "    # 6. Verify topic deletion and reordering\n",
    "    print(\"\\n6. Topic Deletion and Reordering:\")\n",
    "    expected_topic_count = len(sorted_topics) - len(topics_to_delete)\n",
    "    actual_topic_count = len([t for t in topic_model.topic_sizes_.keys() if t != -1])\n",
    "    \n",
    "    print(f\"   Expected topic count: {expected_topic_count}\")\n",
    "    print(f\"   Actual topic count: {actual_topic_count}\")\n",
    "    \n",
    "    # Check sequential numbering\n",
    "    expected_topic_numbers = set(range(expected_topic_count))\n",
    "    actual_topic_numbers = set(t for t in topic_model.topic_sizes_.keys() if t != -1)\n",
    "    sequential_numbering = expected_topic_numbers == actual_topic_numbers\n",
    "    \n",
    "    print(f\"   Sequential topic numbering: {sequential_numbering}\")\n",
    "    if not sequential_numbering:\n",
    "        print(f\"   Expected topics: {sorted(expected_topic_numbers)}\")\n",
    "        print(f\"   Actual topics: {sorted(actual_topic_numbers)}\")\n",
    "    \n",
    "    # Check sizes match (excluding deleted topics)\n",
    "    expected_sizes = sorted([size for topic, size in sorted_topics if topic not in topics_to_delete], reverse=True)\n",
    "    actual_sizes = sorted([size for topic, size in topic_model.topic_sizes_.items() if topic != -1], reverse=True)\n",
    "    sizes_match = expected_sizes == actual_sizes\n",
    "    \n",
    "    print(f\"   Sizes match: {sizes_match}\")\n",
    "    if not sizes_match:\n",
    "        print(f\"   Expected sizes: {expected_sizes}\")\n",
    "        print(f\"   Actual sizes: {actual_sizes}\")\n",
    "\n",
    "    # Update the validations dictionary\n",
    "    validations = {\n",
    "        \"Topic count correct\": actual_topic_count == expected_topic_count,\n",
    "        \"Size ordering correct\": is_ordered,\n",
    "        \"Matrix shapes consistent\": topic_model.c_tf_idf_.shape[0] == topic_model.topic_embeddings_.shape[0],\n",
    "        \"Sequential topic numbering\": sequential_numbering,\n",
    "        \"Topic sizes preserved\": sizes_match,\n",
    "        \"Outlier preserved\": all([-1 in topic_model.topics_, -1 in topic_model.topic_sizes_, -1 in topic_model.topic_representations_])\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_deletion_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bertopic-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
